{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting run....\n",
      "starting run....\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "print('starting run....')\n",
    "\n",
    "import sys\n",
    "\n",
    "print('starting run....')\n",
    "\n",
    "sys.path.insert(0,'/gpfs3/well/rahimi/users/gra027/JNb/')\n",
    "\n",
    "from general_model_newCutCPRD.ModelPkg.BEHRTraw import *\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from general_model_newCutCPRD.ModelPkg import utils\n",
    "from general_model_newCutCPRD.ModelPkg.MLMRaw import *\n",
    "\n",
    "from general_model_newCutCPRD.ModelPkg.DataProc import *\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from general_model_newCutCPRD.pytorch_pretrained_bert  import optimizer\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4124\n",
      "read data....\n",
      "finished reading data....\n",
      "dict_keys(['bert.embeddings.word_embeddings.weight', 'bert.embeddings.segment_embeddings.weight', 'bert.embeddings.age_embeddings.weight', 'bert.embeddings.year_embeddings.weight', 'bert.embeddings.posi_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.embeddings.catmap.weight', 'bert.embeddings.catmap.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight'])\n"
     ]
    }
   ],
   "source": [
    "file_config = {\n",
    "        'vocab': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/GeneralVocDMProc_25k',\n",
    "#     'oldvocab': '/gpfs3/well/rahimi/users/gra027/JNb/ExpHypCancer/Data/AllSubclass/DMBp',\n",
    "    'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_DMProc_for_pretraining_28M_1985_2020__unique_per6m_70pc_sample.parquet/',\n",
    "\n",
    "#     'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_DMProc_for_pretraining_28M_1985_2020__unique_per6m_70pc_sample___10kdebug.parquet/',\n",
    "    #\n",
    "    'yearVocab':  '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/yearVoc_1985_2021',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 156,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:1',\n",
    "    'output_dir':'/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/SavedModels/',\n",
    "    'output_name': 'MLM_CEHR_newcut1985_2020_DM.bin',\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 250,\n",
    "    'max_age': 110,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    'yearOn':True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "YearVocab = utils.load_obj(file_config['yearVocab'])\n",
    "create_folder(global_params['output_dir'])\n",
    "BertVocab = utils.load_obj(file_config['vocab'])\n",
    "print(len(BertVocab['token2idx']))\n",
    "print('read data....')\n",
    "\n",
    "ageVocab, _ = utils.age_vocab(max_age=global_params['max_age'], year=global_params['age_year'], symbol=global_params['age_symbol'])\n",
    "fulldata = pd.read_parquet(file_config['fulld'])\n",
    "print('finished reading data....')\n",
    "\n",
    "trainSet = MLMLoader(token2idx=BertVocab['token2idx'], dataframe=fulldata, max_len=global_params['max_len_seq'], max_age=global_params['max_age'], year=global_params['age_year'], age_symbol=global_params['age_symbol'],year2idx = YearVocab['token2idx'] )\n",
    "trainload = DataLoader(dataset=trainSet, batch_size=global_params['batch_size'], shuffle=True)\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 150, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()), # number of vocab for age embedding\n",
    "\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.15, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 6, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.15, # multi-head attention dropout rate\n",
    "    'intermediate_size': 108, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range,\n",
    "    'yearOn':True,\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()),\n",
    "    'concat_embeddings':True,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForMaskedLM(conf)\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_CEHR_newcut1985_2020_DMProc__6msummary.bin\")\n",
    "model = toLoad(model, output_model_file)\n",
    "model = model.to(global_params['device'])\n",
    "optim = optimizer.adam(params=list(model.named_parameters()), config=optim_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM_CEHR_newcut1985_2020_DMProc__6msummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = BertConfig(model_config)\n",
    "# model = BertForMaskedLM(conf)\n",
    "# output_model_file = os.path.join(global_params['output_dir'], \"MLM_CEHR_newcut1985_2020_DM__6msummary.bin\")\n",
    "# # model = toLoad(model, output_model_file)\n",
    "\n",
    "\n",
    "# dd = torch.load(output_model_file,  map_location='cpu')\n",
    "# modeld = model.state_dict()\n",
    "# modeld['bert.embeddings.word_embeddings.weight'][:3662] = dd['bert.embeddings.word_embeddings.weight']\n",
    "# modeld['cls.predictions.bias'][:3662] = dd['cls.predictions.bias']\n",
    "# modeld['cls.predictions.decoder.weight'][:3662] = dd['cls.predictions.decoder.weight']\n",
    "\n",
    "# # for x in BertVocab['idx2token']:\n",
    "# #     if 'bnf' not in BertVocab['idx2token'][x] and 'vtm' not in BertVocab['idx2token'][x] and 'BANDAGE' not in BertVocab['idx2token'][x]:\n",
    "# #         if BertVocab['idx2token'][x] in BertVocabold['token2idx']:\n",
    "# #             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x]]\n",
    "\n",
    "# #             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "# #             count = count+1\n",
    "# #         elif     BertVocab['idx2token'][x][:-1] in BertVocabold['token2idx']:\n",
    "# #             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x][:-1]]\n",
    "# #             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "# #             count = count+1\n",
    "\n",
    "# #         else:\n",
    "# #             for y in BertVocabold['token2idx']:\n",
    "# #                 if BertVocab['idx2token'][x][:-1] in y :\n",
    "# #                     oldindx = BertVocabold['token2idx'][y]\n",
    "# #     #                 print(BertVocab['idx2token'][x], y )\n",
    "# #                     modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "# #                     count = count+1\n",
    "# #                     break\n",
    "# #     else:\n",
    "# #         if 'bnf' in BertVocab['idx2token'][x]:\n",
    "            \n",
    "# #             text2check = BertVocab['idx2token'][x][4:8]\n",
    "# #             if text2check in BertVocabold['token2idx']:\n",
    "# #                 oldindx = BertVocabold['token2idx'][text2check]\n",
    "# #                 modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "# #                 count = count+1\n",
    "\n",
    "# #             else:\n",
    "# #                 for y in BertVocabold['token2idx']:\n",
    "# #                     if text2check in y :\n",
    "# # #                         print(y, text2check)\n",
    "# #                         count = count+1\n",
    "# #                         oldindx = BertVocabold['token2idx'][y]\n",
    "# #                         modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "\n",
    "# #                         break\n",
    "                \n",
    "                \n",
    "# for x in modeld:\n",
    "#     if x not in ['bert.embeddings.word_embeddings.weight','cls.predictions.bias', 'cls.predictions.decoder.weight']:\n",
    "#         if x in dd:\n",
    "#             print(x)\n",
    "#             modeld[x] = dd[x]\n",
    "# model.load_state_dict(modeld)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = model.to(global_params['device'])\n",
    "# optim = optimizer.adam(params=list(model.named_parameters()), config=optim_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# YearVocab = {'token2idx': {'PAD': 0,\n",
    "#   '1987': 1,\n",
    "#   '1988': 2,\n",
    "#   '1989': 3,\n",
    "#   '1990': 4,\n",
    "#   '1991': 5,\n",
    "#   '1992': 6,\n",
    "#   '1993': 7,\n",
    "#   '1994': 8,\n",
    "#   '1995': 9,\n",
    "#   '1996': 10,\n",
    "#   '1997': 11,\n",
    "#   '1998': 12,\n",
    "#   '1999': 13,\n",
    "#   '2000': 14,\n",
    "#   '2001': 15,\n",
    "#   '2002': 16,\n",
    "#   '2003': 17,\n",
    "#   '2004': 18,\n",
    "#   '2005': 19,\n",
    "#   '2006': 20,\n",
    "#   '2007': 21,\n",
    "#   '2008': 22,\n",
    "#   '2009': 23,\n",
    "#   '2010': 24,\n",
    "#   '2011': 25,\n",
    "#   '2012': 26,\n",
    "#   '2013': 27,\n",
    "#   '2014': 28,\n",
    "#   '2015': 29,\n",
    "#  '2016': 30,\n",
    "#   '2017': 31,\n",
    "#   '2018': 32,\n",
    "#   '2019': 33,\n",
    "#   '2020': 34,\n",
    "#  '2021': 35,\n",
    "\n",
    "#  'UNK': 36\n",
    "\n",
    "# },\n",
    "#  'idx2token': {0: 'PAD',\n",
    "#   1: '1987',\n",
    "#   2: '1988',\n",
    "#   3: '1989',\n",
    "#   4: '1990',\n",
    "#   5: '1991',\n",
    "#   6: '1992',\n",
    "#   7: '1993',\n",
    "#   8: '1994',\n",
    "#   9: '1995',\n",
    "#   10: '1996',\n",
    "#   11: '1997',\n",
    "#   12: '1998',\n",
    "#   13: '1999',\n",
    "#   14: '2000',\n",
    "#   15: '2001',\n",
    "#   16: '2002',\n",
    "#   17: '2003',\n",
    "#   18: '2004',\n",
    "#   19: '2005',\n",
    "#   20: '2006',\n",
    "#   21: '2007',\n",
    "#   22: '2008',\n",
    "#   23: '2009',\n",
    "#   24: '2010',\n",
    "#   25: '2011',\n",
    "#   26: '2012',\n",
    "#   27: '2013',\n",
    "#   28: '2014',\n",
    "#   29: '2015',\n",
    "#                30: '2016',\n",
    "#   31: '2017',\n",
    "#   32: '2018',\n",
    "#   33: '2019',\n",
    "#   34: '2020',\n",
    "#   35: '2021',\n",
    "\n",
    "#   36: 'UNK'}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = utils.load_obj(file_config['vocab'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# olddic = os.path.join('/gpfs3/well/rahimi/users/gra027/JNb/ExpHypCancer/ModelBins/',\n",
    "#                                              \"BEHRT_mlm_DMBp.bin\")\n",
    "# dd = torch.load(olddic,  map_location='cpu')\n",
    "# modeld = model.state_dict()\n",
    "# modeld['bert.embeddings.age_embeddings.weight'] = dd['bert.embeddings.age_embeddings.weight']\n",
    "# modeld['bert.embeddings.year_embeddings.weight'][:31] = dd['bert.embeddings.year_embeddings.weight']\n",
    "# modeld['bert.embeddings.posi_embeddings.weight'] = dd['bert.embeddings.posi_embeddings.weight']\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for x in BertVocab['idx2token']:\n",
    "#     if 'bnf' not in BertVocab['idx2token'][x] and 'vtm' not in BertVocab['idx2token'][x] and 'BANDAGE' not in BertVocab['idx2token'][x]:\n",
    "#         if BertVocab['idx2token'][x] in BertVocabold['token2idx']:\n",
    "#             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x]]\n",
    "\n",
    "#             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#             count = count+1\n",
    "#         elif     BertVocab['idx2token'][x][:-1] in BertVocabold['token2idx']:\n",
    "#             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x][:-1]]\n",
    "#             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#             count = count+1\n",
    "\n",
    "#         else:\n",
    "#             for y in BertVocabold['token2idx']:\n",
    "#                 if BertVocab['idx2token'][x][:-1] in y :\n",
    "#                     oldindx = BertVocabold['token2idx'][y]\n",
    "#     #                 print(BertVocab['idx2token'][x], y )\n",
    "#                     modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#                     count = count+1\n",
    "#                     break\n",
    "#     else:\n",
    "#         if 'bnf' in BertVocab['idx2token'][x]:\n",
    "            \n",
    "#             text2check = BertVocab['idx2token'][x][4:8]\n",
    "#             if text2check in BertVocabold['token2idx']:\n",
    "#                 oldindx = BertVocabold['token2idx'][text2check]\n",
    "#                 modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#                 count = count+1\n",
    "\n",
    "#             else:\n",
    "#                 for y in BertVocabold['token2idx']:\n",
    "#                     if text2check in y :\n",
    "# #                         print(y, text2check)\n",
    "#                         count = count+1\n",
    "#                         oldindx = BertVocabold['token2idx'][y]\n",
    "#                         modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "\n",
    "#                         break\n",
    "                \n",
    "                \n",
    "# for x in modeld:\n",
    "#     if x not in ['cls.predictions.bias','cls.predictions.decoder.weight','bert.embeddings.segment_embeddings.weight' , 'bert.embeddings.word_embeddings.weight','bert.embeddings.age_embeddings.weight', 'bert.embeddings.year_embeddings.weight', 'bert.embeddings.posi_embeddings.weight']:\n",
    "#         if x in dd:\n",
    "#             print(x)\n",
    "#             modeld[x] = dd[x]\n",
    "# model.load_state_dict(modeld)\n",
    "# print(count, len(BertVocab['idx2token']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch0\n",
      "to__save:  /gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/SavedModels/MLM_CEHR_newcut1985_2020_DMProc__6msummary.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/rahimi/users/gra027/JNb/Graph/ModelPkg/utils.py:606: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  truepred = logs(torch.tensor(truepred))\n",
      "/gpfs3/well/rahimi/users/gra027/JNb/Graph/ModelPkg/pytorch_pretrained_bert/optimization.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378065146/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 0.010172133445739745\t| precision: 0.5723888314374354\n",
      "epoch: 0\t| Loss: 2.174755448102951\t| precision: 0.5354245538128718\n",
      "epoch: 0\t| Loss: 2.1708629274368287\t| precision: 0.5443288756810302\n",
      "epoch: 0\t| Loss: 2.1605268961191175\t| precision: 0.5711135611907387\n",
      "epoch: 0\t| Loss: 2.1545482194423675\t| precision: 0.5560415527610716\n",
      "epoch: 0\t| Loss: 2.1678749150037766\t| precision: 0.5625958099131323\n",
      "epoch: 0\t| Loss: 2.1606140679121015\t| precision: 0.5357357357357357\n",
      "epoch: 0\t| Loss: 2.147444821000099\t| precision: 0.5773684210526315\n",
      "epoch: 0\t| Loss: 2.1521757966279984\t| precision: 0.5891980360065466\n",
      "epoch: 0\t| Loss: 2.158366144895554\t| precision: 0.5553349875930521\n",
      "epoch: 0\t| Loss: 2.1506946206092836\t| precision: 0.5561497326203209\n",
      "epoch: 0\t| Loss: 2.1578025728464127\t| precision: 0.579328165374677\n",
      "epoch: 0\t| Loss: 2.1464326179027555\t| precision: 0.5790005047955578\n",
      "epoch: 0\t| Loss: 2.151942965388298\t| precision: 0.5421952564809708\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.147830954194069\t| precision: 0.5401647785787848\n",
      "epoch: 0\t| Loss: 2.1426430571079256\t| precision: 0.5567708333333333\n",
      "epoch: 0\t| Loss: 2.155066742300987\t| precision: 0.5404814004376368\n",
      "epoch: 0\t| Loss: 2.1536792451143265\t| precision: 0.5872184389732844\n",
      "epoch: 0\t| Loss: 2.152291103005409\t| precision: 0.5416004239533652\n",
      "epoch: 0\t| Loss: 2.150382513999939\t| precision: 0.5651302605210421\n",
      "epoch: 0\t| Loss: 2.146186066865921\t| precision: 0.5613015688553167\n",
      "epoch: 0\t| Loss: 2.1512133157253266\t| precision: 0.5323925593329057\n",
      "epoch: 0\t| Loss: 2.14960582613945\t| precision: 0.5712707182320442\n",
      "epoch: 0\t| Loss: 2.139287289381027\t| precision: 0.5442430703624733\n",
      "epoch: 0\t| Loss: 2.1503110271692276\t| precision: 0.5497560975609757\n",
      "epoch: 0\t| Loss: 2.1517100965976717\t| precision: 0.5839532412327312\n",
      "epoch: 0\t| Loss: 2.1481224793195723\t| precision: 0.5579800498753117\n",
      "epoch: 0\t| Loss: 2.154742283821106\t| precision: 0.5281214848143982\n",
      "epoch: 0\t| Loss: 2.1449681049585343\t| precision: 0.5616580310880829\n",
      "mid epoch: 887852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1467358857393264\t| precision: 0.5414141414141415\n",
      "epoch: 0\t| Loss: 2.1494027829170226\t| precision: 0.5243714725500257\n",
      "epoch: 0\t| Loss: 2.1365642774105074\t| precision: 0.5413290113452188\n",
      "epoch: 0\t| Loss: 2.1477417975664137\t| precision: 0.5700049825610364\n",
      "epoch: 0\t| Loss: 2.147615064382553\t| precision: 0.5646670335718217\n",
      "epoch: 0\t| Loss: 2.1561135721206663\t| precision: 0.5484045429962142\n",
      "epoch: 0\t| Loss: 2.157530704140663\t| precision: 0.559768299104792\n",
      "epoch: 0\t| Loss: 2.1418529033660887\t| precision: 0.5472868217054263\n",
      "epoch: 0\t| Loss: 2.1578867906332015\t| precision: 0.5756048387096774\n",
      "epoch: 0\t| Loss: 2.1556477612257003\t| precision: 0.562625250501002\n",
      "epoch: 0\t| Loss: 2.158708170056343\t| precision: 0.5711412770236299\n",
      "epoch: 0\t| Loss: 2.1515909987688064\t| precision: 0.5641025641025641\n",
      "epoch: 0\t| Loss: 2.1515088933706283\t| precision: 0.5661605206073753\n",
      "epoch: 0\t| Loss: 2.152460227012634\t| precision: 0.570919881305638\n",
      "epoch: 0\t| Loss: 2.151948392391205\t| precision: 0.5381927109335995\n",
      "mid epoch: 1331852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.150857250094414\t| precision: 0.532093023255814\n",
      "epoch: 0\t| Loss: 2.1446024537086488\t| precision: 0.5367581930912312\n",
      "epoch: 0\t| Loss: 2.152634544968605\t| precision: 0.5548642533936652\n",
      "epoch: 0\t| Loss: 2.1529863548278807\t| precision: 0.5517993456924755\n",
      "epoch: 0\t| Loss: 2.147453433871269\t| precision: 0.5397672162948594\n",
      "epoch: 0\t| Loss: 2.1466006094217303\t| precision: 0.5900587920897915\n",
      "epoch: 0\t| Loss: 2.1558794605731966\t| precision: 0.5773252614199229\n",
      "epoch: 0\t| Loss: 2.1533599746227265\t| precision: 0.5824915824915825\n",
      "epoch: 0\t| Loss: 2.1489610373973846\t| precision: 0.5773751224289911\n",
      "epoch: 0\t| Loss: 2.151613101363182\t| precision: 0.5723393820500245\n",
      "epoch: 0\t| Loss: 2.1454345178604126\t| precision: 0.5295603840323395\n",
      "epoch: 0\t| Loss: 2.143877347111702\t| precision: 0.5623342175066313\n",
      "epoch: 0\t| Loss: 2.145250980257988\t| precision: 0.5589189189189189\n",
      "epoch: 0\t| Loss: 2.1511938261985777\t| precision: 0.5580448065173116\n",
      "epoch: 0\t| Loss: 2.1480983275175096\t| precision: 0.5666666666666667\n",
      "mid epoch: 1775852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1533500438928606\t| precision: 0.5384615384615384\n",
      "epoch: 0\t| Loss: 2.1538837438821794\t| precision: 0.5820499203398831\n",
      "epoch: 0\t| Loss: 2.1425451427698134\t| precision: 0.570005534034311\n",
      "epoch: 0\t| Loss: 2.1530633074045182\t| precision: 0.5624315443592552\n",
      "epoch: 0\t| Loss: 2.1404132860898972\t| precision: 0.5529145444255801\n",
      "epoch: 0\t| Loss: 2.146706262230873\t| precision: 0.5511904761904762\n",
      "epoch: 0\t| Loss: 2.1521630728244783\t| precision: 0.5848056537102474\n",
      "epoch: 0\t| Loss: 2.150078516602516\t| precision: 0.5590062111801242\n",
      "epoch: 0\t| Loss: 2.1429140090942385\t| precision: 0.5434266729947793\n",
      "epoch: 0\t| Loss: 2.151140453219414\t| precision: 0.5538720538720538\n",
      "epoch: 0\t| Loss: 2.1423224353790284\t| precision: 0.5653969710876549\n",
      "epoch: 0\t| Loss: 2.1514305770397186\t| precision: 0.5461825947677522\n",
      "epoch: 0\t| Loss: 2.148405888080597\t| precision: 0.5597051597051597\n",
      "epoch: 0\t| Loss: 2.13716660797596\t| precision: 0.566579634464752\n",
      "epoch: 0\t| Loss: 2.1464299523830412\t| precision: 0.5459800315291645\n",
      "mid epoch: 2219852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.146348602771759\t| precision: 0.5690834473324213\n",
      "epoch: 0\t| Loss: 2.1514779889583586\t| precision: 0.5854765506807866\n",
      "epoch: 0\t| Loss: 2.1465253692865374\t| precision: 0.5741444866920152\n",
      "epoch: 0\t| Loss: 2.1504850792884826\t| precision: 0.5617760617760618\n",
      "epoch: 0\t| Loss: 2.1495470070838927\t| precision: 0.5388548057259713\n",
      "epoch: 0\t| Loss: 2.1403551322221754\t| precision: 0.5444676409185804\n",
      "epoch: 0\t| Loss: 2.1486848533153533\t| precision: 0.5802348336594912\n",
      "epoch: 0\t| Loss: 2.1487519270181656\t| precision: 0.5476543209876543\n",
      "epoch: 0\t| Loss: 2.1461200314760207\t| precision: 0.5569084171519323\n",
      "epoch: 0\t| Loss: 2.143964018821716\t| precision: 0.5500267522739433\n",
      "epoch: 0\t| Loss: 2.144913055896759\t| precision: 0.5559980089596814\n",
      "epoch: 0\t| Loss: 2.1419487017393113\t| precision: 0.5718278367803242\n",
      "epoch: 0\t| Loss: 2.14502017557621\t| precision: 0.554857419043016\n",
      "epoch: 0\t| Loss: 2.1435026812553404\t| precision: 0.5464684014869888\n",
      "epoch: 0\t| Loss: 2.154127598404884\t| precision: 0.5414364640883977\n",
      "mid epoch: 2663852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.139180547595024\t| precision: 0.568075117370892\n",
      "epoch: 0\t| Loss: 2.1540371495485306\t| precision: 0.5682501503307276\n",
      "epoch: 0\t| Loss: 2.137095425128937\t| precision: 0.556936847339632\n",
      "epoch: 0\t| Loss: 2.1449783635139466\t| precision: 0.5482849604221636\n",
      "epoch: 0\t| Loss: 2.1561648190021514\t| precision: 0.5466981132075471\n",
      "epoch: 0\t| Loss: 2.1576389878988267\t| precision: 0.5504132231404959\n",
      "epoch: 0\t| Loss: 2.147970117330551\t| precision: 0.573754316724223\n",
      "epoch: 0\t| Loss: 2.154603269100189\t| precision: 0.5559511698880977\n",
      "epoch: 0\t| Loss: 2.142164742946625\t| precision: 0.5637480798771122\n",
      "epoch: 0\t| Loss: 2.1511326456069946\t| precision: 0.5608604407135362\n",
      "epoch: 0\t| Loss: 2.1456650358438494\t| precision: 0.5562162162162162\n",
      "epoch: 0\t| Loss: 2.147587520480156\t| precision: 0.5169128156264888\n",
      "epoch: 0\t| Loss: 2.146604765057564\t| precision: 0.55\n",
      "epoch: 0\t| Loss: 2.1488974571228026\t| precision: 0.5510297482837528\n",
      "epoch: 0\t| Loss: 2.156572799682617\t| precision: 0.5718407386035776\n",
      "mid epoch: 3107852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1408058869838715\t| precision: 0.5533078184800437\n",
      "epoch: 0\t| Loss: 2.1513440257310865\t| precision: 0.5600847008999471\n",
      "epoch: 0\t| Loss: 2.1502657639980316\t| precision: 0.5768667642752562\n",
      "epoch: 0\t| Loss: 2.144719035029411\t| precision: 0.5311135371179039\n",
      "epoch: 0\t| Loss: 2.1475650280714036\t| precision: 0.5586652314316469\n",
      "epoch: 0\t| Loss: 2.143442787528038\t| precision: 0.5573521980798383\n",
      "epoch: 0\t| Loss: 2.1435478746891024\t| precision: 0.5434782608695652\n",
      "epoch: 0\t| Loss: 2.1491062903404234\t| precision: 0.5547087642896026\n",
      "epoch: 0\t| Loss: 2.150073813199997\t| precision: 0.5436344969199178\n",
      "epoch: 0\t| Loss: 2.161980926990509\t| precision: 0.547191011235955\n",
      "epoch: 0\t| Loss: 2.1500399231910707\t| precision: 0.5467020805666224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.1556178337335585\t| precision: 0.5534132360604481\n",
      "epoch: 0\t| Loss: 2.1560003119707107\t| precision: 0.5441092771770063\n",
      "epoch: 0\t| Loss: 2.1502130883932113\t| precision: 0.5461689587426326\n",
      "epoch: 0\t| Loss: 2.1445269030332565\t| precision: 0.5526742301458671\n",
      "mid epoch: 3551852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1439756220579147\t| precision: 0.5564005069708492\n",
      "epoch: 0\t| Loss: 2.150937961935997\t| precision: 0.5694037145650049\n",
      "epoch: 0\t| Loss: 2.1544381481409074\t| precision: 0.5503957783641161\n",
      "epoch: 0\t| Loss: 2.13604289829731\t| precision: 0.5766614338042909\n",
      "epoch: 0\t| Loss: 2.1435182350873947\t| precision: 0.5413456599897278\n",
      "epoch: 0\t| Loss: 2.1544006985425947\t| precision: 0.5459940652818991\n",
      "epoch: 0\t| Loss: 2.1419893527030944\t| precision: 0.5551431601272534\n",
      "epoch: 0\t| Loss: 2.141147780418396\t| precision: 0.5280624694973157\n",
      "epoch: 0\t| Loss: 2.1564292770624163\t| precision: 0.5479318734793187\n",
      "epoch: 0\t| Loss: 2.1534647971391676\t| precision: 0.5499247365780231\n",
      "epoch: 0\t| Loss: 2.147895917892456\t| precision: 0.5564304461942258\n",
      "epoch: 0\t| Loss: 2.146681430339813\t| precision: 0.53125\n",
      "epoch: 0\t| Loss: 2.151091794371605\t| precision: 0.5536692223439211\n",
      "epoch: 0\t| Loss: 2.14917986035347\t| precision: 0.5547945205479452\n",
      "epoch: 0\t| Loss: 2.144016874432564\t| precision: 0.5723306544202067\n",
      "mid epoch: 3995852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1477239215373993\t| precision: 0.5334655428854734\n",
      "epoch: 0\t| Loss: 2.153138149380684\t| precision: 0.5663716814159292\n",
      "epoch: 0\t| Loss: 2.1511532926559447\t| precision: 0.5506367495451789\n",
      "epoch: 0\t| Loss: 2.1535542684793474\t| precision: 0.5444839857651246\n",
      "epoch: 0\t| Loss: 2.140671737790108\t| precision: 0.5625\n",
      "epoch: 0\t| Loss: 2.1349784117937087\t| precision: 0.5583705911574764\n",
      "epoch: 0\t| Loss: 2.147743409872055\t| precision: 0.5498470948012233\n",
      "epoch: 0\t| Loss: 2.1481952142715453\t| precision: 0.5651397977394408\n",
      "epoch: 0\t| Loss: 2.13970334649086\t| precision: 0.5682095006090134\n",
      "epoch: 0\t| Loss: 2.147261129021645\t| precision: 0.5718015665796344\n",
      "epoch: 0\t| Loss: 2.1507633048295975\t| precision: 0.562910560889338\n",
      "epoch: 0\t| Loss: 2.146977491378784\t| precision: 0.5779554540262707\n",
      "epoch: 0\t| Loss: 2.14160961329937\t| precision: 0.5760571114772103\n",
      "epoch: 0\t| Loss: 2.1443740195035934\t| precision: 0.5559341283361726\n",
      "epoch: 0\t| Loss: 2.1429020845890046\t| precision: 0.5552407932011332\n",
      "mid epoch: 4439852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.14752217233181\t| precision: 0.5672016048144434\n",
      "epoch: 0\t| Loss: 2.1554152584075927\t| precision: 0.566440854611777\n",
      "epoch: 0\t| Loss: 2.1450701558589937\t| precision: 0.5698808234019501\n",
      "epoch: 0\t| Loss: 2.15141474545002\t| precision: 0.5659500290528763\n",
      "epoch: 0\t| Loss: 2.141691179871559\t| precision: 0.5396356474643033\n",
      "epoch: 0\t| Loss: 2.1575976622104647\t| precision: 0.5142995637421232\n",
      "epoch: 0\t| Loss: 2.1537379962205887\t| precision: 0.5525197328476017\n",
      "epoch: 0\t| Loss: 2.145536791682243\t| precision: 0.5753490611458835\n",
      "epoch: 0\t| Loss: 2.150195133090019\t| precision: 0.5401146131805158\n",
      "epoch: 0\t| Loss: 2.139562485218048\t| precision: 0.5412946428571429\n",
      "epoch: 0\t| Loss: 2.1502030420303346\t| precision: 0.5525821596244131\n",
      "epoch: 0\t| Loss: 2.1425622421503068\t| precision: 0.5479262672811059\n",
      "epoch: 0\t| Loss: 2.1548469144105913\t| precision: 0.532017075773746\n",
      "epoch: 0\t| Loss: 2.1473502951860426\t| precision: 0.5554365291912159\n",
      "epoch: 0\t| Loss: 2.1436408507823943\t| precision: 0.5757428153921091\n",
      "mid epoch: 4883852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.155392217636108\t| precision: 0.5538057742782152\n",
      "epoch: 0\t| Loss: 2.148285652399063\t| precision: 0.5350072428778367\n",
      "epoch: 0\t| Loss: 2.1517145442962646\t| precision: 0.5366506153023007\n",
      "epoch: 0\t| Loss: 2.1550423991680145\t| precision: 0.5507246376811594\n",
      "epoch: 0\t| Loss: 2.1575745540857314\t| precision: 0.5621567648527209\n",
      "epoch: 0\t| Loss: 2.1458094149827955\t| precision: 0.5670945157526255\n",
      "epoch: 0\t| Loss: 2.1369210296869277\t| precision: 0.5504950495049505\n",
      "epoch: 0\t| Loss: 2.1539904081821444\t| precision: 0.554858934169279\n",
      "epoch: 0\t| Loss: 2.1396405333280564\t| precision: 0.5390702274975272\n",
      "epoch: 0\t| Loss: 2.1431514084339143\t| precision: 0.5736813485589994\n",
      "epoch: 0\t| Loss: 2.140035957098007\t| precision: 0.5745341614906833\n",
      "epoch: 0\t| Loss: 2.144436866044998\t| precision: 0.568242640499554\n",
      "epoch: 0\t| Loss: 2.149137706756592\t| precision: 0.5392749244712991\n",
      "epoch: 0\t| Loss: 2.1440754371881483\t| precision: 0.5490970654627539\n",
      "epoch: 0\t| Loss: 2.153702893257141\t| precision: 0.5699821322215605\n",
      "mid epoch: 5327852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.138283585906029\t| precision: 0.57035848047084\n",
      "epoch: 0\t| Loss: 2.159737094640732\t| precision: 0.5801566579634465\n",
      "epoch: 0\t| Loss: 2.147607282996178\t| precision: 0.5611985779583545\n",
      "epoch: 0\t| Loss: 2.14204432785511\t| precision: 0.5592011412268189\n",
      "epoch: 0\t| Loss: 2.1479953575134276\t| precision: 0.5744255744255744\n",
      "epoch: 0\t| Loss: 2.1484112536907194\t| precision: 0.5286555972147831\n",
      "epoch: 0\t| Loss: 2.1461046993732453\t| precision: 0.5626975763962065\n",
      "epoch: 0\t| Loss: 2.1423075300455094\t| precision: 0.5652573529411765\n",
      "epoch: 0\t| Loss: 2.1503537344932555\t| precision: 0.5481586402266289\n",
      "epoch: 0\t| Loss: 2.150938616991043\t| precision: 0.5574776785714286\n",
      "epoch: 0\t| Loss: 2.1523077023029327\t| precision: 0.5633874239350912\n",
      "epoch: 0\t| Loss: 2.146110023856163\t| precision: 0.5449134199134199\n",
      "epoch: 0\t| Loss: 2.1469082778692243\t| precision: 0.5527980535279805\n",
      "epoch: 0\t| Loss: 2.1361352121829986\t| precision: 0.5561959654178674\n",
      "epoch: 0\t| Loss: 2.1506241357326505\t| precision: 0.5647464303298868\n",
      "mid epoch: 5771852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1380883622169495\t| precision: 0.5880728879072336\n",
      "epoch: 0\t| Loss: 2.1585969161987304\t| precision: 0.5563872255489022\n",
      "epoch: 0\t| Loss: 2.149577152132988\t| precision: 0.5332623736029803\n",
      "epoch: 0\t| Loss: 2.143757442831993\t| precision: 0.5640648011782032\n",
      "epoch: 0\t| Loss: 2.1514644277095796\t| precision: 0.5558118081180812\n",
      "epoch: 0\t| Loss: 2.156217482089996\t| precision: 0.5416233090530698\n",
      "epoch: 0\t| Loss: 2.1452594739198685\t| precision: 0.550694801852805\n",
      "epoch: 0\t| Loss: 2.1434869074821474\t| precision: 0.5584352078239608\n",
      "epoch: 0\t| Loss: 2.150684515833855\t| precision: 0.5685230024213075\n",
      "epoch: 0\t| Loss: 2.1456947261095047\t| precision: 0.573080967402734\n",
      "epoch: 0\t| Loss: 2.1413958823680876\t| precision: 0.5391908975979772\n",
      "epoch: 0\t| Loss: 2.151691569685936\t| precision: 0.5527123848515865\n",
      "epoch: 0\t| Loss: 2.1555347454547884\t| precision: 0.5716455696202531\n",
      "epoch: 0\t| Loss: 2.1363313245773314\t| precision: 0.5677710843373494\n",
      "epoch: 0\t| Loss: 2.149265923500061\t| precision: 0.5415549597855228\n",
      "mid epoch: 6215852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.156404894590378\t| precision: 0.5626856803327391\n",
      "epoch: 0\t| Loss: 2.1564982306957243\t| precision: 0.5363545816733067\n",
      "epoch: 0\t| Loss: 2.140043676495552\t| precision: 0.5577264653641207\n",
      "epoch: 0\t| Loss: 2.1520348918437957\t| precision: 0.5584556824361065\n",
      "epoch: 0\t| Loss: 2.151826934814453\t| precision: 0.5459893048128343\n",
      "epoch: 0\t| Loss: 2.1436259859800337\t| precision: 0.5541368743615934\n",
      "epoch: 0\t| Loss: 2.1543962502479554\t| precision: 0.5525291828793775\n",
      "epoch: 0\t| Loss: 2.147281037569046\t| precision: 0.5631205673758866\n",
      "epoch: 0\t| Loss: 2.1579126507043838\t| precision: 0.5455635491606715\n",
      "epoch: 0\t| Loss: 2.1464930242300033\t| precision: 0.5701233439926907\n",
      "epoch: 0\t| Loss: 2.1549079990386963\t| precision: 0.573536645351697\n",
      "epoch: 0\t| Loss: 2.1543033772706988\t| precision: 0.5558455114822547\n",
      "epoch: 0\t| Loss: 2.1445275539159776\t| precision: 0.5625292740046839\n",
      "epoch: 0\t| Loss: 2.1486925357580184\t| precision: 0.5673127291775799\n",
      "epoch: 0\t| Loss: 2.151908881664276\t| precision: 0.5556675062972293\n",
      "mid epoch: 6659852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.141935038566589\t| precision: 0.5634692705341757\n",
      "epoch: 0\t| Loss: 2.1434558737277984\t| precision: 0.5543859649122806\n",
      "epoch: 0\t| Loss: 2.1497880065441133\t| precision: 0.5412979351032449\n",
      "epoch: 0\t| Loss: 2.1486308932304383\t| precision: 0.5573372206025268\n",
      "epoch: 0\t| Loss: 2.152219552397728\t| precision: 0.5744186046511628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.142249013185501\t| precision: 0.5598690364826941\n",
      "epoch: 0\t| Loss: 2.1535888946056367\t| precision: 0.5739549839228296\n",
      "epoch: 0\t| Loss: 2.149383807182312\t| precision: 0.5581668625146886\n",
      "epoch: 0\t| Loss: 2.155576659440994\t| precision: 0.5431499460625674\n",
      "epoch: 0\t| Loss: 2.1481249356269836\t| precision: 0.553030303030303\n",
      "epoch: 0\t| Loss: 2.1578116315603255\t| precision: 0.5989367985823981\n",
      "epoch: 0\t| Loss: 2.150070765018463\t| precision: 0.5699904122722914\n",
      "epoch: 0\t| Loss: 2.1351132023334505\t| precision: 0.5697412823397076\n",
      "epoch: 0\t| Loss: 2.1495276576280595\t| precision: 0.567654986522911\n",
      "epoch: 0\t| Loss: 2.142834809422493\t| precision: 0.541349568746829\n",
      "mid epoch: 7103852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1521626192331316\t| precision: 0.5661016949152542\n",
      "epoch: 0\t| Loss: 2.1373698902130127\t| precision: 0.5617183251767265\n",
      "epoch: 0\t| Loss: 2.1332312750816347\t| precision: 0.555440948942754\n",
      "epoch: 0\t| Loss: 2.137259023785591\t| precision: 0.5261064147190453\n",
      "epoch: 0\t| Loss: 2.1409328305721282\t| precision: 0.5654936461388075\n",
      "epoch: 0\t| Loss: 2.1415035688877104\t| precision: 0.5813720580871307\n",
      "epoch: 0\t| Loss: 2.1569056767225265\t| precision: 0.569164265129683\n",
      "epoch: 0\t| Loss: 2.1534876412153245\t| precision: 0.5743207245604688\n",
      "epoch: 0\t| Loss: 2.143341863155365\t| precision: 0.5336585365853659\n",
      "epoch: 0\t| Loss: 2.1495082169771194\t| precision: 0.5602179836512261\n",
      "epoch: 0\t| Loss: 2.1508308589458465\t| precision: 0.5675070028011204\n",
      "epoch: 0\t| Loss: 2.1436009007692336\t| precision: 0.5600220264317181\n",
      "epoch: 0\t| Loss: 2.1450887817144393\t| precision: 0.5741399762752076\n",
      "epoch: 0\t| Loss: 2.144890542626381\t| precision: 0.5607385811467445\n",
      "epoch: 0\t| Loss: 2.1531563049554823\t| precision: 0.5690900337024555\n",
      "mid epoch: 7547852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1529182994365694\t| precision: 0.5652883569096845\n",
      "epoch: 0\t| Loss: 2.140945613384247\t| precision: 0.5512882839086047\n",
      "epoch: 0\t| Loss: 2.134759567975998\t| precision: 0.5435791645177926\n",
      "epoch: 0\t| Loss: 2.1468734461069108\t| precision: 0.5525866969869244\n",
      "epoch: 0\t| Loss: 2.148504143357277\t| precision: 0.5416453755748595\n",
      "epoch: 0\t| Loss: 2.149231348633766\t| precision: 0.5491118077324973\n",
      "epoch: 0\t| Loss: 2.1515977787971496\t| precision: 0.543254688445251\n",
      "epoch: 0\t| Loss: 2.1494566708803178\t| precision: 0.5525439847836424\n",
      "epoch: 0\t| Loss: 2.1448115569353106\t| precision: 0.5600217273221075\n",
      "epoch: 0\t| Loss: 2.1464905709028246\t| precision: 0.5371819960861057\n",
      "epoch: 0\t| Loss: 2.1403215390443804\t| precision: 0.5505857294994675\n",
      "epoch: 0\t| Loss: 2.144366534948349\t| precision: 0.5470470470470471\n",
      "epoch: 0\t| Loss: 2.1520615470409394\t| precision: 0.5456919060052219\n",
      "epoch: 0\t| Loss: 2.1466921520233155\t| precision: 0.5475773496789259\n",
      "epoch: 0\t| Loss: 2.1387713354825975\t| precision: 0.5809334657398213\n",
      "mid epoch: 7991852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1477460980415346\t| precision: 0.5754956786985257\n",
      "epoch: 0\t| Loss: 2.1423611325025558\t| precision: 0.5405565423327413\n",
      "epoch: 0\t| Loss: 2.1482328206300734\t| precision: 0.5621679064824655\n",
      "epoch: 0\t| Loss: 2.142960072159767\t| precision: 0.5711060948081265\n",
      "epoch: 0\t| Loss: 2.1519526594877245\t| precision: 0.5281995661605207\n",
      "epoch: 0\t| Loss: 2.1381593638658525\t| precision: 0.569620253164557\n",
      "epoch: 0\t| Loss: 2.148132885098457\t| precision: 0.5572192513368984\n",
      "epoch: 0\t| Loss: 2.1358898305892944\t| precision: 0.5819512195121951\n",
      "epoch: 0\t| Loss: 2.1453877121210096\t| precision: 0.5585054080629301\n",
      "epoch: 0\t| Loss: 2.139609802365303\t| precision: 0.5450795521508545\n",
      "epoch: 0\t| Loss: 2.137599601149559\t| precision: 0.5647240845796803\n",
      "epoch: 0\t| Loss: 2.144511299133301\t| precision: 0.573\n",
      "epoch: 0\t| Loss: 2.1503574538230894\t| precision: 0.5654307524536533\n",
      "epoch: 0\t| Loss: 2.1514864325523377\t| precision: 0.5574468085106383\n",
      "epoch: 0\t| Loss: 2.14759808242321\t| precision: 0.5836909871244635\n",
      "mid epoch: 8435852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.149833080172539\t| precision: 0.5584192439862543\n",
      "epoch: 0\t| Loss: 2.147440823316574\t| precision: 0.564844587352626\n",
      "epoch: 0\t| Loss: 2.145170410871506\t| precision: 0.5728105906313645\n",
      "epoch: 0\t| Loss: 2.1565490728616714\t| precision: 0.5705329153605015\n",
      "epoch: 0\t| Loss: 2.1565434378385544\t| precision: 0.5552268244575936\n",
      "epoch: 0\t| Loss: 2.1644214576482774\t| precision: 0.5751738897806313\n",
      "epoch: 0\t| Loss: 2.142896835207939\t| precision: 0.5475\n",
      "epoch: 0\t| Loss: 2.1461908823251723\t| precision: 0.5690890481064483\n",
      "epoch: 0\t| Loss: 2.1428904700279237\t| precision: 0.580352644836272\n",
      "epoch: 0\t| Loss: 2.152415369153023\t| precision: 0.561817387505461\n",
      "epoch: 0\t| Loss: 2.1475630778074266\t| precision: 0.5801838610827375\n",
      "epoch: 0\t| Loss: 2.153831145763397\t| precision: 0.5382613222280063\n",
      "epoch: 0\t| Loss: 2.1443700206279757\t| precision: 0.5596289752650176\n",
      "epoch: 0\t| Loss: 2.1408750039339064\t| precision: 0.5469007131102578\n",
      "epoch: 0\t| Loss: 2.1476699501276015\t| precision: 0.5347159603246168\n",
      "mid epoch: 8879852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1448879939317704\t| precision: 0.5619791666666667\n",
      "epoch: 0\t| Loss: 2.144160795211792\t| precision: 0.5687081832218219\n",
      "epoch: 0\t| Loss: 2.1491516906023027\t| precision: 0.5668367346938775\n",
      "epoch: 0\t| Loss: 2.1512446749210357\t| precision: 0.5380736258194655\n",
      "epoch: 0\t| Loss: 2.137446539402008\t| precision: 0.576657824933687\n",
      "epoch: 0\t| Loss: 2.147597930431366\t| precision: 0.5496688741721855\n",
      "epoch: 0\t| Loss: 2.148804636001587\t| precision: 0.5580082135523614\n",
      "epoch: 0\t| Loss: 2.1482576555013657\t| precision: 0.5679012345679012\n",
      "epoch: 0\t| Loss: 2.1419925510883333\t| precision: 0.5570798628123469\n",
      "epoch: 0\t| Loss: 2.1506890738010407\t| precision: 0.5667621776504298\n",
      "epoch: 0\t| Loss: 2.147636493444443\t| precision: 0.5755324418028727\n",
      "epoch: 0\t| Loss: 2.15114172577858\t| precision: 0.5665950590762621\n",
      "epoch: 0\t| Loss: 2.147883735895157\t| precision: 0.5825147347740668\n",
      "epoch: 0\t| Loss: 2.1571412140131\t| precision: 0.5581395348837209\n",
      "epoch: 0\t| Loss: 2.1450976586341857\t| precision: 0.5574543311726576\n",
      "mid epoch: 9323852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1462467235326765\t| precision: 0.5370453524921419\n",
      "epoch: 0\t| Loss: 2.142283492088318\t| precision: 0.5661036691904484\n",
      "epoch: 0\t| Loss: 2.1402403777837753\t| precision: 0.5689320388349515\n",
      "epoch: 0\t| Loss: 2.145812916755676\t| precision: 0.5437302423603794\n",
      "epoch: 0\t| Loss: 2.145471268296242\t| precision: 0.557336004006009\n",
      "epoch: 0\t| Loss: 2.148107348680496\t| precision: 0.5805882352941176\n",
      "epoch: 0\t| Loss: 2.149721063375473\t| precision: 0.5355839416058394\n",
      "epoch: 0\t| Loss: 2.14544032394886\t| precision: 0.5612298071912455\n",
      "epoch: 0\t| Loss: 2.1458360999822617\t| precision: 0.5571725571725572\n",
      "epoch: 0\t| Loss: 2.1428526926040647\t| precision: 0.5784259768849752\n",
      "epoch: 0\t| Loss: 2.146729485988617\t| precision: 0.5606736007924715\n",
      "epoch: 0\t| Loss: 2.145833777785301\t| precision: 0.5670886075949367\n",
      "epoch: 0\t| Loss: 2.147518361210823\t| precision: 0.5463973799126638\n",
      "epoch: 0\t| Loss: 2.1527669090032577\t| precision: 0.5637209302325581\n",
      "epoch: 0\t| Loss: 2.1512226349115373\t| precision: 0.5348953140578265\n",
      "mid epoch: 9767852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.156336734294891\t| precision: 0.571779744346116\n",
      "epoch: 0\t| Loss: 2.149356375336647\t| precision: 0.5613779718583212\n",
      "epoch: 0\t| Loss: 2.1425631749629974\t| precision: 0.5602733040507565\n",
      "epoch: 0\t| Loss: 2.144051525592804\t| precision: 0.5658995815899581\n",
      "epoch: 0\t| Loss: 2.1391378951072695\t| precision: 0.5510306686777275\n",
      "epoch: 0\t| Loss: 2.1424491786956787\t| precision: 0.5512622359608449\n",
      "epoch: 0\t| Loss: 2.14637224316597\t| precision: 0.5456919060052219\n",
      "epoch: 0\t| Loss: 2.1444667166471483\t| precision: 0.5482791586998088\n",
      "epoch: 0\t| Loss: 2.145096014738083\t| precision: 0.5349364791288567\n",
      "epoch: 0\t| Loss: 2.1478634548187254\t| precision: 0.5350790513833992\n",
      "epoch: 0\t| Loss: 2.142995400428772\t| precision: 0.5513874614594039\n",
      "epoch: 0\t| Loss: 2.138148933649063\t| precision: 0.533502538071066\n",
      "epoch: 0\t| Loss: 2.160284844636917\t| precision: 0.5545350172215844\n",
      "epoch: 0\t| Loss: 2.1414701622724532\t| precision: 0.5696999031945789\n",
      "epoch: 0\t| Loss: 2.1444154024124145\t| precision: 0.5545977011494253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid epoch: 10211852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1432326781749724\t| precision: 0.5608180839612487\n",
      "epoch: 0\t| Loss: 2.150582013130188\t| precision: 0.5653726015171798\n",
      "epoch: 0\t| Loss: 2.1560967910289763\t| precision: 0.5426758938869666\n",
      "epoch: 0\t| Loss: 2.1390407288074496\t| precision: 0.5676599902296043\n",
      "epoch: 0\t| Loss: 2.1454209131002426\t| precision: 0.5719887955182072\n",
      "epoch: 0\t| Loss: 2.145994783639908\t| precision: 0.5446381405176968\n",
      "epoch: 0\t| Loss: 2.143120058774948\t| precision: 0.5417348608837971\n",
      "epoch: 0\t| Loss: 2.1457009130716322\t| precision: 0.5721862109605186\n",
      "epoch: 0\t| Loss: 2.1410961067676544\t| precision: 0.5701844262295082\n",
      "epoch: 0\t| Loss: 2.1440648013353347\t| precision: 0.5365062195781504\n",
      "epoch: 0\t| Loss: 2.1460408121347427\t| precision: 0.5648479427549195\n",
      "epoch: 0\t| Loss: 2.1423108971118925\t| precision: 0.5476772616136919\n",
      "epoch: 0\t| Loss: 2.1442160201072693\t| precision: 0.5549805950840879\n",
      "epoch: 0\t| Loss: 2.1479694950580597\t| precision: 0.5486586007364544\n",
      "epoch: 0\t| Loss: 2.1475710391998293\t| precision: 0.535978835978836\n",
      "mid epoch: 10655852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.138355972766876\t| precision: 0.5325593134780414\n",
      "epoch: 0\t| Loss: 2.1473441022634505\t| precision: 0.5460930640913082\n",
      "epoch: 0\t| Loss: 2.1421768707036972\t| precision: 0.5465903175429464\n",
      "epoch: 0\t| Loss: 2.147184118628502\t| precision: 0.543179587831207\n",
      "epoch: 0\t| Loss: 2.15495339512825\t| precision: 0.5582055650198751\n",
      "epoch: 0\t| Loss: 2.1498422241210937\t| precision: 0.5533558667310478\n",
      "epoch: 0\t| Loss: 2.1406537991762162\t| precision: 0.5635792778649922\n",
      "epoch: 0\t| Loss: 2.1495853024721145\t| precision: 0.5715007579585649\n",
      "epoch: 0\t| Loss: 2.1447316640615464\t| precision: 0.5623145400593472\n",
      "epoch: 0\t| Loss: 2.150755239725113\t| precision: 0.5553121577217963\n",
      "epoch: 0\t| Loss: 2.1397230952978132\t| precision: 0.5420944558521561\n",
      "epoch: 0\t| Loss: 2.1480850374698637\t| precision: 0.5682602267126663\n",
      "epoch: 0\t| Loss: 2.14517716050148\t| precision: 0.5493406093678945\n",
      "epoch: 0\t| Loss: 2.148622257709503\t| precision: 0.5782215882073228\n",
      "epoch: 0\t| Loss: 2.144911013841629\t| precision: 0.5580174927113702\n",
      "mid epoch: 11099852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.14521324634552\t| precision: 0.5448688449148642\n",
      "epoch: 0\t| Loss: 2.140411068201065\t| precision: 0.5667028199566161\n",
      "epoch: 0\t| Loss: 2.142195602655411\t| precision: 0.5465869456900847\n",
      "epoch: 0\t| Loss: 2.147553240060806\t| precision: 0.5380324543610547\n",
      "epoch: 0\t| Loss: 2.1485134041309357\t| precision: 0.5382387022016223\n",
      "epoch: 0\t| Loss: 2.138826230764389\t| precision: 0.5693348365276212\n",
      "epoch: 0\t| Loss: 2.147670939564705\t| precision: 0.5527690700104493\n",
      "epoch: 0\t| Loss: 2.1480083137750627\t| precision: 0.5797511312217195\n",
      "epoch: 0\t| Loss: 2.1385186886787415\t| precision: 0.5558789289871944\n",
      "epoch: 0\t| Loss: 2.1432840567827225\t| precision: 0.5811111111111111\n",
      "epoch: 0\t| Loss: 2.1482739806175233\t| precision: 0.5281058101086443\n",
      "epoch: 0\t| Loss: 2.1392349779605864\t| precision: 0.5740645822655048\n",
      "epoch: 0\t| Loss: 2.1429450088739395\t| precision: 0.5723425196850394\n",
      "epoch: 0\t| Loss: 2.145743225812912\t| precision: 0.5872122762148337\n",
      "epoch: 0\t| Loss: 2.1444022607803346\t| precision: 0.5560693641618497\n",
      "mid epoch: 11543852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.150416513681412\t| precision: 0.5595026642984015\n",
      "epoch: 0\t| Loss: 2.1400644195079805\t| precision: 0.5480116391852571\n",
      "epoch: 0\t| Loss: 2.14674299120903\t| precision: 0.553058024046001\n",
      "epoch: 0\t| Loss: 2.1487689822912217\t| precision: 0.5485372340425532\n",
      "epoch: 0\t| Loss: 2.1446130895614623\t| precision: 0.5486680327868853\n",
      "epoch: 0\t| Loss: 2.139722680449486\t| precision: 0.5452182952182952\n",
      "epoch: 0\t| Loss: 2.142654527425766\t| precision: 0.5398365679264555\n",
      "epoch: 0\t| Loss: 2.1418544453382493\t| precision: 0.5557799091367996\n",
      "epoch: 0\t| Loss: 2.1501283460855483\t| precision: 0.5627009646302251\n",
      "epoch: 0\t| Loss: 2.1434383964538575\t| precision: 0.5525751072961373\n",
      "epoch: 0\t| Loss: 2.155207860469818\t| precision: 0.5368052078117176\n",
      "epoch: 0\t| Loss: 2.1421852141618727\t| precision: 0.5561277033985582\n",
      "epoch: 0\t| Loss: 2.1402453142404556\t| precision: 0.5586490187129165\n",
      "epoch: 0\t| Loss: 2.144038789868355\t| precision: 0.5572320499479708\n",
      "epoch: 0\t| Loss: 2.156411911845207\t| precision: 0.5817745803357314\n",
      "mid epoch: 11987852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.139757134318352\t| precision: 0.5398566005904681\n",
      "epoch: 0\t| Loss: 2.14036258995533\t| precision: 0.5517647058823529\n",
      "epoch: 0\t| Loss: 2.154588620662689\t| precision: 0.5544967242406195\n",
      "epoch: 0\t| Loss: 2.14670763194561\t| precision: 0.5575356415478615\n",
      "epoch: 0\t| Loss: 2.1463283354043963\t| precision: 0.5599334073251943\n",
      "epoch: 0\t| Loss: 2.140283235311508\t| precision: 0.5484031936127745\n",
      "epoch: 0\t| Loss: 2.151627613902092\t| precision: 0.535031847133758\n",
      "epoch: 0\t| Loss: 2.146356880068779\t| precision: 0.5412790697674419\n",
      "epoch: 0\t| Loss: 2.1480065494775773\t| precision: 0.5782586814292904\n",
      "epoch: 0\t| Loss: 2.1502786737680437\t| precision: 0.549618320610687\n",
      "epoch: 0\t| Loss: 2.144492795467377\t| precision: 0.575860397479399\n",
      "epoch: 0\t| Loss: 2.1466748863458633\t| precision: 0.5602314571278275\n",
      "epoch: 0\t| Loss: 2.145490310192108\t| precision: 0.5648340248962656\n",
      "epoch: 0\t| Loss: 2.1431512415409086\t| precision: 0.5786666666666667\n",
      "epoch: 0\t| Loss: 2.1452989602088928\t| precision: 0.5583924349881797\n",
      "mid epoch: 12431852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1507304280996324\t| precision: 0.5670849420849421\n",
      "epoch: 0\t| Loss: 2.1470311635732653\t| precision: 0.5546492659053833\n",
      "epoch: 0\t| Loss: 2.149383821487427\t| precision: 0.5605434140578854\n",
      "epoch: 0\t| Loss: 2.1496746081113813\t| precision: 0.5661290322580645\n",
      "epoch: 0\t| Loss: 2.1400494247674944\t| precision: 0.5383342526199669\n",
      "epoch: 0\t| Loss: 2.1428093272447586\t| precision: 0.5775401069518716\n",
      "epoch: 0\t| Loss: 2.141557756662369\t| precision: 0.5427461139896373\n",
      "epoch: 0\t| Loss: 2.153942428231239\t| precision: 0.5691439322671684\n",
      "epoch: 0\t| Loss: 2.144738634824753\t| precision: 0.5361890694239291\n",
      "epoch: 0\t| Loss: 2.1335136026144026\t| precision: 0.5830721003134797\n",
      "epoch: 0\t| Loss: 2.1406176322698593\t| precision: 0.5677139761646804\n",
      "epoch: 0\t| Loss: 2.1557328140735628\t| precision: 0.5629251700680272\n",
      "epoch: 0\t| Loss: 2.154981098175049\t| precision: 0.5633116883116883\n",
      "epoch: 0\t| Loss: 2.1424708968400954\t| precision: 0.5266599597585513\n",
      "epoch: 0\t| Loss: 2.1502715659141542\t| precision: 0.5637755102040817\n",
      "mid epoch: 12875852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.153290647268295\t| precision: 0.5752439650744735\n",
      "epoch: 0\t| Loss: 2.1449391359090804\t| precision: 0.5579399141630901\n",
      "epoch: 0\t| Loss: 2.1378462326526644\t| precision: 0.5451807228915663\n",
      "epoch: 0\t| Loss: 2.139855760931969\t| precision: 0.5596426694692591\n",
      "epoch: 0\t| Loss: 2.1462455862760543\t| precision: 0.5378071833648393\n",
      "epoch: 0\t| Loss: 2.1493661117553713\t| precision: 0.5576817933296884\n",
      "epoch: 0\t| Loss: 2.1421158474683764\t| precision: 0.5806451612903226\n",
      "epoch: 0\t| Loss: 2.14686170399189\t| precision: 0.562301767104667\n",
      "epoch: 0\t| Loss: 2.136427302956581\t| precision: 0.5520534861509073\n",
      "epoch: 0\t| Loss: 2.146009645462036\t| precision: 0.5680177679067185\n",
      "epoch: 0\t| Loss: 2.1347846561670303\t| precision: 0.5616515837104072\n",
      "epoch: 0\t| Loss: 2.154601868391037\t| precision: 0.5377308707124011\n",
      "epoch: 0\t| Loss: 2.144641087055206\t| precision: 0.54717923115327\n",
      "epoch: 0\t| Loss: 2.140113726258278\t| precision: 0.556745182012848\n",
      "epoch: 0\t| Loss: 2.136943799853325\t| precision: 0.5644067796610169\n",
      "mid epoch: 13319852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1455343145132066\t| precision: 0.5631163708086785\n",
      "epoch: 0\t| Loss: 2.1355604195594786\t| precision: 0.5635987590486039\n",
      "epoch: 0\t| Loss: 2.13992700278759\t| precision: 0.5589155370177268\n",
      "epoch: 0\t| Loss: 2.138396559357643\t| precision: 0.5817629179331307\n",
      "epoch: 0\t| Loss: 2.1480002945661543\t| precision: 0.5540944411237299\n",
      "epoch: 0\t| Loss: 2.1419701009988783\t| precision: 0.5719636169074371\n",
      "epoch: 0\t| Loss: 2.142729287147522\t| precision: 0.5495594713656388\n",
      "epoch: 0\t| Loss: 2.1407701498270035\t| precision: 0.5699132111861138\n",
      "epoch: 0\t| Loss: 2.134076370000839\t| precision: 0.5639913232104121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.146319348216057\t| precision: 0.5529272619751626\n",
      "epoch: 0\t| Loss: 2.1457480442523957\t| precision: 0.5421952564809708\n",
      "epoch: 0\t| Loss: 2.130440545678139\t| precision: 0.5876225490196079\n",
      "epoch: 0\t| Loss: 2.137673450708389\t| precision: 0.5553956834532374\n",
      "epoch: 0\t| Loss: 2.140012107491493\t| precision: 0.5516273849607183\n",
      "epoch: 0\t| Loss: 2.1337796837091445\t| precision: 0.5502070393374742\n",
      "mid epoch: 13763852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1442311835289\t| precision: 0.5393258426966292\n",
      "epoch: 0\t| Loss: 2.1371674239635468\t| precision: 0.5493734335839598\n",
      "epoch: 0\t| Loss: 2.1513101345300676\t| precision: 0.5535381750465549\n",
      "epoch: 0\t| Loss: 2.1479852485656736\t| precision: 0.5533395176252319\n",
      "epoch: 0\t| Loss: 2.14624780356884\t| precision: 0.5408931259407928\n",
      "epoch: 0\t| Loss: 2.1486590510606765\t| precision: 0.5503246753246753\n",
      "epoch: 0\t| Loss: 2.1395035660266877\t| precision: 0.5604345857854233\n",
      "epoch: 0\t| Loss: 2.1513863974809646\t| precision: 0.5536200326619488\n",
      "epoch: 0\t| Loss: 2.1386964464187623\t| precision: 0.5525591634562466\n",
      "epoch: 0\t| Loss: 2.135954351425171\t| precision: 0.5319040326697294\n",
      "epoch: 0\t| Loss: 2.1359875893592832\t| precision: 0.5668759053597296\n",
      "epoch: 0\t| Loss: 2.1440106761455535\t| precision: 0.5384243595940068\n",
      "epoch: 0\t| Loss: 2.1475328397750855\t| precision: 0.5726161369193155\n",
      "epoch: 0\t| Loss: 2.1412522023916245\t| precision: 0.552127162225339\n",
      "epoch: 0\t| Loss: 2.137566953897476\t| precision: 0.5770507349454718\n",
      "mid epoch: 14207852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.147086253166199\t| precision: 0.5685534591194968\n",
      "epoch: 0\t| Loss: 2.134550922513008\t| precision: 0.5575575575575575\n",
      "epoch: 0\t| Loss: 2.142919360399246\t| precision: 0.5341802782819117\n",
      "epoch: 0\t| Loss: 2.141745011806488\t| precision: 0.5526688710439301\n",
      "epoch: 0\t| Loss: 2.1384802931547164\t| precision: 0.5472684085510688\n",
      "epoch: 0\t| Loss: 2.1468661236763\t| precision: 0.5810741687979539\n",
      "epoch: 0\t| Loss: 2.1464636659622194\t| precision: 0.57829373650108\n",
      "epoch: 0\t| Loss: 2.132073157429695\t| precision: 0.5591976516634051\n",
      "epoch: 0\t| Loss: 2.1381277602910997\t| precision: 0.549175667993178\n",
      "epoch: 0\t| Loss: 2.135454308986664\t| precision: 0.5690962099125364\n",
      "epoch: 0\t| Loss: 2.144817135930061\t| precision: 0.5524402907580478\n",
      "epoch: 0\t| Loss: 2.1507271587848664\t| precision: 0.5603917301414582\n",
      "epoch: 0\t| Loss: 2.1478332579135895\t| precision: 0.5440591245025582\n",
      "epoch: 0\t| Loss: 2.1498169076442717\t| precision: 0.571353894406691\n",
      "epoch: 0\t| Loss: 2.1403589880466463\t| precision: 0.546583850931677\n",
      "mid epoch: 14651852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.148561346530914\t| precision: 0.5594309316200092\n",
      "epoch: 0\t| Loss: 2.1449972981214525\t| precision: 0.5476810838978635\n",
      "epoch: 0\t| Loss: 2.1423919934034346\t| precision: 0.5703883495145631\n",
      "epoch: 0\t| Loss: 2.1534965628385545\t| precision: 0.5823045267489712\n",
      "epoch: 0\t| Loss: 2.1280580878257753\t| precision: 0.5513404147698533\n",
      "epoch: 0\t| Loss: 2.140221356153488\t| precision: 0.5732801595214357\n",
      "epoch: 0\t| Loss: 2.13928242623806\t| precision: 0.5595353339787028\n",
      "epoch: 0\t| Loss: 2.1463810056447983\t| precision: 0.5577803203661327\n",
      "epoch: 0\t| Loss: 2.148674967885017\t| precision: 0.5451990632318501\n",
      "epoch: 0\t| Loss: 2.13759036898613\t| precision: 0.5632946379215036\n",
      "epoch: 0\t| Loss: 2.1407109022140505\t| precision: 0.5494912118408881\n",
      "epoch: 0\t| Loss: 2.151592316031456\t| precision: 0.5626223091976517\n",
      "epoch: 0\t| Loss: 2.141173492074013\t| precision: 0.5581787521079258\n",
      "epoch: 0\t| Loss: 2.1454555767774584\t| precision: 0.5624640184225677\n",
      "epoch: 0\t| Loss: 2.1375300920009614\t| precision: 0.5491209927611168\n",
      "mid epoch: 15095852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1467633593082427\t| precision: 0.5762455963764469\n",
      "epoch: 0\t| Loss: 2.1470199000835417\t| precision: 0.5544507575757576\n",
      "epoch: 0\t| Loss: 2.14478519320488\t| precision: 0.5415535035306899\n",
      "epoch: 0\t| Loss: 2.141229572892189\t| precision: 0.5634544497103738\n",
      "epoch: 0\t| Loss: 2.1411712527275086\t| precision: 0.5675422138836773\n",
      "epoch: 0\t| Loss: 2.155269586443901\t| precision: 0.5640561339972838\n",
      "epoch: 0\t| Loss: 2.1359507888555527\t| precision: 0.5474683544303798\n",
      "epoch: 0\t| Loss: 2.1392400497198105\t| precision: 0.5760811532301121\n",
      "epoch: 0\t| Loss: 2.1400585317611696\t| precision: 0.5649072753209701\n",
      "epoch: 0\t| Loss: 2.13883362531662\t| precision: 0.5795275590551181\n",
      "epoch: 0\t| Loss: 2.1372739189863204\t| precision: 0.5764282038085435\n",
      "epoch: 0\t| Loss: 2.123544319272041\t| precision: 0.5523290986085905\n",
      "epoch: 0\t| Loss: 2.134885894060135\t| precision: 0.5660046728971962\n",
      "epoch: 0\t| Loss: 2.142346208691597\t| precision: 0.577259475218659\n",
      "epoch: 0\t| Loss: 2.1444007003307344\t| precision: 0.5524193548387096\n",
      "mid epoch: 15539852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1437443137168883\t| precision: 0.5295039164490861\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "done epoch 0\n",
      "epoch: 1\t| Loss: 0.010146338939666748\t| precision: 0.5821325648414986\n",
      "epoch: 1\t| Loss: 2.141327375769615\t| precision: 0.5486900642609985\n",
      "epoch: 1\t| Loss: 2.147701240181923\t| precision: 0.5689102564102564\n",
      "epoch: 1\t| Loss: 2.1523374354839326\t| precision: 0.5546719681908548\n",
      "epoch: 1\t| Loss: 2.1372360908985137\t| precision: 0.5477308294209703\n",
      "epoch: 1\t| Loss: 2.1474366813898085\t| precision: 0.5583566760037348\n",
      "epoch: 1\t| Loss: 2.144530208706856\t| precision: 0.5627553998832457\n",
      "epoch: 1\t| Loss: 2.1498907268047334\t| precision: 0.5262875536480687\n",
      "epoch: 1\t| Loss: 2.136801693439484\t| precision: 0.5745433117265764\n",
      "epoch: 1\t| Loss: 2.145928864479065\t| precision: 0.561344537815126\n",
      "epoch: 1\t| Loss: 2.1482901108264922\t| precision: 0.5476854433909648\n",
      "epoch: 1\t| Loss: 2.142820464372635\t| precision: 0.5503913894324853\n",
      "epoch: 1\t| Loss: 2.139851142168045\t| precision: 0.569377990430622\n",
      "epoch: 1\t| Loss: 2.136282415986061\t| precision: 0.5640883977900553\n",
      "epoch: 1\t| Loss: 2.135577099919319\t| precision: 0.5470479704797048\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.145499498844147\t| precision: 0.5752161383285302\n",
      "epoch: 1\t| Loss: 2.1444562220573427\t| precision: 0.5376522702104097\n",
      "epoch: 1\t| Loss: 2.142149314880371\t| precision: 0.5665258711721225\n",
      "epoch: 1\t| Loss: 2.136556315422058\t| precision: 0.5538302277432712\n",
      "epoch: 1\t| Loss: 2.134126053452492\t| precision: 0.560459492140266\n",
      "epoch: 1\t| Loss: 2.135573872923851\t| precision: 0.5578512396694215\n",
      "epoch: 1\t| Loss: 2.1379826909303663\t| precision: 0.5554371002132196\n",
      "epoch: 1\t| Loss: 2.133116870522499\t| precision: 0.5602409638554217\n",
      "epoch: 1\t| Loss: 2.144684312939644\t| precision: 0.5789473684210527\n",
      "epoch: 1\t| Loss: 2.1433809077739716\t| precision: 0.5539070227497527\n",
      "epoch: 1\t| Loss: 2.137927867770195\t| precision: 0.5690690690690691\n",
      "epoch: 1\t| Loss: 2.147202605009079\t| precision: 0.5742521367521367\n",
      "epoch: 1\t| Loss: 2.1465789270401\t| precision: 0.576017130620985\n",
      "epoch: 1\t| Loss: 2.1480859112739563\t| precision: 0.5513078470824949\n",
      "epoch: 1\t| Loss: 2.1344618141651153\t| precision: 0.5662525879917184\n",
      "mid epoch: 887852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.145199739933014\t| precision: 0.5279755849440488\n",
      "epoch: 1\t| Loss: 2.146675688624382\t| precision: 0.5538140020898642\n",
      "epoch: 1\t| Loss: 2.1499552726745605\t| precision: 0.5913279132791328\n",
      "epoch: 1\t| Loss: 2.1514521396160124\t| precision: 0.5628169014084508\n",
      "epoch: 1\t| Loss: 2.139189273715019\t| precision: 0.545655783065855\n",
      "epoch: 1\t| Loss: 2.1438202840089797\t| precision: 0.5462686567164179\n",
      "epoch: 1\t| Loss: 2.138161479830742\t| precision: 0.551743853630646\n",
      "epoch: 1\t| Loss: 2.1426512032747267\t| precision: 0.5482431678750698\n",
      "epoch: 1\t| Loss: 2.1524322563409806\t| precision: 0.5487620010106115\n",
      "epoch: 1\t| Loss: 2.1371330350637434\t| precision: 0.5458515283842795\n",
      "epoch: 1\t| Loss: 2.145954185128212\t| precision: 0.5684150513112884\n",
      "epoch: 1\t| Loss: 2.1367473870515825\t| precision: 0.5136268343815513\n",
      "epoch: 1\t| Loss: 2.151631200313568\t| precision: 0.5828571428571429\n",
      "epoch: 1\t| Loss: 2.145952096581459\t| precision: 0.5438180956892468\n",
      "epoch: 1\t| Loss: 2.1413143253326417\t| precision: 0.5570776255707762\n",
      "mid epoch: 1331852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1485609930753706\t| precision: 0.5546517539400102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.138341926932335\t| precision: 0.5604113110539846\n",
      "epoch: 1\t| Loss: 2.1438528549671174\t| precision: 0.5627777777777778\n",
      "epoch: 1\t| Loss: 2.1335747265815734\t| precision: 0.5775173148641449\n",
      "epoch: 1\t| Loss: 2.135724290013313\t| precision: 0.5409015025041736\n",
      "epoch: 1\t| Loss: 2.143988435268402\t| precision: 0.5738605161998902\n",
      "epoch: 1\t| Loss: 2.153408322930336\t| precision: 0.5611729019211324\n",
      "epoch: 1\t| Loss: 2.136853415966034\t| precision: 0.5753268902785673\n",
      "epoch: 1\t| Loss: 2.14315812587738\t| precision: 0.5642746515229736\n",
      "epoch: 1\t| Loss: 2.1410969138145446\t| precision: 0.5513288170922356\n",
      "epoch: 1\t| Loss: 2.143412150144577\t| precision: 0.551604938271605\n",
      "epoch: 1\t| Loss: 2.1409231197834013\t| precision: 0.5545306565382528\n",
      "epoch: 1\t| Loss: 2.136399055123329\t| precision: 0.564795918367347\n",
      "epoch: 1\t| Loss: 2.1460082435607912\t| precision: 0.5288319369147363\n",
      "epoch: 1\t| Loss: 2.153934910297394\t| precision: 0.5650723025583982\n",
      "mid epoch: 1775852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1341634237766267\t| precision: 0.5586130985140341\n",
      "epoch: 1\t| Loss: 2.1397871845960617\t| precision: 0.5418410041841004\n",
      "epoch: 1\t| Loss: 2.141667508482933\t| precision: 0.5539832285115304\n",
      "epoch: 1\t| Loss: 2.154025447368622\t| precision: 0.543046357615894\n",
      "epoch: 1\t| Loss: 2.1521237075328825\t| precision: 0.5441714584422374\n",
      "epoch: 1\t| Loss: 2.1410882198810577\t| precision: 0.5566631689401889\n",
      "epoch: 1\t| Loss: 2.1434564328193666\t| precision: 0.5698134539287734\n",
      "epoch: 1\t| Loss: 2.1462150061130525\t| precision: 0.5387634936211972\n",
      "epoch: 1\t| Loss: 2.1406243413686754\t| precision: 0.5483689538807649\n",
      "epoch: 1\t| Loss: 2.1469883996248247\t| precision: 0.5616686819830713\n",
      "epoch: 1\t| Loss: 2.1317322838306425\t| precision: 0.5336538461538461\n",
      "epoch: 1\t| Loss: 2.1433480912446976\t| precision: 0.5559461042765085\n",
      "epoch: 1\t| Loss: 2.151315298080444\t| precision: 0.5719044975013882\n",
      "epoch: 1\t| Loss: 2.141695985198021\t| precision: 0.563727959697733\n",
      "epoch: 1\t| Loss: 2.1425001049041748\t| precision: 0.5521676989042401\n",
      "mid epoch: 2219852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1431014209985735\t| precision: 0.5556792873051225\n",
      "epoch: 1\t| Loss: 2.140314694046974\t| precision: 0.5528044466902476\n",
      "epoch: 1\t| Loss: 2.1461553925275805\t| precision: 0.5792584631918324\n",
      "epoch: 1\t| Loss: 2.1320182037353517\t| precision: 0.5772921108742004\n",
      "epoch: 1\t| Loss: 2.1425146424770354\t| precision: 0.5726202158979392\n",
      "epoch: 1\t| Loss: 2.1447131812572477\t| precision: 0.5665024630541872\n",
      "epoch: 1\t| Loss: 2.1428699082136156\t| precision: 0.5467741935483871\n",
      "epoch: 1\t| Loss: 2.1491336321830747\t| precision: 0.5593400745077168\n",
      "epoch: 1\t| Loss: 2.144378686547279\t| precision: 0.5716502845318159\n",
      "epoch: 1\t| Loss: 2.1377115440368653\t| precision: 0.5551102204408818\n",
      "epoch: 1\t| Loss: 2.1391939502954482\t| precision: 0.55893536121673\n",
      "epoch: 1\t| Loss: 2.1329610353708266\t| precision: 0.55\n",
      "epoch: 1\t| Loss: 2.1496196031570434\t| precision: 0.572384137601529\n",
      "epoch: 1\t| Loss: 2.1448124569654463\t| precision: 0.5666156202143952\n",
      "epoch: 1\t| Loss: 2.1415334272384645\t| precision: 0.5496422182468694\n",
      "mid epoch: 2663852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.132756141424179\t| precision: 0.5705622932745315\n",
      "epoch: 1\t| Loss: 2.1453637045621874\t| precision: 0.5516613563950842\n",
      "epoch: 1\t| Loss: 2.140641018152237\t| precision: 0.5423819742489271\n",
      "epoch: 1\t| Loss: 2.149263962507248\t| precision: 0.5574636723910171\n",
      "epoch: 1\t| Loss: 2.144144718647003\t| precision: 0.5770975056689343\n",
      "epoch: 1\t| Loss: 2.1432456618547437\t| precision: 0.581592039800995\n",
      "epoch: 1\t| Loss: 2.138619078397751\t| precision: 0.580718508412915\n",
      "epoch: 1\t| Loss: 2.133348044157028\t| precision: 0.557935735150925\n",
      "epoch: 1\t| Loss: 2.139438278675079\t| precision: 0.5381995133819951\n",
      "epoch: 1\t| Loss: 2.135189613103867\t| precision: 0.5554981930820857\n",
      "epoch: 1\t| Loss: 2.1496224999427795\t| precision: 0.5339702760084926\n",
      "epoch: 1\t| Loss: 2.142913907766342\t| precision: 0.566306203756403\n",
      "epoch: 1\t| Loss: 2.138396843671799\t| precision: 0.5707019328585962\n",
      "epoch: 1\t| Loss: 2.141600986123085\t| precision: 0.5893746541228556\n",
      "epoch: 1\t| Loss: 2.1433884572982786\t| precision: 0.5588681152097019\n",
      "mid epoch: 3107852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.143478377461433\t| precision: 0.5380082498526813\n",
      "epoch: 1\t| Loss: 2.147447826266289\t| precision: 0.5616740088105727\n",
      "epoch: 1\t| Loss: 2.14146875500679\t| precision: 0.5621301775147929\n",
      "epoch: 1\t| Loss: 2.143854593038559\t| precision: 0.5440269209197981\n",
      "epoch: 1\t| Loss: 2.1528509384393693\t| precision: 0.5514918190567853\n",
      "epoch: 1\t| Loss: 2.1446004831790924\t| precision: 0.5602716468590832\n",
      "epoch: 1\t| Loss: 2.1368545573949813\t| precision: 0.5460364352535697\n",
      "epoch: 1\t| Loss: 2.1403481954336168\t| precision: 0.5326659641728135\n",
      "epoch: 1\t| Loss: 2.1403172206878662\t| precision: 0.5510204081632653\n",
      "epoch: 1\t| Loss: 2.1492366689443587\t| precision: 0.558891454965358\n",
      "epoch: 1\t| Loss: 2.13870690882206\t| precision: 0.5599803343166175\n",
      "epoch: 1\t| Loss: 2.1390115892887116\t| precision: 0.5536842105263158\n",
      "epoch: 1\t| Loss: 2.14731931746006\t| precision: 0.5413642960812772\n",
      "epoch: 1\t| Loss: 2.1403401070833206\t| precision: 0.5607533414337789\n",
      "epoch: 1\t| Loss: 2.1448611468076706\t| precision: 0.5560344827586207\n",
      "mid epoch: 3551852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1487920516729355\t| precision: 0.574761399787911\n",
      "epoch: 1\t| Loss: 2.1445501548051835\t| precision: 0.5444203683640303\n",
      "epoch: 1\t| Loss: 2.1363759166002274\t| precision: 0.5705426356589147\n",
      "epoch: 1\t| Loss: 2.152409973144531\t| precision: 0.5604155276107162\n",
      "epoch: 1\t| Loss: 2.152938369512558\t| precision: 0.5657894736842105\n",
      "epoch: 1\t| Loss: 2.1428808444738388\t| precision: 0.5717451523545707\n",
      "epoch: 1\t| Loss: 2.147657784819603\t| precision: 0.5388323513658275\n",
      "epoch: 1\t| Loss: 2.142125360369682\t| precision: 0.5613399728383884\n",
      "epoch: 1\t| Loss: 2.134216883778572\t| precision: 0.5537321334039175\n",
      "epoch: 1\t| Loss: 2.1437914252281187\t| precision: 0.5610708310094813\n",
      "epoch: 1\t| Loss: 2.1367507284879683\t| precision: 0.5774722369116869\n",
      "epoch: 1\t| Loss: 2.139951936006546\t| precision: 0.5472222222222223\n",
      "epoch: 1\t| Loss: 2.1469704794883726\t| precision: 0.5554973821989528\n",
      "epoch: 1\t| Loss: 2.1450537353754044\t| precision: 0.536734693877551\n",
      "epoch: 1\t| Loss: 2.1396552872657777\t| precision: 0.5628865979381443\n",
      "mid epoch: 3995852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1486479622125625\t| precision: 0.5585402939685757\n",
      "epoch: 1\t| Loss: 2.145548585653305\t| precision: 0.5754817987152034\n",
      "epoch: 1\t| Loss: 2.1513413709402083\t| precision: 0.5521761929732564\n",
      "epoch: 1\t| Loss: 2.142424336075783\t| precision: 0.5565927654609102\n",
      "epoch: 1\t| Loss: 2.1405426621437074\t| precision: 0.5324324324324324\n",
      "epoch: 1\t| Loss: 2.15417072057724\t| precision: 0.5361083249749248\n",
      "epoch: 1\t| Loss: 2.1357364678382873\t| precision: 0.5431125131440588\n",
      "epoch: 1\t| Loss: 2.132716702222824\t| precision: 0.5609874152952565\n",
      "epoch: 1\t| Loss: 2.1448980176448824\t| precision: 0.5442890442890443\n",
      "epoch: 1\t| Loss: 2.1326784777641294\t| precision: 0.5712401055408971\n",
      "epoch: 1\t| Loss: 2.1344682800769808\t| precision: 0.5361552028218695\n",
      "epoch: 1\t| Loss: 2.1460227209329603\t| precision: 0.5635952515545506\n",
      "epoch: 1\t| Loss: 2.145338459610939\t| precision: 0.574869109947644\n",
      "epoch: 1\t| Loss: 2.135835577249527\t| precision: 0.5594541910331384\n",
      "epoch: 1\t| Loss: 2.1540344274044037\t| precision: 0.5904811715481172\n",
      "mid epoch: 4439852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1393373942375185\t| precision: 0.5727699530516432\n",
      "epoch: 1\t| Loss: 2.1399223011732103\t| precision: 0.5646723646723647\n",
      "epoch: 1\t| Loss: 2.142002938389778\t| precision: 0.5512506130456106\n",
      "epoch: 1\t| Loss: 2.144531310200691\t| precision: 0.5250521920668059\n",
      "epoch: 1\t| Loss: 2.1536078333854674\t| precision: 0.5486586007364544\n",
      "epoch: 1\t| Loss: 2.133997585773468\t| precision: 0.5398445092322643\n",
      "epoch: 1\t| Loss: 2.1447201603651047\t| precision: 0.544069640914037\n",
      "epoch: 1\t| Loss: 2.1402019727230073\t| precision: 0.5351447135262847\n",
      "epoch: 1\t| Loss: 2.1557653588056565\t| precision: 0.5481239804241436\n",
      "epoch: 1\t| Loss: 2.143234460353851\t| precision: 0.5903614457831325\n",
      "epoch: 1\t| Loss: 2.140423240661621\t| precision: 0.5623529411764706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.1371219807863238\t| precision: 0.5400990099009901\n",
      "epoch: 1\t| Loss: 2.1409968411922455\t| precision: 0.5729109776078646\n",
      "epoch: 1\t| Loss: 2.1362065452337267\t| precision: 0.5765199161425576\n",
      "epoch: 1\t| Loss: 2.1376349824666976\t| precision: 0.5749385749385749\n",
      "mid epoch: 4883852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1392205435037615\t| precision: 0.5597638957206099\n",
      "epoch: 1\t| Loss: 2.137467846274376\t| precision: 0.5522935779816514\n",
      "epoch: 1\t| Loss: 2.143376666307449\t| precision: 0.5847193347193347\n",
      "epoch: 1\t| Loss: 2.1385116493701934\t| precision: 0.5541149943630215\n",
      "epoch: 1\t| Loss: 2.1518529403209685\t| precision: 0.5577309236947792\n",
      "epoch: 1\t| Loss: 2.127696000933647\t| precision: 0.5507832238504295\n",
      "epoch: 1\t| Loss: 2.1435559964179993\t| precision: 0.5333011583011583\n",
      "epoch: 1\t| Loss: 2.155397181510925\t| precision: 0.5625338386572821\n",
      "epoch: 1\t| Loss: 2.134338184595108\t| precision: 0.55435847208619\n",
      "epoch: 1\t| Loss: 2.1446720284223555\t| precision: 0.5685745140388769\n",
      "epoch: 1\t| Loss: 2.142973273396492\t| precision: 0.5408388520971302\n",
      "epoch: 1\t| Loss: 2.1448297441005706\t| precision: 0.5668991756499683\n",
      "epoch: 1\t| Loss: 2.1454712098836897\t| precision: 0.5591304347826087\n",
      "epoch: 1\t| Loss: 2.1360149413347242\t| precision: 0.5698587127158555\n",
      "epoch: 1\t| Loss: 2.148715829253197\t| precision: 0.5526450950179764\n",
      "mid epoch: 5327852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.147261914014816\t| precision: 0.5638832997987927\n",
      "epoch: 1\t| Loss: 2.134228531122208\t| precision: 0.5734331150608045\n",
      "epoch: 1\t| Loss: 2.1243177247047424\t| precision: 0.5937976614133198\n",
      "epoch: 1\t| Loss: 2.132930093407631\t| precision: 0.5226570545829042\n",
      "epoch: 1\t| Loss: 2.153049176335335\t| precision: 0.5670538542766631\n",
      "epoch: 1\t| Loss: 2.1382793980836867\t| precision: 0.573394495412844\n",
      "epoch: 1\t| Loss: 2.1548872154951098\t| precision: 0.5564356435643565\n",
      "epoch: 1\t| Loss: 2.143818412423134\t| precision: 0.5510083036773428\n",
      "epoch: 1\t| Loss: 2.1485827881097794\t| precision: 0.551487414187643\n",
      "epoch: 1\t| Loss: 2.14150837123394\t| precision: 0.5782493368700266\n",
      "epoch: 1\t| Loss: 2.1603817313909532\t| precision: 0.5304878048780488\n",
      "epoch: 1\t| Loss: 2.139294549226761\t| precision: 0.5716403162055336\n",
      "epoch: 1\t| Loss: 2.146663562655449\t| precision: 0.5547908946532557\n",
      "epoch: 1\t| Loss: 2.15158746778965\t| precision: 0.5544871794871795\n",
      "epoch: 1\t| Loss: 2.139926724433899\t| precision: 0.5400624349635796\n",
      "mid epoch: 5771852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.134865943193436\t| precision: 0.558311345646438\n",
      "epoch: 1\t| Loss: 2.138237454891205\t| precision: 0.554074074074074\n",
      "epoch: 1\t| Loss: 2.1476652139425276\t| precision: 0.547062179121506\n",
      "epoch: 1\t| Loss: 2.131511529684067\t| precision: 0.5250114731528224\n",
      "epoch: 1\t| Loss: 2.1417441040277483\t| precision: 0.5549597855227882\n",
      "epoch: 1\t| Loss: 2.153103093504906\t| precision: 0.5518867924528302\n",
      "epoch: 1\t| Loss: 2.134253979921341\t| precision: 0.5833333333333334\n",
      "epoch: 1\t| Loss: 2.1400644624233247\t| precision: 0.5774358974358974\n",
      "epoch: 1\t| Loss: 2.1523663556575774\t| precision: 0.548235294117647\n",
      "epoch: 1\t| Loss: 2.1344409132003785\t| precision: 0.5384207737148914\n",
      "epoch: 1\t| Loss: 2.1514166557788847\t| precision: 0.5648044692737431\n",
      "epoch: 1\t| Loss: 2.1339106303453446\t| precision: 0.5273109243697479\n",
      "epoch: 1\t| Loss: 2.1469891357421873\t| precision: 0.5588235294117647\n",
      "epoch: 1\t| Loss: 2.147142000198364\t| precision: 0.556924882629108\n",
      "epoch: 1\t| Loss: 2.141304615736008\t| precision: 0.5539929683576093\n",
      "mid epoch: 6215852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1336852824687957\t| precision: 0.5779283639883833\n",
      "epoch: 1\t| Loss: 2.143123195171356\t| precision: 0.5794074793589121\n",
      "epoch: 1\t| Loss: 2.1380333924293518\t| precision: 0.5510526315789473\n",
      "epoch: 1\t| Loss: 2.135066618323326\t| precision: 0.5816876122082585\n",
      "epoch: 1\t| Loss: 2.13700228869915\t| precision: 0.5725469728601252\n",
      "epoch: 1\t| Loss: 2.131263042092323\t| precision: 0.556989247311828\n",
      "epoch: 1\t| Loss: 2.138003408908844\t| precision: 0.5613036730470771\n",
      "epoch: 1\t| Loss: 2.1454227006435396\t| precision: 0.5409556313993175\n",
      "epoch: 1\t| Loss: 2.1368215000629425\t| precision: 0.5696455317024464\n",
      "epoch: 1\t| Loss: 2.1274801272153856\t| precision: 0.5526838966202783\n",
      "epoch: 1\t| Loss: 2.1428842085599897\t| precision: 0.559214326978625\n",
      "epoch: 1\t| Loss: 2.137580522298813\t| precision: 0.5640771895101435\n",
      "epoch: 1\t| Loss: 2.146778870820999\t| precision: 0.5394799054373522\n",
      "epoch: 1\t| Loss: 2.1412396663427353\t| precision: 0.5492807671816728\n",
      "epoch: 1\t| Loss: 2.1492611569166185\t| precision: 0.5446265938069217\n",
      "mid epoch: 6659852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1418358844518663\t| precision: 0.535752401280683\n",
      "epoch: 1\t| Loss: 2.14257936835289\t| precision: 0.5684729064039409\n",
      "epoch: 1\t| Loss: 2.139449341893196\t| precision: 0.5594231725509696\n",
      "epoch: 1\t| Loss: 2.144858646392822\t| precision: 0.5655141037306642\n",
      "epoch: 1\t| Loss: 2.1481014662981033\t| precision: 0.5302942873629544\n",
      "epoch: 1\t| Loss: 2.1454372668266295\t| precision: 0.575107296137339\n",
      "epoch: 1\t| Loss: 2.1428960490226747\t| precision: 0.5516866158868335\n",
      "epoch: 1\t| Loss: 2.138037761449814\t| precision: 0.5813270698766883\n",
      "epoch: 1\t| Loss: 2.145805324316025\t| precision: 0.5613718411552346\n",
      "epoch: 1\t| Loss: 2.1371818560361864\t| precision: 0.5592140201805629\n",
      "epoch: 1\t| Loss: 2.1401848727464676\t| precision: 0.5465958474167069\n",
      "epoch: 1\t| Loss: 2.1448994505405428\t| precision: 0.5547703180212014\n",
      "epoch: 1\t| Loss: 2.131834835410118\t| precision: 0.5939887926642894\n",
      "epoch: 1\t| Loss: 2.1398269253969193\t| precision: 0.574585635359116\n",
      "epoch: 1\t| Loss: 2.1507992368936537\t| precision: 0.5568445475638051\n",
      "mid epoch: 7103852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.134486810564995\t| precision: 0.5803882528621205\n",
      "epoch: 1\t| Loss: 2.1469262504577635\t| precision: 0.5249433106575964\n",
      "epoch: 1\t| Loss: 2.1407913082838057\t| precision: 0.5402750491159135\n",
      "epoch: 1\t| Loss: 2.1390016478300096\t| precision: 0.562152133580705\n",
      "epoch: 1\t| Loss: 2.1411279875040052\t| precision: 0.587820835430297\n",
      "epoch: 1\t| Loss: 2.148260291814804\t| precision: 0.5593299208934388\n",
      "epoch: 1\t| Loss: 2.1416370701789855\t| precision: 0.5633062589584329\n",
      "epoch: 1\t| Loss: 2.1368505501747133\t| precision: 0.5538528896672504\n",
      "epoch: 1\t| Loss: 2.132198491692543\t| precision: 0.5569687334393216\n",
      "epoch: 1\t| Loss: 2.145634833574295\t| precision: 0.5745614035087719\n",
      "epoch: 1\t| Loss: 2.1372699534893034\t| precision: 0.5520661157024793\n",
      "epoch: 1\t| Loss: 2.136323364377022\t| precision: 0.5581996896016554\n",
      "epoch: 1\t| Loss: 2.1399402743577958\t| precision: 0.5758323057953144\n",
      "epoch: 1\t| Loss: 2.145706307888031\t| precision: 0.5754895767530006\n",
      "epoch: 1\t| Loss: 2.1389829659461976\t| precision: 0.5687074829931973\n",
      "mid epoch: 7547852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1420708644390105\t| precision: 0.5834236186348862\n",
      "epoch: 1\t| Loss: 2.135938109755516\t| precision: 0.5574082896617437\n",
      "epoch: 1\t| Loss: 2.1424296259880067\t| precision: 0.5583580613254204\n",
      "epoch: 1\t| Loss: 2.1406667220592497\t| precision: 0.5418261344997266\n",
      "epoch: 1\t| Loss: 2.1373283767700197\t| precision: 0.5656951743908266\n",
      "epoch: 1\t| Loss: 2.144935888051987\t| precision: 0.5454545454545454\n",
      "epoch: 1\t| Loss: 2.1455176883935927\t| precision: 0.5375418460066954\n",
      "epoch: 1\t| Loss: 2.1372718918323517\t| precision: 0.588774341351661\n",
      "epoch: 1\t| Loss: 2.143889061808586\t| precision: 0.5502676977989293\n",
      "epoch: 1\t| Loss: 2.149895180463791\t| precision: 0.5518672199170125\n",
      "epoch: 1\t| Loss: 2.140316832661629\t| precision: 0.5418502202643172\n",
      "epoch: 1\t| Loss: 2.136631172299385\t| precision: 0.5405117270788913\n",
      "epoch: 1\t| Loss: 2.1344701397418975\t| precision: 0.5662955465587044\n",
      "epoch: 1\t| Loss: 2.1351190811395644\t| precision: 0.5561250640697079\n",
      "epoch: 1\t| Loss: 2.148352406024933\t| precision: 0.5472193074501573\n",
      "mid epoch: 7991852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1386964571475984\t| precision: 0.5737250554323725\n",
      "epoch: 1\t| Loss: 2.131363807916641\t| precision: 0.5526709935097354\n",
      "epoch: 1\t| Loss: 2.1467087692022324\t| precision: 0.5635775862068966\n",
      "epoch: 1\t| Loss: 2.152505923509598\t| precision: 0.5711060948081265\n",
      "epoch: 1\t| Loss: 2.1453304755687714\t| precision: 0.5495447241564007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.1456141382455827\t| precision: 0.5324873096446701\n",
      "epoch: 1\t| Loss: 2.1450186425447466\t| precision: 0.5730394669400307\n",
      "epoch: 1\t| Loss: 2.135745381116867\t| precision: 0.5338470005503577\n",
      "epoch: 1\t| Loss: 2.136443501710892\t| precision: 0.5846905537459284\n",
      "epoch: 1\t| Loss: 2.1253921502828597\t| precision: 0.5608604407135362\n",
      "epoch: 1\t| Loss: 2.1365089893341063\t| precision: 0.5679586563307494\n",
      "epoch: 1\t| Loss: 2.1443651902675627\t| precision: 0.5482330468003821\n",
      "epoch: 1\t| Loss: 2.1488254868984225\t| precision: 0.5544499723604202\n",
      "epoch: 1\t| Loss: 2.1412809985876082\t| precision: 0.5630252100840336\n",
      "epoch: 1\t| Loss: 2.139952441453934\t| precision: 0.5605028810895757\n",
      "mid epoch: 8435852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.147833634018898\t| precision: 0.5637149028077754\n",
      "epoch: 1\t| Loss: 2.1507401883602144\t| precision: 0.5559701492537313\n",
      "epoch: 1\t| Loss: 2.1437903660535813\t| precision: 0.5410299943406904\n",
      "epoch: 1\t| Loss: 2.1326252150535585\t| precision: 0.5530155642023347\n",
      "epoch: 1\t| Loss: 2.1378359282016755\t| precision: 0.5686947988223748\n",
      "epoch: 1\t| Loss: 2.142588258981705\t| precision: 0.5649103747963063\n",
      "epoch: 1\t| Loss: 2.124148027896881\t| precision: 0.5699404761904762\n",
      "epoch: 1\t| Loss: 2.151957170367241\t| precision: 0.5697674418604651\n",
      "epoch: 1\t| Loss: 2.1426335901021956\t| precision: 0.5634233316352522\n",
      "epoch: 1\t| Loss: 2.14724723637104\t| precision: 0.5448215839860748\n",
      "epoch: 1\t| Loss: 2.1424221432209016\t| precision: 0.560302866414278\n",
      "epoch: 1\t| Loss: 2.131764903664589\t| precision: 0.5428126621691749\n",
      "epoch: 1\t| Loss: 2.1319550400972367\t| precision: 0.5591836734693878\n",
      "epoch: 1\t| Loss: 2.143405488729477\t| precision: 0.5734989648033126\n",
      "epoch: 1\t| Loss: 2.150391035079956\t| precision: 0.5339321357285429\n",
      "mid epoch: 8879852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.142973527908325\t| precision: 0.5570400822199383\n",
      "epoch: 1\t| Loss: 2.1413250094652176\t| precision: 0.5501672240802675\n",
      "epoch: 1\t| Loss: 2.1374228978157044\t| precision: 0.5689858490566038\n",
      "epoch: 1\t| Loss: 2.145961783528328\t| precision: 0.5777653003930376\n",
      "epoch: 1\t| Loss: 2.1569599676132203\t| precision: 0.5523223279238948\n",
      "epoch: 1\t| Loss: 2.134764314889908\t| precision: 0.570088587806149\n",
      "epoch: 1\t| Loss: 2.1373407506942748\t| precision: 0.5587695133149678\n",
      "epoch: 1\t| Loss: 2.142320856451988\t| precision: 0.5531095211887727\n",
      "epoch: 1\t| Loss: 2.1430281662940978\t| precision: 0.5604838709677419\n",
      "epoch: 1\t| Loss: 2.140322895050049\t| precision: 0.5626911314984709\n",
      "epoch: 1\t| Loss: 2.1487128710746766\t| precision: 0.5396391470749043\n",
      "epoch: 1\t| Loss: 2.141002274751663\t| precision: 0.5681426106958022\n",
      "epoch: 1\t| Loss: 2.131520071029663\t| precision: 0.5675082327113062\n",
      "epoch: 1\t| Loss: 2.1354888504743577\t| precision: 0.5402770651616213\n",
      "epoch: 1\t| Loss: 2.1403686916828155\t| precision: 0.5726452905811623\n",
      "mid epoch: 9323852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.148623926639557\t| precision: 0.5507692307692308\n",
      "epoch: 1\t| Loss: 2.1344767904281614\t| precision: 0.5423553719008265\n",
      "epoch: 1\t| Loss: 2.1406804597377778\t| precision: 0.5472610096670247\n",
      "epoch: 1\t| Loss: 2.136733386516571\t| precision: 0.5642673521850899\n",
      "epoch: 1\t| Loss: 2.146166088581085\t| precision: 0.5543933054393305\n",
      "epoch: 1\t| Loss: 2.143481185436249\t| precision: 0.57225156024964\n",
      "epoch: 1\t| Loss: 2.1358099591732027\t| precision: 0.5756218905472636\n",
      "epoch: 1\t| Loss: 2.1444744020700455\t| precision: 0.5608591885441527\n",
      "epoch: 1\t| Loss: 2.1350210320949556\t| precision: 0.5727420227149811\n",
      "epoch: 1\t| Loss: 2.1345760649442673\t| precision: 0.5608034744842563\n",
      "epoch: 1\t| Loss: 2.1407485711574554\t| precision: 0.553224155578301\n",
      "epoch: 1\t| Loss: 2.136791812181473\t| precision: 0.5836673346693386\n",
      "epoch: 1\t| Loss: 2.1432489162683486\t| precision: 0.571347678369196\n",
      "epoch: 1\t| Loss: 2.14343257188797\t| precision: 0.5652173913043478\n",
      "epoch: 1\t| Loss: 2.1417191767692567\t| precision: 0.5756853396901073\n",
      "mid epoch: 9767852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1426695829629896\t| precision: 0.5654475457170356\n",
      "epoch: 1\t| Loss: 2.1390451908111574\t| precision: 0.549171270718232\n",
      "epoch: 1\t| Loss: 2.128462293744087\t| precision: 0.5506364139457665\n",
      "epoch: 1\t| Loss: 2.1393014013767244\t| precision: 0.5599790466212676\n",
      "epoch: 1\t| Loss: 2.1483185404539107\t| precision: 0.5460055096418733\n",
      "epoch: 1\t| Loss: 2.1486870354413985\t| precision: 0.5657026889903602\n",
      "epoch: 1\t| Loss: 2.142413944005966\t| precision: 0.5800770500825536\n",
      "epoch: 1\t| Loss: 2.147186043858528\t| precision: 0.5781914893617022\n",
      "epoch: 1\t| Loss: 2.141224977374077\t| precision: 0.5465968586387434\n",
      "epoch: 1\t| Loss: 2.1492179840803147\t| precision: 0.5502325581395349\n",
      "epoch: 1\t| Loss: 2.1502089112997056\t| precision: 0.539447731755424\n",
      "epoch: 1\t| Loss: 2.136518920660019\t| precision: 0.5322793148880105\n",
      "epoch: 1\t| Loss: 2.142471580505371\t| precision: 0.5500957243139758\n",
      "epoch: 1\t| Loss: 2.143109363913536\t| precision: 0.588659793814433\n",
      "epoch: 1\t| Loss: 2.1409240889549257\t| precision: 0.5517899761336515\n",
      "mid epoch: 10211852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1472754150629045\t| precision: 0.5583547557840617\n",
      "epoch: 1\t| Loss: 2.148238196372986\t| precision: 0.5401891252955082\n",
      "epoch: 1\t| Loss: 2.1372341930866243\t| precision: 0.5619266055045872\n",
      "epoch: 1\t| Loss: 2.140745833516121\t| precision: 0.5768556399766218\n",
      "epoch: 1\t| Loss: 2.1382790887355805\t| precision: 0.5722836095764272\n",
      "epoch: 1\t| Loss: 2.1448074102401735\t| precision: 0.5525059665871122\n",
      "epoch: 1\t| Loss: 2.1452171087265013\t| precision: 0.5369386769676485\n",
      "epoch: 1\t| Loss: 2.1468733352422715\t| precision: 0.5134139926354551\n",
      "epoch: 1\t| Loss: 2.139942520260811\t| precision: 0.5657754010695187\n",
      "epoch: 1\t| Loss: 2.1398754745721815\t| precision: 0.5656037637219028\n",
      "epoch: 1\t| Loss: 2.1426072937250136\t| precision: 0.5584615384615385\n",
      "epoch: 1\t| Loss: 2.147449346780777\t| precision: 0.5418768920282543\n",
      "epoch: 1\t| Loss: 2.1415849339962008\t| precision: 0.5790513833992095\n",
      "epoch: 1\t| Loss: 2.1438657385110855\t| precision: 0.5432852386237513\n",
      "epoch: 1\t| Loss: 2.142946922779083\t| precision: 0.5807580174927114\n",
      "mid epoch: 10655852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1436331152915953\t| precision: 0.5438034188034188\n",
      "epoch: 1\t| Loss: 2.1443264377117157\t| precision: 0.5696142991533396\n",
      "epoch: 1\t| Loss: 2.1328920155763624\t| precision: 0.5486935866983373\n",
      "epoch: 1\t| Loss: 2.1459097295999525\t| precision: 0.5603190428713859\n",
      "epoch: 1\t| Loss: 2.1225435465574263\t| precision: 0.5745454545454546\n",
      "epoch: 1\t| Loss: 2.1404840570688246\t| precision: 0.5369978858350951\n",
      "epoch: 1\t| Loss: 2.1418866902589797\t| precision: 0.5770528683914511\n",
      "epoch: 1\t| Loss: 2.1463612681627273\t| precision: 0.5587188612099644\n",
      "epoch: 1\t| Loss: 2.140612361431122\t| precision: 0.5547445255474452\n",
      "epoch: 1\t| Loss: 2.1378656923770905\t| precision: 0.5547275267815557\n",
      "epoch: 1\t| Loss: 2.1410722523927688\t| precision: 0.5737618545837724\n",
      "epoch: 1\t| Loss: 2.1494213092327117\t| precision: 0.5655946911689638\n",
      "epoch: 1\t| Loss: 2.1474610418081284\t| precision: 0.5425733207190161\n",
      "epoch: 1\t| Loss: 2.1382196629047394\t| precision: 0.5806267806267806\n",
      "epoch: 1\t| Loss: 2.1427527004480362\t| precision: 0.5346003898635477\n",
      "mid epoch: 11099852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1491825479269027\t| precision: 0.5537537537537538\n",
      "epoch: 1\t| Loss: 2.1453639072179795\t| precision: 0.5441458733205374\n",
      "epoch: 1\t| Loss: 2.1432577574253084\t| precision: 0.5457317073170732\n",
      "epoch: 1\t| Loss: 2.135106748342514\t| precision: 0.5553907022749752\n",
      "epoch: 1\t| Loss: 2.142843931913376\t| precision: 0.5653964984552008\n",
      "epoch: 1\t| Loss: 2.1422183310985563\t| precision: 0.5542234332425068\n",
      "epoch: 1\t| Loss: 2.139969700574875\t| precision: 0.5738669805768098\n",
      "epoch: 1\t| Loss: 2.1426663506031036\t| precision: 0.5797169811320755\n",
      "epoch: 1\t| Loss: 2.138967997431755\t| precision: 0.5907692307692308\n",
      "epoch: 1\t| Loss: 2.1449850672483444\t| precision: 0.5695437053326003\n",
      "epoch: 1\t| Loss: 2.1451212584972383\t| precision: 0.5681581685744017\n",
      "epoch: 1\t| Loss: 2.135115602016449\t| precision: 0.5628445424476296\n",
      "epoch: 1\t| Loss: 2.1489212596416474\t| precision: 0.5535219079312257\n",
      "epoch: 1\t| Loss: 2.143220129609108\t| precision: 0.5773134328358209\n",
      "epoch: 1\t| Loss: 2.1490307343006134\t| precision: 0.5710889205896339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid epoch: 11543852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1408235025405884\t| precision: 0.5623342175066313\n",
      "epoch: 1\t| Loss: 2.1439686357975005\t| precision: 0.5248306997742663\n",
      "epoch: 1\t| Loss: 2.1402091717720033\t| precision: 0.5557564798071127\n",
      "epoch: 1\t| Loss: 2.144790272116661\t| precision: 0.5284671532846715\n",
      "epoch: 1\t| Loss: 2.1266123139858246\t| precision: 0.5378151260504201\n",
      "epoch: 1\t| Loss: 2.1433381801843643\t| precision: 0.5332685769791161\n",
      "epoch: 1\t| Loss: 2.1343832367658617\t| precision: 0.5673603504928806\n",
      "epoch: 1\t| Loss: 2.1364637213945388\t| precision: 0.5682960255824577\n",
      "epoch: 1\t| Loss: 2.1348081994056702\t| precision: 0.5672609400324149\n",
      "epoch: 1\t| Loss: 2.1395688754320146\t| precision: 0.5675675675675675\n",
      "epoch: 1\t| Loss: 2.1382579338550567\t| precision: 0.5364025695931478\n",
      "epoch: 1\t| Loss: 2.130821409225464\t| precision: 0.5490643541761753\n",
      "epoch: 1\t| Loss: 2.1406522756814956\t| precision: 0.557711950970378\n",
      "epoch: 1\t| Loss: 2.1334788888692855\t| precision: 0.5370152761457109\n",
      "epoch: 1\t| Loss: 2.144119324684143\t| precision: 0.5517241379310345\n",
      "mid epoch: 11987852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1467869240045547\t| precision: 0.5610448608745031\n",
      "epoch: 1\t| Loss: 2.142818186879158\t| precision: 0.5707915273132664\n",
      "epoch: 1\t| Loss: 2.1432888758182527\t| precision: 0.5509039010466222\n",
      "epoch: 1\t| Loss: 2.1457893592119217\t| precision: 0.546751968503937\n",
      "epoch: 1\t| Loss: 2.138280511498451\t| precision: 0.5487528344671202\n",
      "epoch: 1\t| Loss: 2.133352208733559\t| precision: 0.5535991714137752\n",
      "epoch: 1\t| Loss: 2.1329479867219927\t| precision: 0.5349087003222341\n",
      "epoch: 1\t| Loss: 2.144537735581398\t| precision: 0.5510204081632653\n",
      "epoch: 1\t| Loss: 2.1409895384311675\t| precision: 0.564327485380117\n",
      "epoch: 1\t| Loss: 2.134118231534958\t| precision: 0.5874089490114464\n",
      "epoch: 1\t| Loss: 2.142065550684929\t| precision: 0.565675057208238\n",
      "epoch: 1\t| Loss: 2.1463194584846494\t| precision: 0.5311827956989247\n",
      "epoch: 1\t| Loss: 2.1510966831445693\t| precision: 0.5421467556094602\n",
      "epoch: 1\t| Loss: 2.132296231985092\t| precision: 0.5397333333333333\n",
      "epoch: 1\t| Loss: 2.1390289199352264\t| precision: 0.5706833594157538\n",
      "mid epoch: 12431852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.133810747861862\t| precision: 0.5436702649656526\n",
      "epoch: 1\t| Loss: 2.1405948680639266\t| precision: 0.5373883928571429\n",
      "epoch: 1\t| Loss: 2.1434674775600433\t| precision: 0.5586592178770949\n",
      "epoch: 1\t| Loss: 2.1422932237386703\t| precision: 0.5730713943459511\n",
      "epoch: 1\t| Loss: 2.130639806985855\t| precision: 0.5720263000597728\n",
      "epoch: 1\t| Loss: 2.123870224952698\t| precision: 0.532967032967033\n",
      "epoch: 1\t| Loss: 2.1285620129108427\t| precision: 0.5518763796909493\n",
      "epoch: 1\t| Loss: 2.134444034099579\t| precision: 0.5594594594594594\n",
      "epoch: 1\t| Loss: 2.139295980334282\t| precision: 0.5732448866777226\n",
      "epoch: 1\t| Loss: 2.142522963285446\t| precision: 0.5638297872340425\n",
      "epoch: 1\t| Loss: 2.136955553293228\t| precision: 0.5543374642516683\n",
      "epoch: 1\t| Loss: 2.1364304709434507\t| precision: 0.5440710209258085\n",
      "epoch: 1\t| Loss: 2.1410278993844987\t| precision: 0.5658995815899581\n",
      "epoch: 1\t| Loss: 2.1416744530200957\t| precision: 0.5644900161899622\n",
      "epoch: 1\t| Loss: 2.1476388674974443\t| precision: 0.562126642771804\n",
      "mid epoch: 12875852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.140749198794365\t| precision: 0.563918096292197\n",
      "epoch: 1\t| Loss: 2.1386104732751847\t| precision: 0.5563086548488009\n",
      "epoch: 1\t| Loss: 2.1493026143312455\t| precision: 0.5523321956769056\n",
      "epoch: 1\t| Loss: 2.150555098056793\t| precision: 0.542825361512792\n",
      "epoch: 1\t| Loss: 2.1344618606567383\t| precision: 0.5513866231647635\n",
      "epoch: 1\t| Loss: 2.134898083806038\t| precision: 0.551967116852613\n",
      "epoch: 1\t| Loss: 2.141877743601799\t| precision: 0.5590626494500239\n",
      "epoch: 1\t| Loss: 2.148575026392937\t| precision: 0.5627070515854236\n",
      "epoch: 1\t| Loss: 2.141968619823456\t| precision: 0.5774263904034896\n",
      "epoch: 1\t| Loss: 2.140093985795975\t| precision: 0.5531914893617021\n",
      "epoch: 1\t| Loss: 2.14905430495739\t| precision: 0.5586854460093896\n",
      "epoch: 1\t| Loss: 2.1409053444862365\t| precision: 0.5504158004158004\n",
      "epoch: 1\t| Loss: 2.1369985967874525\t| precision: 0.5381658429434377\n",
      "epoch: 1\t| Loss: 2.1508951449394225\t| precision: 0.5776357827476039\n",
      "epoch: 1\t| Loss: 2.148117560148239\t| precision: 0.5444280805105547\n",
      "mid epoch: 13319852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.141863352060318\t| precision: 0.549740932642487\n",
      "epoch: 1\t| Loss: 2.144098048210144\t| precision: 0.5366774541531824\n",
      "epoch: 1\t| Loss: 2.1373273622989655\t| precision: 0.550314465408805\n",
      "epoch: 1\t| Loss: 2.142087611556053\t| precision: 0.560378408458542\n",
      "epoch: 1\t| Loss: 2.1366854161024094\t| precision: 0.5822470515207946\n",
      "epoch: 1\t| Loss: 2.141204959154129\t| precision: 0.5551902301550024\n",
      "epoch: 1\t| Loss: 2.1414030051231383\t| precision: 0.5701754385964912\n",
      "epoch: 1\t| Loss: 2.151780821084976\t| precision: 0.5588842975206612\n",
      "epoch: 1\t| Loss: 2.135777448415756\t| precision: 0.5343000557724484\n",
      "epoch: 1\t| Loss: 2.1493487936258315\t| precision: 0.5384215991692627\n",
      "epoch: 1\t| Loss: 2.1458206748962403\t| precision: 0.58887171561051\n",
      "epoch: 1\t| Loss: 2.1501371014118194\t| precision: 0.5605738575982997\n",
      "epoch: 1\t| Loss: 2.1332059681415556\t| precision: 0.5411483253588517\n",
      "epoch: 1\t| Loss: 2.133762193918228\t| precision: 0.5459681561376477\n",
      "epoch: 1\t| Loss: 2.1417805123329163\t| precision: 0.54931640625\n",
      "mid epoch: 13763852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.139810506105423\t| precision: 0.5710023866348448\n",
      "epoch: 1\t| Loss: 2.1417611211538317\t| precision: 0.5537806176783813\n",
      "epoch: 1\t| Loss: 2.135191375017166\t| precision: 0.5518843572534847\n",
      "epoch: 1\t| Loss: 2.1346804970502853\t| precision: 0.5497251374312844\n",
      "epoch: 1\t| Loss: 2.1468729734420777\t| precision: 0.5614861329147044\n",
      "epoch: 1\t| Loss: 2.1397004961967467\t| precision: 0.5783641160949868\n",
      "epoch: 1\t| Loss: 2.1411452615261077\t| precision: 0.5609990393852066\n",
      "epoch: 1\t| Loss: 2.145620141029358\t| precision: 0.5559265442404007\n",
      "epoch: 1\t| Loss: 2.1405974292755126\t| precision: 0.5323238206173558\n",
      "epoch: 1\t| Loss: 2.1413506543636323\t| precision: 0.562625250501002\n",
      "epoch: 1\t| Loss: 2.1359317153692245\t| precision: 0.5453156822810591\n",
      "epoch: 1\t| Loss: 2.1533501148223877\t| precision: 0.5611052072263549\n",
      "epoch: 1\t| Loss: 2.140808029770851\t| precision: 0.5699481865284974\n",
      "epoch: 1\t| Loss: 2.14639177441597\t| precision: 0.5659955257270693\n",
      "epoch: 1\t| Loss: 2.15183464884758\t| precision: 0.5277777777777778\n",
      "mid epoch: 14207852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.137556360960007\t| precision: 0.5566188197767146\n",
      "epoch: 1\t| Loss: 2.1420284980535507\t| precision: 0.5797991071428571\n",
      "epoch: 1\t| Loss: 2.1333156514167784\t| precision: 0.5576744186046512\n",
      "epoch: 1\t| Loss: 2.13125559091568\t| precision: 0.5492371705963939\n",
      "epoch: 1\t| Loss: 2.1491434669494627\t| precision: 0.5637755102040817\n",
      "epoch: 1\t| Loss: 2.152191457152367\t| precision: 0.5060240963855421\n",
      "epoch: 1\t| Loss: 2.126508738398552\t| precision: 0.5706548498276711\n",
      "epoch: 1\t| Loss: 2.135411134362221\t| precision: 0.5506361323155217\n",
      "epoch: 1\t| Loss: 2.1478516125679015\t| precision: 0.5297718419588202\n",
      "epoch: 1\t| Loss: 2.1361201077699663\t| precision: 0.5774725274725274\n",
      "epoch: 1\t| Loss: 2.1291701769828797\t| precision: 0.5487398953875416\n",
      "epoch: 1\t| Loss: 2.1463890290260315\t| precision: 0.542528735632184\n",
      "epoch: 1\t| Loss: 2.1417313557863236\t| precision: 0.5490196078431373\n",
      "epoch: 1\t| Loss: 2.1304404962062837\t| precision: 0.5757113821138211\n",
      "epoch: 1\t| Loss: 2.142871667742729\t| precision: 0.5724828676858197\n",
      "mid epoch: 14651852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.148851826786995\t| precision: 0.5560035842293907\n",
      "epoch: 1\t| Loss: 2.136630254983902\t| precision: 0.5747406955460647\n",
      "epoch: 1\t| Loss: 2.1423599195480345\t| precision: 0.5640287769784172\n",
      "epoch: 1\t| Loss: 2.1433949077129366\t| precision: 0.5435663627152989\n",
      "epoch: 1\t| Loss: 2.1396814036369323\t| precision: 0.5584677419354839\n",
      "epoch: 1\t| Loss: 2.1413527488708497\t| precision: 0.5764462809917356\n",
      "epoch: 1\t| Loss: 2.1440835201740267\t| precision: 0.5374592833876222\n",
      "epoch: 1\t| Loss: 2.1416456371545793\t| precision: 0.5669781931464174\n",
      "epoch: 1\t| Loss: 2.128332402706146\t| precision: 0.5408805031446541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.144928714632988\t| precision: 0.5631786771964462\n",
      "epoch: 1\t| Loss: 2.1379819399118425\t| precision: 0.5581146223888591\n",
      "epoch: 1\t| Loss: 2.1418326246738433\t| precision: 0.5422860071758073\n",
      "epoch: 1\t| Loss: 2.131229836344719\t| precision: 0.5536635706914345\n",
      "epoch: 1\t| Loss: 2.144889476299286\t| precision: 0.5159655347187024\n",
      "epoch: 1\t| Loss: 2.1317044526338575\t| precision: 0.5516332982086407\n",
      "mid epoch: 15095852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1479498779773714\t| precision: 0.5687002652519894\n",
      "epoch: 1\t| Loss: 2.1348871690034867\t| precision: 0.5714285714285714\n",
      "epoch: 1\t| Loss: 2.139750930070877\t| precision: 0.5515610217596972\n",
      "epoch: 1\t| Loss: 2.1333281004428866\t| precision: 0.5459770114942529\n",
      "epoch: 1\t| Loss: 2.142324498295784\t| precision: 0.5377980720446474\n",
      "epoch: 1\t| Loss: 2.1400569385290145\t| precision: 0.564042303172738\n",
      "epoch: 1\t| Loss: 2.136780619621277\t| precision: 0.562931492299522\n",
      "epoch: 1\t| Loss: 2.127666331529617\t| precision: 0.5645645645645646\n",
      "epoch: 1\t| Loss: 2.148801792263985\t| precision: 0.5596473029045643\n",
      "epoch: 1\t| Loss: 2.140127884745598\t| precision: 0.5700737618545838\n",
      "epoch: 1\t| Loss: 2.1443748766183854\t| precision: 0.5230400957510473\n",
      "epoch: 1\t| Loss: 2.1452371579408647\t| precision: 0.5476575121163166\n",
      "epoch: 1\t| Loss: 2.1350907683372498\t| precision: 0.5673009161381254\n",
      "epoch: 1\t| Loss: 2.1374388736486436\t| precision: 0.5612575127138234\n",
      "epoch: 1\t| Loss: 2.1427984255552293\t| precision: 0.5769407441433165\n",
      "mid epoch: 15539852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1476979726552963\t| precision: 0.5657065706570658\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "done epoch 1\n",
      "epoch: 2\t| Loss: 0.01092682957649231\t| precision: 0.5472779369627507\n",
      "epoch: 2\t| Loss: 2.142902681827545\t| precision: 0.53470715835141\n",
      "epoch: 2\t| Loss: 2.1336288583278655\t| precision: 0.5536504424778761\n",
      "epoch: 2\t| Loss: 2.1320191913843156\t| precision: 0.5632\n",
      "epoch: 2\t| Loss: 2.138495398759842\t| precision: 0.5616359447004609\n",
      "epoch: 2\t| Loss: 2.1402080994844437\t| precision: 0.5355787476280834\n",
      "epoch: 2\t| Loss: 2.1355523580312727\t| precision: 0.5617164898746384\n",
      "epoch: 2\t| Loss: 2.1359994125366213\t| precision: 0.5455465587044535\n",
      "epoch: 2\t| Loss: 2.1474125826358796\t| precision: 0.557815223707147\n",
      "epoch: 2\t| Loss: 2.1391687500476837\t| precision: 0.5678824721377913\n",
      "epoch: 2\t| Loss: 2.1407224237918854\t| precision: 0.5571131879543094\n",
      "epoch: 2\t| Loss: 2.13975241959095\t| precision: 0.5625363583478766\n",
      "epoch: 2\t| Loss: 2.142108849287033\t| precision: 0.554140127388535\n",
      "epoch: 2\t| Loss: 2.1418888849020004\t| precision: 0.5663430420711975\n",
      "epoch: 2\t| Loss: 2.137102503180504\t| precision: 0.5612549800796812\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.132204439640045\t| precision: 0.5803054239073197\n",
      "epoch: 2\t| Loss: 2.139015245437622\t| precision: 0.5709146968139774\n",
      "epoch: 2\t| Loss: 2.1356128787994386\t| precision: 0.5732848232848233\n",
      "epoch: 2\t| Loss: 2.1393468391895296\t| precision: 0.5602803738317756\n",
      "epoch: 2\t| Loss: 2.1304297864437105\t| precision: 0.5611031859248692\n",
      "epoch: 2\t| Loss: 2.1501404213905335\t| precision: 0.5786106032906764\n",
      "epoch: 2\t| Loss: 2.139473554491997\t| precision: 0.5712128837443382\n",
      "epoch: 2\t| Loss: 2.1342664724588394\t| precision: 0.5609622744669218\n",
      "epoch: 2\t| Loss: 2.1406898814439774\t| precision: 0.58011634056055\n",
      "epoch: 2\t| Loss: 2.143212378025055\t| precision: 0.5443155452436195\n",
      "epoch: 2\t| Loss: 2.13804905295372\t| precision: 0.5654622243078367\n",
      "epoch: 2\t| Loss: 2.135965130329132\t| precision: 0.5562248995983936\n",
      "epoch: 2\t| Loss: 2.136936542391777\t| precision: 0.5552617662612375\n",
      "epoch: 2\t| Loss: 2.1510076451301576\t| precision: 0.5500963391136802\n",
      "epoch: 2\t| Loss: 2.13757638335228\t| precision: 0.5280118988596926\n",
      "mid epoch: 887852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.140848684310913\t| precision: 0.5700104493207941\n",
      "epoch: 2\t| Loss: 2.1304184257984162\t| precision: 0.5749537322640346\n",
      "epoch: 2\t| Loss: 2.1431406128406523\t| precision: 0.5414710485133021\n",
      "epoch: 2\t| Loss: 2.1407922822237015\t| precision: 0.5592875318066157\n",
      "epoch: 2\t| Loss: 2.1396481162309646\t| precision: 0.5442359249329759\n",
      "epoch: 2\t| Loss: 2.1378269243240355\t| precision: 0.580046403712297\n",
      "epoch: 2\t| Loss: 2.13592292368412\t| precision: 0.5384615384615384\n",
      "epoch: 2\t| Loss: 2.137975734472275\t| precision: 0.5776553106212425\n",
      "epoch: 2\t| Loss: 2.140849367380142\t| precision: 0.5662847790507365\n",
      "epoch: 2\t| Loss: 2.1378121131658556\t| precision: 0.5789186237028946\n",
      "epoch: 2\t| Loss: 2.12724555850029\t| precision: 0.5457534246575343\n",
      "epoch: 2\t| Loss: 2.129833552837372\t| precision: 0.585480093676815\n",
      "epoch: 2\t| Loss: 2.146870715022087\t| precision: 0.5595463137996219\n",
      "epoch: 2\t| Loss: 2.1386483281850817\t| precision: 0.5676767676767677\n",
      "mid epoch: 1331852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.142510126233101\t| precision: 0.5715942028985507\n",
      "epoch: 2\t| Loss: 2.136355860829353\t| precision: 0.5612348178137652\n",
      "epoch: 2\t| Loss: 2.1347852605581283\t| precision: 0.5639327559857361\n",
      "epoch: 2\t| Loss: 2.137318271994591\t| precision: 0.567016317016317\n",
      "epoch: 2\t| Loss: 2.1364007526636124\t| precision: 0.5600408788962699\n",
      "epoch: 2\t| Loss: 2.1391159391403196\t| precision: 0.5638297872340425\n",
      "epoch: 2\t| Loss: 2.1436258041858673\t| precision: 0.5743562795585917\n",
      "epoch: 2\t| Loss: 2.1315182673931123\t| precision: 0.5536409116175653\n",
      "epoch: 2\t| Loss: 2.1298121535778045\t| precision: 0.5509877202349173\n",
      "epoch: 2\t| Loss: 2.141930277943611\t| precision: 0.5831062670299727\n",
      "epoch: 2\t| Loss: 2.142485873103142\t| precision: 0.5448235974551764\n",
      "epoch: 2\t| Loss: 2.1337786346673964\t| precision: 0.5365504061156235\n",
      "epoch: 2\t| Loss: 2.129526028037071\t| precision: 0.557628979143798\n",
      "epoch: 2\t| Loss: 2.133963239192963\t| precision: 0.5455026455026455\n",
      "epoch: 2\t| Loss: 2.132720012664795\t| precision: 0.5447409733124019\n",
      "mid epoch: 1775852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.142276967167854\t| precision: 0.5664335664335665\n",
      "epoch: 2\t| Loss: 2.147562327980995\t| precision: 0.5815450643776824\n",
      "epoch: 2\t| Loss: 2.125097607374191\t| precision: 0.5765281173594132\n",
      "epoch: 2\t| Loss: 2.13206585586071\t| precision: 0.5748765770707625\n",
      "epoch: 2\t| Loss: 2.147951062321663\t| precision: 0.55005382131324\n",
      "epoch: 2\t| Loss: 2.143139265179634\t| precision: 0.5594978165938864\n",
      "epoch: 2\t| Loss: 2.1423854571580887\t| precision: 0.5590961761297798\n",
      "epoch: 2\t| Loss: 2.139411399960518\t| precision: 0.5305394560855996\n",
      "epoch: 2\t| Loss: 2.134641797542572\t| precision: 0.5775910364145658\n",
      "epoch: 2\t| Loss: 2.1328908413648606\t| precision: 0.5598219254312743\n",
      "epoch: 2\t| Loss: 2.1272088050842286\t| precision: 0.5669330669330669\n",
      "epoch: 2\t| Loss: 2.136562407016754\t| precision: 0.5381565906838454\n",
      "epoch: 2\t| Loss: 2.1385900366306303\t| precision: 0.5750246791707798\n",
      "epoch: 2\t| Loss: 2.131438736319542\t| precision: 0.5418088737201365\n",
      "epoch: 2\t| Loss: 2.133376777768135\t| precision: 0.5592991913746631\n",
      "mid epoch: 2219852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1383697354793547\t| precision: 0.5628140703517588\n",
      "epoch: 2\t| Loss: 2.140202438235283\t| precision: 0.5587529976019184\n",
      "epoch: 2\t| Loss: 2.143621943593025\t| precision: 0.5401574803149606\n",
      "epoch: 2\t| Loss: 2.129666904211044\t| precision: 0.5617352614015573\n",
      "epoch: 2\t| Loss: 2.1432985997200014\t| precision: 0.5601589103291714\n",
      "epoch: 2\t| Loss: 2.134090827703476\t| precision: 0.5497679216090768\n",
      "epoch: 2\t| Loss: 2.137711542844772\t| precision: 0.5486670799752015\n",
      "epoch: 2\t| Loss: 2.1351993060112\t| precision: 0.5564556962025317\n",
      "epoch: 2\t| Loss: 2.1369883120059967\t| precision: 0.5446478092068774\n",
      "epoch: 2\t| Loss: 2.1419390767812727\t| precision: 0.5554469956033219\n",
      "epoch: 2\t| Loss: 2.1450471794605255\t| precision: 0.55\n",
      "epoch: 2\t| Loss: 2.1461933094263075\t| precision: 0.5476309226932669\n",
      "epoch: 2\t| Loss: 2.1290743803977965\t| precision: 0.5752469494479954\n",
      "epoch: 2\t| Loss: 2.131572400927544\t| precision: 0.5430597771023303\n",
      "epoch: 2\t| Loss: 2.1418600344657897\t| precision: 0.6020719073735528\n",
      "mid epoch: 2663852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.138169896602631\t| precision: 0.5495495495495496\n",
      "epoch: 2\t| Loss: 2.142337675690651\t| precision: 0.5596239485403266\n",
      "epoch: 2\t| Loss: 2.135772374868393\t| precision: 0.5658329400660689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.1403445482254027\t| precision: 0.5587544065804936\n",
      "epoch: 2\t| Loss: 2.1526091575622557\t| precision: 0.5727404543257613\n",
      "epoch: 2\t| Loss: 2.132385882735252\t| precision: 0.5605247465712582\n",
      "epoch: 2\t| Loss: 2.129627473950386\t| precision: 0.5753569539925966\n",
      "epoch: 2\t| Loss: 2.139601689577103\t| precision: 0.5515916575192097\n",
      "epoch: 2\t| Loss: 2.1496164309978485\t| precision: 0.568560235063663\n",
      "epoch: 2\t| Loss: 2.1353797590732575\t| precision: 0.5697874784606548\n",
      "epoch: 2\t| Loss: 2.137040399312973\t| precision: 0.5645161290322581\n",
      "epoch: 2\t| Loss: 2.149465588927269\t| precision: 0.5907258064516129\n",
      "epoch: 2\t| Loss: 2.1438006007671357\t| precision: 0.5873836608066184\n",
      "epoch: 2\t| Loss: 2.1321955454349517\t| precision: 0.5690032858707558\n",
      "epoch: 2\t| Loss: 2.14208332657814\t| precision: 0.5839598997493735\n",
      "mid epoch: 3107852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.134100207686424\t| precision: 0.5783195020746889\n",
      "epoch: 2\t| Loss: 2.133895420432091\t| precision: 0.5845333333333333\n",
      "epoch: 2\t| Loss: 2.1272647303342818\t| precision: 0.5452069716775599\n",
      "epoch: 2\t| Loss: 2.1484813576936723\t| precision: 0.555607917059378\n",
      "epoch: 2\t| Loss: 2.1450756841897967\t| precision: 0.5597269624573379\n",
      "epoch: 2\t| Loss: 2.139467407464981\t| precision: 0.5737704918032787\n",
      "epoch: 2\t| Loss: 2.1427130407094954\t| precision: 0.5613810741687979\n",
      "epoch: 2\t| Loss: 2.1551356256008147\t| precision: 0.5652446675031367\n",
      "epoch: 2\t| Loss: 2.1392193549871443\t| precision: 0.5657303370786517\n",
      "epoch: 2\t| Loss: 2.131733494400978\t| precision: 0.5471698113207547\n",
      "epoch: 2\t| Loss: 2.1407481586933135\t| precision: 0.5571565802113353\n",
      "epoch: 2\t| Loss: 2.1368521946668624\t| precision: 0.5657817109144543\n",
      "epoch: 2\t| Loss: 2.1477119410037995\t| precision: 0.5620950323974082\n",
      "epoch: 2\t| Loss: 2.1352257496118545\t| precision: 0.5684029765311963\n",
      "epoch: 2\t| Loss: 2.1401670563220976\t| precision: 0.5588531187122736\n",
      "mid epoch: 3551852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.138119868636131\t| precision: 0.5644808743169399\n",
      "epoch: 2\t| Loss: 2.141220093369484\t| precision: 0.5787567893783947\n",
      "epoch: 2\t| Loss: 2.1354819887876513\t| precision: 0.5569815195071869\n",
      "epoch: 2\t| Loss: 2.145582213997841\t| precision: 0.5725190839694656\n",
      "epoch: 2\t| Loss: 2.135315815806389\t| precision: 0.5600632244467861\n",
      "epoch: 2\t| Loss: 2.135894119143486\t| precision: 0.5482938114517062\n",
      "epoch: 2\t| Loss: 2.148712751865387\t| precision: 0.541015625\n",
      "epoch: 2\t| Loss: 2.1202963471412657\t| precision: 0.5594250967385296\n",
      "epoch: 2\t| Loss: 2.1337347412109375\t| precision: 0.5433628318584071\n",
      "epoch: 2\t| Loss: 2.137305477261543\t| precision: 0.5566239316239316\n",
      "epoch: 2\t| Loss: 2.145087519288063\t| precision: 0.5688487584650113\n",
      "epoch: 2\t| Loss: 2.146131404042244\t| precision: 0.5624349635796045\n",
      "epoch: 2\t| Loss: 2.1314339607954027\t| precision: 0.5816023738872403\n",
      "epoch: 2\t| Loss: 2.13843940615654\t| precision: 0.5473170731707317\n",
      "epoch: 2\t| Loss: 2.128520966172218\t| precision: 0.5668756530825496\n",
      "mid epoch: 3995852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.137280820608139\t| precision: 0.5505673408978786\n",
      "epoch: 2\t| Loss: 2.1427572268247603\t| precision: 0.5460240963855422\n",
      "epoch: 2\t| Loss: 2.142769697904587\t| precision: 0.5615819209039548\n",
      "epoch: 2\t| Loss: 2.1362634432315826\t| precision: 0.5741935483870968\n",
      "epoch: 2\t| Loss: 2.135836135149002\t| precision: 0.5386861313868613\n",
      "epoch: 2\t| Loss: 2.1357122087478637\t| precision: 0.5547686496694996\n",
      "epoch: 2\t| Loss: 2.136178312897682\t| precision: 0.5753268902785673\n",
      "epoch: 2\t| Loss: 2.1307599991559982\t| precision: 0.5489527520701413\n",
      "epoch: 2\t| Loss: 2.1514909195899965\t| precision: 0.5255198487712666\n",
      "epoch: 2\t| Loss: 2.1455135315656664\t| precision: 0.5540101993509504\n",
      "epoch: 2\t| Loss: 2.1362019962072374\t| precision: 0.5634328358208955\n",
      "epoch: 2\t| Loss: 2.1326400244235995\t| precision: 0.5436746987951807\n",
      "epoch: 2\t| Loss: 2.137213178277016\t| precision: 0.5575079872204473\n",
      "epoch: 2\t| Loss: 2.1409318429231643\t| precision: 0.5508021390374331\n",
      "epoch: 2\t| Loss: 2.1395938181877137\t| precision: 0.5581717451523546\n",
      "mid epoch: 4439852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.142567088007927\t| precision: 0.5704697986577181\n",
      "epoch: 2\t| Loss: 2.144455625414848\t| precision: 0.5408539799683711\n",
      "epoch: 2\t| Loss: 2.1381328284740446\t| precision: 0.5513603553581343\n",
      "epoch: 2\t| Loss: 2.1323049199581146\t| precision: 0.5509641873278237\n",
      "epoch: 2\t| Loss: 2.127954754829407\t| precision: 0.5547300643883111\n",
      "epoch: 2\t| Loss: 2.1477647376060487\t| precision: 0.558708959376739\n",
      "epoch: 2\t| Loss: 2.1450881427526474\t| precision: 0.5692307692307692\n",
      "epoch: 2\t| Loss: 2.132784653902054\t| precision: 0.5565982404692082\n",
      "epoch: 2\t| Loss: 2.137319951057434\t| precision: 0.5499717673630717\n",
      "epoch: 2\t| Loss: 2.12858540892601\t| precision: 0.5633941785890478\n",
      "epoch: 2\t| Loss: 2.1428347671031953\t| precision: 0.5529953917050692\n",
      "epoch: 2\t| Loss: 2.1388186049461364\t| precision: 0.5517948717948717\n",
      "epoch: 2\t| Loss: 2.1364949584007262\t| precision: 0.5542328042328042\n",
      "epoch: 2\t| Loss: 2.136230707168579\t| precision: 0.5532467532467532\n",
      "epoch: 2\t| Loss: 2.129848709702492\t| precision: 0.56088933804952\n",
      "mid epoch: 4883852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1335565245151518\t| precision: 0.5807909604519774\n",
      "epoch: 2\t| Loss: 2.1425459438562395\t| precision: 0.5548509828788839\n",
      "epoch: 2\t| Loss: 2.1357851779460906\t| precision: 0.5573164289487585\n",
      "epoch: 2\t| Loss: 2.13213155567646\t| precision: 0.5452890304961311\n",
      "epoch: 2\t| Loss: 2.128812864422798\t| precision: 0.5668849391955099\n",
      "epoch: 2\t| Loss: 2.1437611079216\t| precision: 0.5567296996662959\n",
      "epoch: 2\t| Loss: 2.124457797408104\t| precision: 0.5479056331246991\n",
      "epoch: 2\t| Loss: 2.13793782889843\t| precision: 0.5554520037278659\n",
      "epoch: 2\t| Loss: 2.1445771688222885\t| precision: 0.5305859626529298\n",
      "epoch: 2\t| Loss: 2.131947312951088\t| precision: 0.5550239234449761\n",
      "epoch: 2\t| Loss: 2.1259157198667524\t| precision: 0.5545722713864307\n",
      "epoch: 2\t| Loss: 2.1308642446994783\t| precision: 0.5754771544245229\n",
      "epoch: 2\t| Loss: 2.138603082895279\t| precision: 0.5370553359683794\n",
      "epoch: 2\t| Loss: 2.1282352966070177\t| precision: 0.553388090349076\n",
      "epoch: 2\t| Loss: 2.1329638558626174\t| precision: 0.5540298507462686\n",
      "mid epoch: 5327852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.141075024008751\t| precision: 0.551829268292683\n",
      "epoch: 2\t| Loss: 2.137703335881233\t| precision: 0.5769439912996194\n",
      "epoch: 2\t| Loss: 2.130794408917427\t| precision: 0.5514851485148515\n",
      "epoch: 2\t| Loss: 2.1402169281244277\t| precision: 0.5656370656370656\n",
      "epoch: 2\t| Loss: 2.1322911351919176\t| precision: 0.5494897959183673\n",
      "epoch: 2\t| Loss: 2.1474914461374284\t| precision: 0.5719079578139981\n",
      "epoch: 2\t| Loss: 2.1451535034179687\t| precision: 0.5581807442409923\n",
      "epoch: 2\t| Loss: 2.132677347660065\t| precision: 0.5640889830508474\n",
      "epoch: 2\t| Loss: 2.139936788678169\t| precision: 0.5738419618528611\n",
      "epoch: 2\t| Loss: 2.130417384505272\t| precision: 0.5708661417322834\n",
      "epoch: 2\t| Loss: 2.135026890039444\t| precision: 0.5643015521064302\n",
      "epoch: 2\t| Loss: 2.140282726287842\t| precision: 0.5530951182687468\n",
      "epoch: 2\t| Loss: 2.1415953028202055\t| precision: 0.5335968379446641\n",
      "epoch: 2\t| Loss: 2.1372085040807725\t| precision: 0.5648584905660378\n",
      "epoch: 2\t| Loss: 2.1369258749485014\t| precision: 0.5489492567913891\n",
      "mid epoch: 5771852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.138497694134712\t| precision: 0.5823798627002288\n",
      "epoch: 2\t| Loss: 2.137842929363251\t| precision: 0.5570154856874707\n",
      "epoch: 2\t| Loss: 2.1406358766555784\t| precision: 0.5545232273838631\n",
      "epoch: 2\t| Loss: 2.135228500366211\t| precision: 0.5625711035267349\n",
      "epoch: 2\t| Loss: 2.1347440481185913\t| precision: 0.5576649264356905\n",
      "epoch: 2\t| Loss: 2.1391980057954787\t| precision: 0.5429962141698216\n",
      "epoch: 2\t| Loss: 2.1436980098485945\t| precision: 0.5552311435523114\n",
      "epoch: 2\t| Loss: 2.139986976981163\t| precision: 0.5602409638554217\n",
      "epoch: 2\t| Loss: 2.1431605690717697\t| precision: 0.564143007360673\n",
      "epoch: 2\t| Loss: 2.1390544229745867\t| precision: 0.5652173913043478\n",
      "epoch: 2\t| Loss: 2.1333123874664306\t| precision: 0.5665188470066519\n",
      "epoch: 2\t| Loss: 2.1259496170282364\t| precision: 0.5936339522546419\n",
      "epoch: 2\t| Loss: 2.147681375741959\t| precision: 0.5607064017660044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.1256016647815703\t| precision: 0.5446428571428571\n",
      "epoch: 2\t| Loss: 2.1294434785842897\t| precision: 0.5516888433981576\n",
      "mid epoch: 6215852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1374534606933593\t| precision: 0.5533299949924887\n",
      "epoch: 2\t| Loss: 2.1376712882518767\t| precision: 0.5498027613412229\n",
      "epoch: 2\t| Loss: 2.1454273492097853\t| precision: 0.558288243974422\n",
      "epoch: 2\t| Loss: 2.1382159399986267\t| precision: 0.573320255026974\n",
      "epoch: 2\t| Loss: 2.12960114300251\t| precision: 0.5457967377666249\n",
      "epoch: 2\t| Loss: 2.1352593952417376\t| precision: 0.5609756097560976\n",
      "epoch: 2\t| Loss: 2.1388078969717026\t| precision: 0.5716134598792062\n",
      "epoch: 2\t| Loss: 2.1328011846542356\t| precision: 0.5497297297297298\n",
      "epoch: 2\t| Loss: 2.1434857374429703\t| precision: 0.5431675242995998\n",
      "epoch: 2\t| Loss: 2.143339982032776\t| precision: 0.5368581596339603\n",
      "epoch: 2\t| Loss: 2.1353820431232453\t| precision: 0.5773584905660377\n",
      "epoch: 2\t| Loss: 2.1388709390163423\t| precision: 0.5372222222222223\n",
      "epoch: 2\t| Loss: 2.1447352427244186\t| precision: 0.5733922434953362\n",
      "epoch: 2\t| Loss: 2.141929431557655\t| precision: 0.5700325732899023\n",
      "epoch: 2\t| Loss: 2.1383441883325576\t| precision: 0.5269058295964125\n",
      "mid epoch: 6659852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1357317042350767\t| precision: 0.5630011454753723\n",
      "epoch: 2\t| Loss: 2.1275669467449188\t| precision: 0.5584886128364389\n",
      "epoch: 2\t| Loss: 2.1368123435974122\t| precision: 0.5572666025024061\n",
      "epoch: 2\t| Loss: 2.1318249773979185\t| precision: 0.542433234421365\n",
      "epoch: 2\t| Loss: 2.1273105823993683\t| precision: 0.5718562874251497\n",
      "epoch: 2\t| Loss: 2.1393337655067444\t| precision: 0.5466123280692817\n",
      "epoch: 2\t| Loss: 2.1360431331396104\t| precision: 0.5732345248474281\n",
      "epoch: 2\t| Loss: 2.140480701327324\t| precision: 0.5963124696749151\n",
      "epoch: 2\t| Loss: 2.1445138841867446\t| precision: 0.5538302277432712\n",
      "epoch: 2\t| Loss: 2.1429508644342423\t| precision: 0.6007442849548112\n",
      "epoch: 2\t| Loss: 2.1413510155677797\t| precision: 0.565766800689259\n",
      "epoch: 2\t| Loss: 2.141303094625473\t| precision: 0.5589396503102086\n",
      "epoch: 2\t| Loss: 2.1390570056438447\t| precision: 0.5674974039460021\n",
      "epoch: 2\t| Loss: 2.1347757536172867\t| precision: 0.5649249870667357\n",
      "epoch: 2\t| Loss: 2.127744759917259\t| precision: 0.5601282736504543\n",
      "mid epoch: 7103852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.14128317296505\t| precision: 0.5557851239669421\n",
      "epoch: 2\t| Loss: 2.133573116064072\t| precision: 0.5458911419423693\n",
      "epoch: 2\t| Loss: 2.1339925217628477\t| precision: 0.5477611940298508\n",
      "epoch: 2\t| Loss: 2.1369318956136705\t| precision: 0.5746140651801029\n",
      "epoch: 2\t| Loss: 2.1395274132490156\t| precision: 0.5740090316106372\n",
      "epoch: 2\t| Loss: 2.1341518145799636\t| precision: 0.56656346749226\n",
      "epoch: 2\t| Loss: 2.140670955181122\t| precision: 0.5349977708426215\n",
      "epoch: 2\t| Loss: 2.1407448589801787\t| precision: 0.5864621893178212\n",
      "epoch: 2\t| Loss: 2.13815623819828\t| precision: 0.5584477046852816\n",
      "epoch: 2\t| Loss: 2.130456766486168\t| precision: 0.5707656612529002\n",
      "epoch: 2\t| Loss: 2.1320616257190705\t| precision: 0.5708548479632817\n",
      "epoch: 2\t| Loss: 2.127782461643219\t| precision: 0.5502584721424468\n",
      "epoch: 2\t| Loss: 2.13626218020916\t| precision: 0.5582627118644068\n",
      "epoch: 2\t| Loss: 2.138019078373909\t| precision: 0.5721896643580181\n",
      "epoch: 2\t| Loss: 2.129193056821823\t| precision: 0.5745085605580216\n",
      "mid epoch: 7547852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1348622274398803\t| precision: 0.5613861386138614\n",
      "epoch: 2\t| Loss: 2.1309679698944093\t| precision: 0.5424657534246575\n",
      "epoch: 2\t| Loss: 2.1387889951467516\t| precision: 0.5699949824385349\n",
      "epoch: 2\t| Loss: 2.1362705624103544\t| precision: 0.5508860759493671\n",
      "epoch: 2\t| Loss: 2.137758606672287\t| precision: 0.5595611285266457\n",
      "epoch: 2\t| Loss: 2.137861757278442\t| precision: 0.5548990645002462\n",
      "epoch: 2\t| Loss: 2.1390116316080094\t| precision: 0.5461538461538461\n",
      "epoch: 2\t| Loss: 2.145201518535614\t| precision: 0.5576062639821029\n",
      "epoch: 2\t| Loss: 2.1354290413856507\t| precision: 0.5520159283225485\n",
      "epoch: 2\t| Loss: 2.137459759712219\t| precision: 0.5575364667747164\n",
      "epoch: 2\t| Loss: 2.1355860477685926\t| precision: 0.5651093439363817\n",
      "epoch: 2\t| Loss: 2.1340476965904234\t| precision: 0.5658773527625987\n",
      "epoch: 2\t| Loss: 2.1380911123752595\t| precision: 0.5754159957058508\n",
      "epoch: 2\t| Loss: 2.129380514621735\t| precision: 0.5593719332679097\n",
      "epoch: 2\t| Loss: 2.142944047451019\t| precision: 0.5302508607968519\n",
      "mid epoch: 7991852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.137239082455635\t| precision: 0.5486111111111112\n",
      "epoch: 2\t| Loss: 2.1334833973646163\t| precision: 0.5490767735665695\n",
      "epoch: 2\t| Loss: 2.1426373559236525\t| precision: 0.5708622398414271\n",
      "epoch: 2\t| Loss: 2.131688586473465\t| precision: 0.5561712846347607\n",
      "epoch: 2\t| Loss: 2.141099302768707\t| precision: 0.5315052508751459\n",
      "epoch: 2\t| Loss: 2.1373701798915863\t| precision: 0.5814977973568282\n",
      "epoch: 2\t| Loss: 2.1336065405607223\t| precision: 0.5731244064577398\n",
      "epoch: 2\t| Loss: 2.1372292202711107\t| precision: 0.5538979788257941\n",
      "epoch: 2\t| Loss: 2.1370494788885117\t| precision: 0.5568708183221822\n",
      "epoch: 2\t| Loss: 2.1428631418943405\t| precision: 0.5547275267815557\n",
      "epoch: 2\t| Loss: 2.133940889239311\t| precision: 0.5349862258953169\n",
      "epoch: 2\t| Loss: 2.1363971573114395\t| precision: 0.5705719557195572\n",
      "epoch: 2\t| Loss: 2.1410391372442246\t| precision: 0.5366949575636545\n",
      "epoch: 2\t| Loss: 2.1321284580230713\t| precision: 0.5438681531749879\n",
      "epoch: 2\t| Loss: 2.1272865682840347\t| precision: 0.5710540115364446\n",
      "mid epoch: 8435852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.135040445923805\t| precision: 0.5565077664702731\n",
      "epoch: 2\t| Loss: 2.1325189727544784\t| precision: 0.559593023255814\n",
      "epoch: 2\t| Loss: 2.1344082522392274\t| precision: 0.5380802163136548\n",
      "epoch: 2\t| Loss: 2.1422297483682633\t| precision: 0.5423340961098398\n",
      "epoch: 2\t| Loss: 2.133289355635643\t| precision: 0.5724708171206225\n",
      "epoch: 2\t| Loss: 2.1413470900058744\t| precision: 0.5610267155578837\n",
      "epoch: 2\t| Loss: 2.1330869609117507\t| precision: 0.576133909287257\n",
      "epoch: 2\t| Loss: 2.1377766329050063\t| precision: 0.5384615384615384\n",
      "epoch: 2\t| Loss: 2.133764175772667\t| precision: 0.5396039603960396\n",
      "epoch: 2\t| Loss: 2.137232882976532\t| precision: 0.5418863503222027\n",
      "epoch: 2\t| Loss: 2.1416452318429946\t| precision: 0.5443968156766688\n",
      "epoch: 2\t| Loss: 2.135612807869911\t| precision: 0.5513326752221125\n",
      "epoch: 2\t| Loss: 2.12563332259655\t| precision: 0.5715050883770755\n",
      "epoch: 2\t| Loss: 2.1287253415584564\t| precision: 0.5738019169329074\n",
      "epoch: 2\t| Loss: 2.1363735002279283\t| precision: 0.5590909090909091\n",
      "mid epoch: 8879852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.140441265702248\t| precision: 0.5605028810895757\n",
      "epoch: 2\t| Loss: 2.136550290584564\t| precision: 0.5533333333333333\n",
      "epoch: 2\t| Loss: 2.142003976106644\t| precision: 0.5444801714898178\n",
      "epoch: 2\t| Loss: 2.146726362705231\t| precision: 0.539568345323741\n",
      "epoch: 2\t| Loss: 2.1340672957897184\t| precision: 0.5261554088369731\n",
      "epoch: 2\t| Loss: 2.131458747982979\t| precision: 0.5434322033898306\n",
      "epoch: 2\t| Loss: 2.1336100667715074\t| precision: 0.5540467137425312\n",
      "epoch: 2\t| Loss: 2.136128390431404\t| precision: 0.5289739500265817\n",
      "epoch: 2\t| Loss: 2.1319471538066863\t| precision: 0.5463541666666667\n",
      "epoch: 2\t| Loss: 2.1382806485891344\t| precision: 0.5454545454545454\n",
      "epoch: 2\t| Loss: 2.137245717048645\t| precision: 0.5478966041561074\n",
      "epoch: 2\t| Loss: 2.140157635807991\t| precision: 0.5646827348745695\n",
      "epoch: 2\t| Loss: 2.144379096031189\t| precision: 0.5388917132689375\n",
      "epoch: 2\t| Loss: 2.1366296869516375\t| precision: 0.5551020408163265\n",
      "epoch: 2\t| Loss: 2.1379003882408143\t| precision: 0.5582756785524216\n",
      "mid epoch: 9323852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1333469772338867\t| precision: 0.5492682926829269\n",
      "epoch: 2\t| Loss: 2.1312317687273024\t| precision: 0.5687203791469194\n",
      "epoch: 2\t| Loss: 2.1480271714925765\t| precision: 0.5539053905390539\n",
      "epoch: 2\t| Loss: 2.1357917052507402\t| precision: 0.5531811559009228\n",
      "epoch: 2\t| Loss: 2.135986458659172\t| precision: 0.546742209631728\n",
      "epoch: 2\t| Loss: 2.1288983911275863\t| precision: 0.5723612622415669\n",
      "epoch: 2\t| Loss: 2.139923750758171\t| precision: 0.5400298359025361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.13724355340004\t| precision: 0.5556206088992974\n",
      "epoch: 2\t| Loss: 2.134967074394226\t| precision: 0.5581936685288641\n",
      "epoch: 2\t| Loss: 2.1421440804004668\t| precision: 0.573321554770318\n",
      "epoch: 2\t| Loss: 2.1367736035585403\t| precision: 0.5692225772097976\n",
      "epoch: 2\t| Loss: 2.131026081442833\t| precision: 0.5849056603773585\n",
      "epoch: 2\t| Loss: 2.141780159473419\t| precision: 0.5704772475027747\n",
      "epoch: 2\t| Loss: 2.135790258049965\t| precision: 0.5713527851458886\n",
      "epoch: 2\t| Loss: 2.1428558230400085\t| precision: 0.5763224181360201\n",
      "mid epoch: 9767852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1315051513910293\t| precision: 0.5577330508474576\n",
      "epoch: 2\t| Loss: 2.122534216046333\t| precision: 0.5768688293370945\n",
      "epoch: 2\t| Loss: 2.1361446940898894\t| precision: 0.5435064935064935\n",
      "epoch: 2\t| Loss: 2.136515534520149\t| precision: 0.540281973816717\n",
      "epoch: 2\t| Loss: 2.1321104258298873\t| precision: 0.5596977329974812\n",
      "epoch: 2\t| Loss: 2.1500236988067627\t| precision: 0.5690021231422505\n",
      "epoch: 2\t| Loss: 2.1334205228090286\t| precision: 0.5732558139534883\n",
      "epoch: 2\t| Loss: 2.138630805015564\t| precision: 0.5603487838458008\n",
      "epoch: 2\t| Loss: 2.1323075753450396\t| precision: 0.5512249443207127\n",
      "epoch: 2\t| Loss: 2.130868680477142\t| precision: 0.5548421615837347\n",
      "epoch: 2\t| Loss: 2.1391199725866317\t| precision: 0.5692068429237948\n",
      "epoch: 2\t| Loss: 2.136117406487465\t| precision: 0.5585585585585585\n",
      "epoch: 2\t| Loss: 2.128947750329971\t| precision: 0.6\n",
      "epoch: 2\t| Loss: 2.1418963342905046\t| precision: 0.5675818373812038\n",
      "epoch: 2\t| Loss: 2.1417136657238007\t| precision: 0.5373054213633924\n",
      "mid epoch: 10211852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1396361112594606\t| precision: 0.5400100654252642\n",
      "epoch: 2\t| Loss: 2.1331937712430955\t| precision: 0.5575836592785746\n",
      "epoch: 2\t| Loss: 2.1302655851840973\t| precision: 0.5641580942068218\n",
      "epoch: 2\t| Loss: 2.12905263364315\t| precision: 0.5550572994519183\n",
      "epoch: 2\t| Loss: 2.1371876925230024\t| precision: 0.5491759702286018\n",
      "epoch: 2\t| Loss: 2.138644902706146\t| precision: 0.5646794150731158\n",
      "epoch: 2\t| Loss: 2.1440739291906357\t| precision: 0.5440792905581638\n",
      "epoch: 2\t| Loss: 2.129568589925766\t| precision: 0.5596234309623431\n",
      "epoch: 2\t| Loss: 2.142928197979927\t| precision: 0.5542922114837976\n",
      "epoch: 2\t| Loss: 2.1345846968889237\t| precision: 0.5779770802192327\n",
      "epoch: 2\t| Loss: 2.1407515662908554\t| precision: 0.5568760611205433\n",
      "epoch: 2\t| Loss: 2.136503087878227\t| precision: 0.567738791423002\n",
      "epoch: 2\t| Loss: 2.133212750554085\t| precision: 0.5553168635875403\n",
      "epoch: 2\t| Loss: 2.1338463515043258\t| precision: 0.5624123422159888\n",
      "epoch: 2\t| Loss: 2.141042714715004\t| precision: 0.5741993685160126\n",
      "mid epoch: 10655852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.126284970641136\t| precision: 0.5626780626780626\n",
      "epoch: 2\t| Loss: 2.142144038081169\t| precision: 0.535394265232975\n",
      "epoch: 2\t| Loss: 2.134730952978134\t| precision: 0.5590838105153566\n",
      "epoch: 2\t| Loss: 2.1325888061523437\t| precision: 0.5647693817468106\n",
      "epoch: 2\t| Loss: 2.1334625267982483\t| precision: 0.555401019935095\n",
      "epoch: 2\t| Loss: 2.1225235748291014\t| precision: 0.5861297539149888\n",
      "epoch: 2\t| Loss: 2.1341282570362092\t| precision: 0.5684713375796179\n",
      "epoch: 2\t| Loss: 2.138569697737694\t| precision: 0.5805180180180181\n",
      "epoch: 2\t| Loss: 2.1346134370565415\t| precision: 0.5572167912593444\n",
      "epoch: 2\t| Loss: 2.1345763599872587\t| precision: 0.5579462102689486\n",
      "epoch: 2\t| Loss: 2.1291513323783873\t| precision: 0.555878084179971\n",
      "epoch: 2\t| Loss: 2.1424157536029815\t| precision: 0.5261522527187985\n",
      "epoch: 2\t| Loss: 2.1471383756399156\t| precision: 0.5584569732937685\n",
      "epoch: 2\t| Loss: 2.1356116050481795\t| precision: 0.5520637898686679\n",
      "epoch: 2\t| Loss: 2.141178880929947\t| precision: 0.5656565656565656\n",
      "mid epoch: 11099852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1333920723199844\t| precision: 0.5495495495495496\n",
      "epoch: 2\t| Loss: 2.134721481204033\t| precision: 0.5734806629834254\n",
      "epoch: 2\t| Loss: 2.1339226859807967\t| precision: 0.5822287677957781\n",
      "epoch: 2\t| Loss: 2.119427202939987\t| precision: 0.5766666666666667\n",
      "epoch: 2\t| Loss: 2.141123521924019\t| precision: 0.5659432387312187\n",
      "epoch: 2\t| Loss: 2.1292297077178954\t| precision: 0.5533848250795094\n",
      "epoch: 2\t| Loss: 2.1361379224061965\t| precision: 0.5378510378510378\n",
      "epoch: 2\t| Loss: 2.1243084186315535\t| precision: 0.5705622932745315\n",
      "epoch: 2\t| Loss: 2.142169828414917\t| precision: 0.5590351248413035\n",
      "epoch: 2\t| Loss: 2.133173640370369\t| precision: 0.5614834092387768\n",
      "epoch: 2\t| Loss: 2.140357490777969\t| precision: 0.5566579634464752\n",
      "epoch: 2\t| Loss: 2.133282727599144\t| precision: 0.5565766800689259\n",
      "epoch: 2\t| Loss: 2.130088423490524\t| precision: 0.5674846625766872\n",
      "epoch: 2\t| Loss: 2.1443868523836134\t| precision: 0.568119139547711\n",
      "epoch: 2\t| Loss: 2.1463921707868576\t| precision: 0.5370666666666667\n",
      "mid epoch: 11543852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1402738869190214\t| precision: 0.5692233478051134\n",
      "epoch: 2\t| Loss: 2.132793679833412\t| precision: 0.5606337599024985\n",
      "epoch: 2\t| Loss: 2.13288992524147\t| precision: 0.5451770451770451\n",
      "epoch: 2\t| Loss: 2.1280634784698487\t| precision: 0.562349542609533\n",
      "epoch: 2\t| Loss: 2.1220210379362108\t| precision: 0.5889196675900277\n",
      "epoch: 2\t| Loss: 2.1431400430202485\t| precision: 0.573520050922979\n",
      "epoch: 2\t| Loss: 2.134870874285698\t| precision: 0.543424317617866\n",
      "epoch: 2\t| Loss: 2.1375991666316985\t| precision: 0.5827625570776256\n",
      "epoch: 2\t| Loss: 2.136871653199196\t| precision: 0.5584481878509444\n",
      "epoch: 2\t| Loss: 2.1320954740047453\t| precision: 0.5515669515669516\n",
      "epoch: 2\t| Loss: 2.1380783969163897\t| precision: 0.5525114155251142\n",
      "epoch: 2\t| Loss: 2.1367179423570635\t| precision: 0.5700824499411072\n",
      "epoch: 2\t| Loss: 2.1377439230680464\t| precision: 0.5383104125736738\n",
      "epoch: 2\t| Loss: 2.134915965795517\t| precision: 0.5353585657370518\n",
      "epoch: 2\t| Loss: 2.130038178563118\t| precision: 0.5683411214953271\n",
      "mid epoch: 11987852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.13309530377388\t| precision: 0.5706860706860707\n",
      "epoch: 2\t| Loss: 2.125817618370056\t| precision: 0.5572709163346613\n",
      "epoch: 2\t| Loss: 2.128650285601616\t| precision: 0.5645737621235324\n",
      "epoch: 2\t| Loss: 2.129233394265175\t| precision: 0.5454099165439371\n",
      "epoch: 2\t| Loss: 2.1328368580341337\t| precision: 0.5516252390057361\n",
      "epoch: 2\t| Loss: 2.138441579937935\t| precision: 0.5583989501312336\n",
      "epoch: 2\t| Loss: 2.138693214058876\t| precision: 0.5384223918575064\n",
      "epoch: 2\t| Loss: 2.131251046657562\t| precision: 0.5680473372781065\n",
      "epoch: 2\t| Loss: 2.140116862654686\t| precision: 0.5765472312703583\n",
      "epoch: 2\t| Loss: 2.132214534878731\t| precision: 0.5585079202861523\n",
      "epoch: 2\t| Loss: 2.1286768108606338\t| precision: 0.5709515859766278\n",
      "epoch: 2\t| Loss: 2.1350979977846145\t| precision: 0.5501234567901234\n",
      "epoch: 2\t| Loss: 2.1345099526643754\t| precision: 0.5347912524850894\n",
      "epoch: 2\t| Loss: 2.1349275451898575\t| precision: 0.5452620434092113\n",
      "epoch: 2\t| Loss: 2.129363875389099\t| precision: 0.565112926477655\n",
      "mid epoch: 12431852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1345198917388917\t| precision: 0.5665024630541872\n",
      "epoch: 2\t| Loss: 2.133988754749298\t| precision: 0.555682467161622\n",
      "epoch: 2\t| Loss: 2.130224258303642\t| precision: 0.5753493013972056\n",
      "epoch: 2\t| Loss: 2.1193419235944746\t| precision: 0.550974930362117\n",
      "epoch: 2\t| Loss: 2.1421247297525405\t| precision: 0.5610176800344976\n",
      "epoch: 2\t| Loss: 2.1321713614463804\t| precision: 0.5727370689655172\n",
      "epoch: 2\t| Loss: 2.1263529473543166\t| precision: 0.5550755939524838\n",
      "epoch: 2\t| Loss: 2.1419188171625136\t| precision: 0.5735941320293398\n",
      "epoch: 2\t| Loss: 2.13369595348835\t| precision: 0.5386458957459557\n",
      "epoch: 2\t| Loss: 2.1379194831848145\t| precision: 0.5235873509590462\n",
      "epoch: 2\t| Loss: 2.1425184452533723\t| precision: 0.5602377093462994\n",
      "epoch: 2\t| Loss: 2.13835248708725\t| precision: 0.5535444947209653\n",
      "epoch: 2\t| Loss: 2.136779650449753\t| precision: 0.5629750271444083\n",
      "epoch: 2\t| Loss: 2.1392253816127775\t| precision: 0.5561385099685204\n",
      "epoch: 2\t| Loss: 2.13956166267395\t| precision: 0.5451213925790197\n",
      "mid epoch: 12875852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.132168714404106\t| precision: 0.562962962962963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.127465271949768\t| precision: 0.5301810865191147\n",
      "epoch: 2\t| Loss: 2.1237046301364897\t| precision: 0.5567970204841713\n",
      "epoch: 2\t| Loss: 2.1255055260658264\t| precision: 0.5557213930348258\n",
      "epoch: 2\t| Loss: 2.1313216960430146\t| precision: 0.5430902600713922\n",
      "epoch: 2\t| Loss: 2.136247825026512\t| precision: 0.5743779777660137\n",
      "epoch: 2\t| Loss: 2.133825766444206\t| precision: 0.545835272219637\n",
      "epoch: 2\t| Loss: 2.1326661211252214\t| precision: 0.566497461928934\n",
      "epoch: 2\t| Loss: 2.1459572702646255\t| precision: 0.561618062088429\n",
      "epoch: 2\t| Loss: 2.14045685172081\t| precision: 0.568655303030303\n",
      "epoch: 2\t| Loss: 2.140598255991936\t| precision: 0.5688342120679555\n",
      "epoch: 2\t| Loss: 2.1414389425516127\t| precision: 0.5662778366914104\n",
      "epoch: 2\t| Loss: 2.1321226602792738\t| precision: 0.5558455114822547\n",
      "epoch: 2\t| Loss: 2.1359992825984957\t| precision: 0.5400313971742543\n",
      "epoch: 2\t| Loss: 2.1400681030750275\t| precision: 0.5572519083969466\n",
      "mid epoch: 13319852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1377195662260053\t| precision: 0.5603537981269511\n",
      "epoch: 2\t| Loss: 2.135456682443619\t| precision: 0.5705165409170052\n",
      "epoch: 2\t| Loss: 2.1322301769256593\t| precision: 0.5782442748091603\n",
      "epoch: 2\t| Loss: 2.1284292471408843\t| precision: 0.5509138381201044\n",
      "epoch: 2\t| Loss: 2.137293175458908\t| precision: 0.5706831119544592\n",
      "epoch: 2\t| Loss: 2.129859347939491\t| precision: 0.5484429065743944\n",
      "epoch: 2\t| Loss: 2.138362104892731\t| precision: 0.5715065502183406\n",
      "epoch: 2\t| Loss: 2.1451012325286865\t| precision: 0.5407901487942535\n",
      "epoch: 2\t| Loss: 2.1417146760225294\t| precision: 0.5705722070844687\n",
      "epoch: 2\t| Loss: 2.141055842638016\t| precision: 0.5494505494505495\n",
      "epoch: 2\t| Loss: 2.145612080693245\t| precision: 0.5441988950276243\n",
      "epoch: 2\t| Loss: 2.133762196302414\t| precision: 0.5534454693434617\n",
      "epoch: 2\t| Loss: 2.131485740542412\t| precision: 0.5546058879392213\n",
      "epoch: 2\t| Loss: 2.143204055428505\t| precision: 0.5706781279847183\n",
      "epoch: 2\t| Loss: 2.135553830265999\t| precision: 0.5440356744704571\n",
      "mid epoch: 13763852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.129023272395134\t| precision: 0.5706084959816303\n",
      "epoch: 2\t| Loss: 2.128202974200249\t| precision: 0.5413983440662373\n",
      "epoch: 2\t| Loss: 2.1289967358112336\t| precision: 0.5474613686534217\n",
      "epoch: 2\t| Loss: 2.1405202823877336\t| precision: 0.5868665977249224\n",
      "epoch: 2\t| Loss: 2.1388691955804826\t| precision: 0.5529100529100529\n",
      "epoch: 2\t| Loss: 2.1389924019575117\t| precision: 0.5514157973174366\n",
      "epoch: 2\t| Loss: 2.1330922305583955\t| precision: 0.5434052757793765\n",
      "epoch: 2\t| Loss: 2.1302253186702726\t| precision: 0.5342920353982301\n",
      "epoch: 2\t| Loss: 2.1300420916080474\t| precision: 0.5541724296911019\n",
      "epoch: 2\t| Loss: 2.144746395945549\t| precision: 0.5681935151827071\n",
      "epoch: 2\t| Loss: 2.1359636372327806\t| precision: 0.5707013574660633\n",
      "epoch: 2\t| Loss: 2.135678814649582\t| precision: 0.5242378810594702\n",
      "epoch: 2\t| Loss: 2.13768842279911\t| precision: 0.5467907888290053\n",
      "epoch: 2\t| Loss: 2.1356125313043592\t| precision: 0.5493513818386915\n",
      "epoch: 2\t| Loss: 2.1389738661050797\t| precision: 0.5511853448275862\n",
      "mid epoch: 14207852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1422922682762144\t| precision: 0.5486074619022596\n",
      "epoch: 2\t| Loss: 2.147014688849449\t| precision: 0.5612565445026177\n",
      "epoch: 2\t| Loss: 2.130239742398262\t| precision: 0.5371203599550056\n",
      "epoch: 2\t| Loss: 2.133599424958229\t| precision: 0.5613614573346117\n",
      "epoch: 2\t| Loss: 2.1340083461999892\t| precision: 0.5202477370176274\n",
      "epoch: 2\t| Loss: 2.1245960468053817\t| precision: 0.5569125549584758\n",
      "epoch: 2\t| Loss: 2.1382358831167223\t| precision: 0.5621103117505996\n",
      "epoch: 2\t| Loss: 2.1282393354177476\t| precision: 0.5383033419023137\n",
      "epoch: 2\t| Loss: 2.138209336400032\t| precision: 0.5670611439842209\n",
      "epoch: 2\t| Loss: 2.137790035009384\t| precision: 0.5790305584826133\n",
      "epoch: 2\t| Loss: 2.137390140891075\t| precision: 0.5548621944877795\n",
      "epoch: 2\t| Loss: 2.138132894039154\t| precision: 0.5619001919385797\n",
      "epoch: 2\t| Loss: 2.135337311029434\t| precision: 0.577079107505071\n",
      "epoch: 2\t| Loss: 2.1278211349248886\t| precision: 0.5495702005730659\n",
      "epoch: 2\t| Loss: 2.129444050788879\t| precision: 0.5564087418783225\n",
      "mid epoch: 14651852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.132909630537033\t| precision: 0.5645481628599801\n",
      "epoch: 2\t| Loss: 2.129917793273926\t| precision: 0.5663064833005894\n",
      "epoch: 2\t| Loss: 2.129327509403229\t| precision: 0.5715112912565142\n",
      "epoch: 2\t| Loss: 2.130615305900574\t| precision: 0.5491047205642974\n",
      "epoch: 2\t| Loss: 2.135125895142555\t| precision: 0.558300395256917\n",
      "epoch: 2\t| Loss: 2.130460416674614\t| precision: 0.5609243697478992\n",
      "epoch: 2\t| Loss: 2.142472111582756\t| precision: 0.5467820443482964\n",
      "epoch: 2\t| Loss: 2.1413825470209122\t| precision: 0.5577136514983352\n",
      "epoch: 2\t| Loss: 2.139069058895111\t| precision: 0.5434362934362934\n",
      "epoch: 2\t| Loss: 2.129059897065163\t| precision: 0.5535261288685946\n",
      "epoch: 2\t| Loss: 2.1445503568649293\t| precision: 0.5604283528811831\n",
      "epoch: 2\t| Loss: 2.1372902965545655\t| precision: 0.5684830633284241\n",
      "epoch: 2\t| Loss: 2.1401402807235717\t| precision: 0.5337457817772778\n",
      "epoch: 2\t| Loss: 2.128460528254509\t| precision: 0.5667408231368187\n",
      "epoch: 2\t| Loss: 2.1470895904302596\t| precision: 0.5429864253393665\n",
      "mid epoch: 15095852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1436648893356325\t| precision: 0.5610510046367851\n",
      "epoch: 2\t| Loss: 2.132480977773666\t| precision: 0.58\n",
      "epoch: 2\t| Loss: 2.1314712178707125\t| precision: 0.5648635710866443\n",
      "epoch: 2\t| Loss: 2.1355067884922025\t| precision: 0.5515247108307045\n",
      "epoch: 2\t| Loss: 2.137643423080444\t| precision: 0.5542553191489362\n",
      "epoch: 2\t| Loss: 2.1385529536008834\t| precision: 0.543731778425656\n",
      "epoch: 2\t| Loss: 2.1314099442958834\t| precision: 0.5685195376995047\n",
      "epoch: 2\t| Loss: 2.1354614770412446\t| precision: 0.5656665027053616\n",
      "epoch: 2\t| Loss: 2.1390626722574235\t| precision: 0.5651055069480185\n",
      "epoch: 2\t| Loss: 2.139572492837906\t| precision: 0.562019758507135\n",
      "epoch: 2\t| Loss: 2.1340646070241927\t| precision: 0.5255613951266125\n",
      "epoch: 2\t| Loss: 2.1397095292806627\t| precision: 0.5401360544217687\n",
      "epoch: 2\t| Loss: 2.1285048824548722\t| precision: 0.5733091239219247\n",
      "epoch: 2\t| Loss: 2.1371036541461943\t| precision: 0.5625686059275521\n",
      "epoch: 2\t| Loss: 2.1304532170295714\t| precision: 0.5456660849331006\n",
      "mid epoch: 15539852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1295784080028533\t| precision: 0.5371200797209765\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "done epoch 2\n",
      "epoch: 3\t| Loss: 0.0105732524394989\t| precision: 0.56210283073368\n",
      "epoch: 3\t| Loss: 2.134012452960014\t| precision: 0.5211796246648793\n",
      "epoch: 3\t| Loss: 2.1414126318693163\t| precision: 0.5847589424572317\n",
      "epoch: 3\t| Loss: 2.13644352376461\t| precision: 0.5686492495831017\n",
      "epoch: 3\t| Loss: 2.1315941059589387\t| precision: 0.5503963759909399\n",
      "epoch: 3\t| Loss: 2.1360554790496824\t| precision: 0.5723943661971831\n",
      "epoch: 3\t| Loss: 2.147815590500832\t| precision: 0.5627044711014176\n",
      "epoch: 3\t| Loss: 2.1388958609104156\t| precision: 0.5420412480169222\n",
      "epoch: 3\t| Loss: 2.1272696828842164\t| precision: 0.5453091684434968\n",
      "epoch: 3\t| Loss: 2.1353962415456773\t| precision: 0.5280077934729663\n",
      "epoch: 3\t| Loss: 2.1307581275701524\t| precision: 0.5659574468085107\n",
      "epoch: 3\t| Loss: 2.13472310423851\t| precision: 0.5571808510638298\n",
      "epoch: 3\t| Loss: 2.1355264538526537\t| precision: 0.548918640576725\n",
      "epoch: 3\t| Loss: 2.136435624361038\t| precision: 0.5480386583285958\n",
      "epoch: 3\t| Loss: 2.137641282081604\t| precision: 0.5411428571428571\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t| Loss: 2.1333414655923844\t| precision: 0.5375921375921376\n",
      "epoch: 3\t| Loss: 2.1414382433891297\t| precision: 0.5347368421052632\n",
      "epoch: 3\t| Loss: 2.1280471163988115\t| precision: 0.5709677419354838\n",
      "epoch: 3\t| Loss: 2.1381186175346376\t| precision: 0.5455723542116631\n",
      "epoch: 3\t| Loss: 2.137338162660599\t| precision: 0.5539488320355951\n",
      "epoch: 3\t| Loss: 2.1375797152519227\t| precision: 0.5523529411764706\n",
      "epoch: 3\t| Loss: 2.141825518012047\t| precision: 0.5489338436303991\n",
      "epoch: 3\t| Loss: 2.127901435494423\t| precision: 0.5495764823119084\n",
      "epoch: 3\t| Loss: 2.129292989373207\t| precision: 0.5684062059238364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\t| Loss: 2.128296617269516\t| precision: 0.5220476431829701\n",
      "epoch: 3\t| Loss: 2.1288508480787276\t| precision: 0.5691428571428572\n",
      "epoch: 3\t| Loss: 2.1416302019357683\t| precision: 0.5775142731664471\n",
      "epoch: 3\t| Loss: 2.1272650760412217\t| precision: 0.5466598674145844\n",
      "epoch: 3\t| Loss: 2.1315444815158844\t| precision: 0.5547945205479452\n",
      "epoch: 3\t| Loss: 2.1339723801612855\t| precision: 0.5533653846153846\n",
      "mid epoch: 887852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t| Loss: 2.136832554936409\t| precision: 0.5691014188124015\n",
      "epoch: 3\t| Loss: 2.130525398850441\t| precision: 0.5636722606120435\n",
      "epoch: 3\t| Loss: 2.134888373017311\t| precision: 0.5561893896177981\n",
      "epoch: 3\t| Loss: 2.1336758196353913\t| precision: 0.5783811475409836\n",
      "epoch: 3\t| Loss: 2.1259217858314514\t| precision: 0.5560321715817694\n",
      "epoch: 3\t| Loss: 2.144002075791359\t| precision: 0.5555555555555556\n",
      "epoch: 3\t| Loss: 2.136302618384361\t| precision: 0.5562913907284768\n",
      "epoch: 3\t| Loss: 2.135786957144737\t| precision: 0.5683890577507599\n",
      "epoch: 3\t| Loss: 2.13317297577858\t| precision: 0.5795246800731262\n",
      "epoch: 3\t| Loss: 2.128254799246788\t| precision: 0.5436978480456741\n",
      "epoch: 3\t| Loss: 2.131725123524666\t| precision: 0.5502318392581144\n",
      "epoch: 3\t| Loss: 2.1345247620344163\t| precision: 0.5628571428571428\n",
      "epoch: 3\t| Loss: 2.132038798332214\t| precision: 0.5650759219088937\n",
      "epoch: 3\t| Loss: 2.130741515159607\t| precision: 0.5530575539568345\n",
      "epoch: 3\t| Loss: 2.125870438218117\t| precision: 0.5562395075545608\n",
      "mid epoch: 1331852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t| Loss: 2.130023468732834\t| precision: 0.563615428900403\n",
      "epoch: 3\t| Loss: 2.13807542681694\t| precision: 0.5378238341968912\n",
      "epoch: 3\t| Loss: 2.127777224779129\t| precision: 0.5845980888139404\n",
      "epoch: 3\t| Loss: 2.1317483723163604\t| precision: 0.5593487394957983\n",
      "epoch: 3\t| Loss: 2.132434684634209\t| precision: 0.5434310532030402\n",
      "epoch: 3\t| Loss: 2.128606333732605\t| precision: 0.5546475995914198\n",
      "epoch: 3\t| Loss: 2.1255269086360933\t| precision: 0.5625766871165644\n",
      "epoch: 3\t| Loss: 2.130046035051346\t| precision: 0.5354107648725213\n",
      "epoch: 3\t| Loss: 2.1366823720932007\t| precision: 0.5467449828683308\n",
      "epoch: 3\t| Loss: 2.1296466994285583\t| precision: 0.5423553719008265\n",
      "epoch: 3\t| Loss: 2.1384876000881197\t| precision: 0.5470914127423823\n",
      "epoch: 3\t| Loss: 2.1406553953886034\t| precision: 0.5313513513513514\n",
      "epoch: 3\t| Loss: 2.1328239530324935\t| precision: 0.5709156193895871\n",
      "epoch: 3\t| Loss: 2.1378656536340714\t| precision: 0.5687203791469194\n",
      "epoch: 3\t| Loss: 2.1327726316452025\t| precision: 0.5900168255748738\n",
      "mid epoch: 1775852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t| Loss: 2.1196759086847305\t| precision: 0.5887902330743618\n",
      "epoch: 3\t| Loss: 2.1408409905433654\t| precision: 0.5411585365853658\n",
      "epoch: 3\t| Loss: 2.1458292269706725\t| precision: 0.5787781350482315\n",
      "epoch: 3\t| Loss: 2.13547288775444\t| precision: 0.5448648648648649\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a442351e9e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'to__save: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"** ** * Saving fine - tuned model ** ** * \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a442351e9e3d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, e, validload, optim)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_accumulation_steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_accumulation_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtemp_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/well/rahimi/users/gra027/conda/envs/MLGPvision/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/well/rahimi/users/gra027/conda/envs/MLGPvision/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(model, e, validload, optim):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(validload):\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "        age_ids, year_ids,input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "#         print(batch)\n",
    "        loss, pred, label = model(input_ids, age_ids, segment_ids, posi_ids,year_ids,  attention_mask=attMask,\n",
    "                                  masked_lm_labels=masked_label)\n",
    "\n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if step % 200 == 0:\n",
    "            print(\"epoch: {}\\t| Loss: {}\\t| precision: {}\".format(e, temp_loss / 200, cal_acc(label, pred)))\n",
    "            temp_loss = 0\n",
    "\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        if (step+1 )%3000 ==0:\n",
    "            print(\"mid epoch: \" +str(step *148) + \"..... ** ** * Saving fine - tuned model ** ** * \")\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "print('starting epoch0')\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_CEHR_newcut1985_2020_DMProc__6msummary.bin\")\n",
    "\n",
    "print('to__save: ',output_model_file)\n",
    "for e in range(50):\n",
    "    train(model, e, trainload, optim)\n",
    "\n",
    "    print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "    #         create_folder(global_params['output_dir'])\n",
    "    print('done epoch', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, e, validload, optim):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(validload):\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "        age_ids, year_ids,input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "#         print(batch)\n",
    "        print(age_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python MLGPvision",
   "language": "python",
   "name": "mlgpvision"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
