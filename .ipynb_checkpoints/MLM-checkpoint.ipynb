{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting run....\n",
      "starting run....\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "print('starting run....')\n",
    "\n",
    "import sys\n",
    "\n",
    "print('starting run....')\n",
    "\n",
    "sys.path.insert(0,'/gpfs3/well/rahimi/users/gra027/JNb/')\n",
    "\n",
    "from general_model_newCutCPRD.ModelPkg.BEHRTraw import *\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from general_model_newCutCPRD.ModelPkg import utils\n",
    "from general_model_newCutCPRD.ModelPkg.MLMRaw import *\n",
    "\n",
    "from general_model_newCutCPRD.ModelPkg.DataProc import *\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from general_model_newCutCPRD.pytorch_pretrained_bert  import optimizer\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3662\n",
      "read data....\n"
     ]
    }
   ],
   "source": [
    "file_config = {\n",
    "        'vocab': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/GeneralVocDM_25k',\n",
    "    'oldvocab': '/gpfs3/well/rahimi/users/gra027/JNb/ExpHypCancer/Data/AllSubclass/DMBp',\n",
    "    'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_for_pretraining_28M_1985_2020__unique_per6m_50pc_sample.parquet/',\n",
    "\n",
    "#     'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_for_pretraining_28M_1985_2020__unique_per6m_50pc_sample___10kdebug.parquet/',\n",
    "    #\n",
    "    'yearVocab':  '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/yearVoc_1985_2021',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 148,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'output_dir':'/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/SavedModels/',\n",
    "    'output_name': 'MLM_newcut1985_2020_DM.bin',\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 250,\n",
    "    'max_age': 110,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    'yearOn':True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "YearVocab = utils.load_obj(file_config['yearVocab'])\n",
    "create_folder(global_params['output_dir'])\n",
    "BertVocab = utils.load_obj(file_config['vocab'])\n",
    "print(len(BertVocab['token2idx']))\n",
    "\n",
    "ageVocab, _ = utils.age_vocab(max_age=global_params['max_age'], year=global_params['age_year'], symbol=global_params['age_symbol'])\n",
    "fulldata = pd.read_parquet(file_config['fulld'])\n",
    "print('read data....')\n",
    "\n",
    "trainSet = MLMLoader(token2idx=BertVocab['token2idx'], dataframe=fulldata, max_len=global_params['max_len_seq'], max_age=global_params['max_age'], year=global_params['age_year'], age_symbol=global_params['age_symbol'],year2idx = YearVocab['token2idx'] )\n",
    "trainload = DataLoader(dataset=trainSet, batch_size=global_params['batch_size'], shuffle=True)\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 150, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()), # number of vocab for age embedding\n",
    "\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.15, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 6, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.15, # multi-head attention dropout rate\n",
    "    'intermediate_size': 108, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range,\n",
    "    'yearOn':True,\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()),\n",
    "\n",
    "}\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForMaskedLM(conf)\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_newcut1985_2020_DM.bin\")\n",
    "model = toLoad(model, output_model_file)\n",
    "model = model.to(global_params['device'])\n",
    "optim = optimizer.adam(params=list(model.named_parameters()), config=optim_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# YearVocab = {'token2idx': {'PAD': 0,\n",
    "#   '1987': 1,\n",
    "#   '1988': 2,\n",
    "#   '1989': 3,\n",
    "#   '1990': 4,\n",
    "#   '1991': 5,\n",
    "#   '1992': 6,\n",
    "#   '1993': 7,\n",
    "#   '1994': 8,\n",
    "#   '1995': 9,\n",
    "#   '1996': 10,\n",
    "#   '1997': 11,\n",
    "#   '1998': 12,\n",
    "#   '1999': 13,\n",
    "#   '2000': 14,\n",
    "#   '2001': 15,\n",
    "#   '2002': 16,\n",
    "#   '2003': 17,\n",
    "#   '2004': 18,\n",
    "#   '2005': 19,\n",
    "#   '2006': 20,\n",
    "#   '2007': 21,\n",
    "#   '2008': 22,\n",
    "#   '2009': 23,\n",
    "#   '2010': 24,\n",
    "#   '2011': 25,\n",
    "#   '2012': 26,\n",
    "#   '2013': 27,\n",
    "#   '2014': 28,\n",
    "#   '2015': 29,\n",
    "#  '2016': 30,\n",
    "#   '2017': 31,\n",
    "#   '2018': 32,\n",
    "#   '2019': 33,\n",
    "#   '2020': 34,\n",
    "#  '2021': 35,\n",
    "\n",
    "#  'UNK': 36\n",
    "\n",
    "# },\n",
    "#  'idx2token': {0: 'PAD',\n",
    "#   1: '1987',\n",
    "#   2: '1988',\n",
    "#   3: '1989',\n",
    "#   4: '1990',\n",
    "#   5: '1991',\n",
    "#   6: '1992',\n",
    "#   7: '1993',\n",
    "#   8: '1994',\n",
    "#   9: '1995',\n",
    "#   10: '1996',\n",
    "#   11: '1997',\n",
    "#   12: '1998',\n",
    "#   13: '1999',\n",
    "#   14: '2000',\n",
    "#   15: '2001',\n",
    "#   16: '2002',\n",
    "#   17: '2003',\n",
    "#   18: '2004',\n",
    "#   19: '2005',\n",
    "#   20: '2006',\n",
    "#   21: '2007',\n",
    "#   22: '2008',\n",
    "#   23: '2009',\n",
    "#   24: '2010',\n",
    "#   25: '2011',\n",
    "#   26: '2012',\n",
    "#   27: '2013',\n",
    "#   28: '2014',\n",
    "#   29: '2015',\n",
    "#                30: '2016',\n",
    "#   31: '2017',\n",
    "#   32: '2018',\n",
    "#   33: '2019',\n",
    "#   34: '2020',\n",
    "#   35: '2021',\n",
    "\n",
    "#   36: 'UNK'}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = utils.load_obj(file_config['vocab'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "1918 3662\n"
     ]
    }
   ],
   "source": [
    "# olddic = os.path.join('/gpfs3/well/rahimi/users/gra027/JNb/ExpHypCancer/ModelBins/',\n",
    "#                                              \"BEHRT_mlm_DMBp.bin\")\n",
    "# dd = torch.load(olddic,  map_location='cpu')\n",
    "# modeld = model.state_dict()\n",
    "# modeld['bert.embeddings.age_embeddings.weight'] = dd['bert.embeddings.age_embeddings.weight']\n",
    "# modeld['bert.embeddings.year_embeddings.weight'][:31] = dd['bert.embeddings.year_embeddings.weight']\n",
    "# modeld['bert.embeddings.posi_embeddings.weight'] = dd['bert.embeddings.posi_embeddings.weight']\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for x in BertVocab['idx2token']:\n",
    "#     if 'bnf' not in BertVocab['idx2token'][x] and 'vtm' not in BertVocab['idx2token'][x] and 'BANDAGE' not in BertVocab['idx2token'][x]:\n",
    "#         if BertVocab['idx2token'][x] in BertVocabold['token2idx']:\n",
    "#             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x]]\n",
    "\n",
    "#             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#             count = count+1\n",
    "#         elif     BertVocab['idx2token'][x][:-1] in BertVocabold['token2idx']:\n",
    "#             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x][:-1]]\n",
    "#             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#             count = count+1\n",
    "\n",
    "#         else:\n",
    "#             for y in BertVocabold['token2idx']:\n",
    "#                 if BertVocab['idx2token'][x][:-1] in y :\n",
    "#                     oldindx = BertVocabold['token2idx'][y]\n",
    "#     #                 print(BertVocab['idx2token'][x], y )\n",
    "#                     modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#                     count = count+1\n",
    "#                     break\n",
    "#     else:\n",
    "#         if 'bnf' in BertVocab['idx2token'][x]:\n",
    "            \n",
    "#             text2check = BertVocab['idx2token'][x][4:8]\n",
    "#             if text2check in BertVocabold['token2idx']:\n",
    "#                 oldindx = BertVocabold['token2idx'][text2check]\n",
    "#                 modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#                 count = count+1\n",
    "\n",
    "#             else:\n",
    "#                 for y in BertVocabold['token2idx']:\n",
    "#                     if text2check in y :\n",
    "# #                         print(y, text2check)\n",
    "#                         count = count+1\n",
    "#                         oldindx = BertVocabold['token2idx'][y]\n",
    "#                         modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "\n",
    "#                         break\n",
    "                \n",
    "                \n",
    "# for x in modeld:\n",
    "#     if x not in ['cls.predictions.bias','cls.predictions.decoder.weight','bert.embeddings.segment_embeddings.weight' , 'bert.embeddings.word_embeddings.weight','bert.embeddings.age_embeddings.weight', 'bert.embeddings.year_embeddings.weight', 'bert.embeddings.posi_embeddings.weight']:\n",
    "#         if x in dd:\n",
    "#             print(x)\n",
    "#             modeld[x] = dd[x]\n",
    "# model.load_state_dict(modeld)\n",
    "# print(count, len(BertVocab['idx2token']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch0\n",
      "to__save: output_model_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/rahimi/users/gra027/JNb/Graph/ModelPkg/utils.py:606: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  truepred = logs(torch.tensor(truepred))\n",
      "/gpfs3/well/rahimi/users/gra027/JNb/Graph/ModelPkg/pytorch_pretrained_bert/optimization.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378065146/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 0.03995170593261719\t| precision: 0.31540697674418605\n",
      "epoch: 0\t| Loss: 4.5851280403137205\t| precision: 0.3211177278973889\n",
      "epoch: 0\t| Loss: 4.0995870935916905\t| precision: 0.3135297054418372\n",
      "epoch: 0\t| Loss: 3.955364681482315\t| precision: 0.3344497607655502\n",
      "epoch: 0\t| Loss: 3.867819049358368\t| precision: 0.297029702970297\n",
      "epoch: 0\t| Loss: 3.8215436446666717\t| precision: 0.28539724811362627\n",
      "epoch: 0\t| Loss: 3.7678792369365692\t| precision: 0.3076923076923077\n",
      "epoch: 0\t| Loss: 3.7336873424053194\t| precision: 0.29716117216117216\n",
      "epoch: 0\t| Loss: 3.7043872511386873\t| precision: 0.33451015004413065\n",
      "epoch: 0\t| Loss: 3.658749969005585\t| precision: 0.33488372093023255\n",
      "epoch: 0\t| Loss: 3.639651844501495\t| precision: 0.3408762638420799\n",
      "epoch: 0\t| Loss: 3.6060316443443297\t| precision: 0.32028836251287335\n",
      "epoch: 0\t| Loss: 3.575617370605469\t| precision: 0.3137254901960784\n",
      "epoch: 0\t| Loss: 3.5383382380008697\t| precision: 0.3311491935483871\n",
      "epoch: 0\t| Loss: 3.464460995197296\t| precision: 0.3602036487059822\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 3.410730330944061\t| precision: 0.3793399057008144\n",
      "epoch: 0\t| Loss: 3.3554887163639067\t| precision: 0.365675057208238\n",
      "epoch: 0\t| Loss: 3.2810237085819245\t| precision: 0.392399658411614\n",
      "epoch: 0\t| Loss: 3.2358625388145446\t| precision: 0.3795066413662239\n",
      "epoch: 0\t| Loss: 3.180306947231293\t| precision: 0.4148365056124939\n",
      "epoch: 0\t| Loss: 3.148405419588089\t| precision: 0.3935125906956893\n",
      "epoch: 0\t| Loss: 3.096606720685959\t| precision: 0.41774675972083747\n",
      "epoch: 0\t| Loss: 3.0647435104846954\t| precision: 0.4328824141519251\n",
      "epoch: 0\t| Loss: 3.0264420533180236\t| precision: 0.4251469923111714\n",
      "epoch: 0\t| Loss: 2.9944729840755464\t| precision: 0.4110760918505178\n",
      "epoch: 0\t| Loss: 2.950374732017517\t| precision: 0.4363476733977173\n",
      "epoch: 0\t| Loss: 2.9195845437049868\t| precision: 0.4150776053215078\n",
      "epoch: 0\t| Loss: 2.90651221036911\t| precision: 0.4637554585152838\n",
      "epoch: 0\t| Loss: 2.894566411972046\t| precision: 0.441079034548036\n",
      "epoch: 0\t| Loss: 2.8395810759067537\t| precision: 0.44561551433389546\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.82690341591835\t| precision: 0.4892703862660944\n",
      "epoch: 0\t| Loss: 2.818315786123276\t| precision: 0.48258706467661694\n",
      "epoch: 0\t| Loss: 2.7951574087142945\t| precision: 0.4618705035971223\n",
      "epoch: 0\t| Loss: 2.7883812487125397\t| precision: 0.48712559117183396\n",
      "epoch: 0\t| Loss: 2.76386984705925\t| precision: 0.5024674742036788\n",
      "epoch: 0\t| Loss: 2.731869171857834\t| precision: 0.49144706426259827\n",
      "epoch: 0\t| Loss: 2.724097682237625\t| precision: 0.521484375\n",
      "epoch: 0\t| Loss: 2.708579910993576\t| precision: 0.4868421052631579\n",
      "epoch: 0\t| Loss: 2.6896121180057526\t| precision: 0.48410061699098245\n",
      "epoch: 0\t| Loss: 2.678853225708008\t| precision: 0.4968220338983051\n",
      "epoch: 0\t| Loss: 2.6795647132396696\t| precision: 0.4884437596302003\n",
      "epoch: 0\t| Loss: 2.659473397731781\t| precision: 0.47044088176352705\n",
      "epoch: 0\t| Loss: 2.641658351421356\t| precision: 0.49420160570918825\n",
      "epoch: 0\t| Loss: 2.6410222733020783\t| precision: 0.4990421455938697\n",
      "epoch: 0\t| Loss: 2.6254380881786346\t| precision: 0.4686013320647003\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.62278267621994\t| precision: 0.5104929233772572\n",
      "epoch: 0\t| Loss: 2.613828808069229\t| precision: 0.4733806566104703\n",
      "epoch: 0\t| Loss: 2.6073439943790437\t| precision: 0.49531876950512704\n",
      "epoch: 0\t| Loss: 2.584292851686478\t| precision: 0.4976657329598506\n",
      "epoch: 0\t| Loss: 2.5854067730903627\t| precision: 0.5061064973131412\n",
      "epoch: 0\t| Loss: 2.572910200357437\t| precision: 0.5094509450945095\n",
      "epoch: 0\t| Loss: 2.562240122556686\t| precision: 0.5207846800560486\n",
      "epoch: 0\t| Loss: 2.5545916306972503\t| precision: 0.48811126065500227\n",
      "epoch: 0\t| Loss: 2.546187001466751\t| precision: 0.5179076343072573\n",
      "epoch: 0\t| Loss: 2.532372622489929\t| precision: 0.5282833251352681\n",
      "epoch: 0\t| Loss: 2.529546698331833\t| precision: 0.5203619909502263\n",
      "epoch: 0\t| Loss: 2.519248353242874\t| precision: 0.48364153627311524\n",
      "epoch: 0\t| Loss: 2.5241374564170838\t| precision: 0.5227646736149204\n",
      "epoch: 0\t| Loss: 2.512497270107269\t| precision: 0.5145185939887926\n",
      "epoch: 0\t| Loss: 2.5097962629795076\t| precision: 0.5414634146341464\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.502555527687073\t| precision: 0.4880514705882353\n",
      "epoch: 0\t| Loss: 2.5062416315078737\t| precision: 0.4931748128577719\n",
      "epoch: 0\t| Loss: 2.4940222930908202\t| precision: 0.5258980785296575\n",
      "epoch: 0\t| Loss: 2.468484332561493\t| precision: 0.5059326056003797\n",
      "epoch: 0\t| Loss: 2.465534802675247\t| precision: 0.5046454767726162\n",
      "epoch: 0\t| Loss: 2.478957189321518\t| precision: 0.5073250135648399\n",
      "epoch: 0\t| Loss: 2.4629473376274107\t| precision: 0.5167652859960552\n",
      "epoch: 0\t| Loss: 2.4562751030921937\t| precision: 0.49733278621255644\n",
      "epoch: 0\t| Loss: 2.4515788662433624\t| precision: 0.5215496368038741\n",
      "epoch: 0\t| Loss: 2.451664298772812\t| precision: 0.5270200720535255\n",
      "epoch: 0\t| Loss: 2.443621952533722\t| precision: 0.5240384615384616\n",
      "epoch: 0\t| Loss: 2.4441090512275694\t| precision: 0.5185542168674698\n",
      "epoch: 0\t| Loss: 2.4296647441387176\t| precision: 0.5068675232609658\n",
      "epoch: 0\t| Loss: 2.4343120193481447\t| precision: 0.49317837291561395\n",
      "epoch: 0\t| Loss: 2.428342982530594\t| precision: 0.5040827436037016\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.4245312702655792\t| precision: 0.5199258572752549\n",
      "epoch: 0\t| Loss: 2.4207441842556\t| precision: 0.5212876427829699\n",
      "epoch: 0\t| Loss: 2.4115213668346405\t| precision: 0.5191578947368422\n",
      "epoch: 0\t| Loss: 2.4015291595458983\t| precision: 0.5338417540514776\n",
      "epoch: 0\t| Loss: 2.4046357583999636\t| precision: 0.5050547598989048\n",
      "epoch: 0\t| Loss: 2.4017886364459993\t| precision: 0.5204081632653061\n",
      "epoch: 0\t| Loss: 2.3979076516628264\t| precision: 0.5118358758548133\n",
      "epoch: 0\t| Loss: 2.3888568091392517\t| precision: 0.540990990990991\n",
      "epoch: 0\t| Loss: 2.3863448560237885\t| precision: 0.5098499061913696\n",
      "epoch: 0\t| Loss: 2.3774941051006317\t| precision: 0.5135717363205515\n",
      "epoch: 0\t| Loss: 2.380261378288269\t| precision: 0.531738730450782\n",
      "epoch: 0\t| Loss: 2.376551432609558\t| precision: 0.5189066059225512\n",
      "epoch: 0\t| Loss: 2.3737483441829683\t| precision: 0.5407331975560081\n",
      "epoch: 0\t| Loss: 2.3700114130973815\t| precision: 0.548820179007323\n",
      "epoch: 0\t| Loss: 2.3775295877456664\t| precision: 0.5115185707569346\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.3627354633808135\t| precision: 0.5350673478866698\n",
      "epoch: 0\t| Loss: 2.3518994498252868\t| precision: 0.5170454545454546\n",
      "epoch: 0\t| Loss: 2.353969351053238\t| precision: 0.5108464483198639\n",
      "epoch: 0\t| Loss: 2.3551659631729125\t| precision: 0.5264982664685488\n",
      "epoch: 0\t| Loss: 2.3342983782291413\t| precision: 0.548992673992674\n",
      "epoch: 0\t| Loss: 2.340131313800812\t| precision: 0.5085817524841915\n",
      "epoch: 0\t| Loss: 2.3488083469867704\t| precision: 0.5259570005243839\n",
      "epoch: 0\t| Loss: 2.3324813294410705\t| precision: 0.5228405315614618\n",
      "epoch: 0\t| Loss: 2.3251144790649416\t| precision: 0.4771838331160365\n",
      "epoch: 0\t| Loss: 2.3463356459140776\t| precision: 0.5445652173913044\n",
      "epoch: 0\t| Loss: 2.3355003952980042\t| precision: 0.48853211009174313\n",
      "epoch: 0\t| Loss: 2.33566055059433\t| precision: 0.540045766590389\n",
      "epoch: 0\t| Loss: 2.333900902271271\t| precision: 0.5313396149119214\n",
      "epoch: 0\t| Loss: 2.327906172275543\t| precision: 0.5111607142857143\n",
      "epoch: 0\t| Loss: 2.3205066561698913\t| precision: 0.5547856214811607\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.3221350002288816\t| precision: 0.5508166969147006\n",
      "epoch: 0\t| Loss: 2.326034107208252\t| precision: 0.5506391347099312\n",
      "epoch: 0\t| Loss: 2.3117049920558927\t| precision: 0.5212854757929883\n",
      "epoch: 0\t| Loss: 2.305451647043228\t| precision: 0.5385674931129476\n",
      "epoch: 0\t| Loss: 2.3076981997489927\t| precision: 0.5223148602256008\n",
      "epoch: 0\t| Loss: 2.302170184850693\t| precision: 0.508164275111331\n",
      "epoch: 0\t| Loss: 2.301391448378563\t| precision: 0.5300925925925926\n",
      "epoch: 0\t| Loss: 2.2946122777462006\t| precision: 0.5516316495306214\n",
      "epoch: 0\t| Loss: 2.2875931215286256\t| precision: 0.5362457993278925\n",
      "epoch: 0\t| Loss: 2.297345335483551\t| precision: 0.530296329057939\n",
      "epoch: 0\t| Loss: 2.292007795572281\t| precision: 0.5249252243270189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.2894866997003556\t| precision: 0.5302410186448385\n",
      "epoch: 0\t| Loss: 2.2761635184288025\t| precision: 0.5124378109452736\n",
      "epoch: 0\t| Loss: 2.288372861146927\t| precision: 0.5139107611548557\n",
      "epoch: 0\t| Loss: 2.2916652941703797\t| precision: 0.5042348955392434\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.2868762147426605\t| precision: 0.5458368376787216\n",
      "epoch: 0\t| Loss: 2.28084174990654\t| precision: 0.5377503852080123\n",
      "epoch: 0\t| Loss: 2.2668881464004516\t| precision: 0.5436893203883495\n",
      "epoch: 0\t| Loss: 2.2772352480888367\t| precision: 0.5495634309193631\n",
      "epoch: 0\t| Loss: 2.2695917665958403\t| precision: 0.559645852749301\n",
      "epoch: 0\t| Loss: 2.277605851888657\t| precision: 0.5630896226415094\n",
      "epoch: 0\t| Loss: 2.264287070631981\t| precision: 0.5188866799204771\n",
      "epoch: 0\t| Loss: 2.2545276999473574\t| precision: 0.5240456122954883\n",
      "epoch: 0\t| Loss: 2.261774615049362\t| precision: 0.5412445730824892\n",
      "epoch: 0\t| Loss: 2.2532971405982973\t| precision: 0.5150575730735164\n",
      "epoch: 0\t| Loss: 2.2534834188222885\t| precision: 0.5506709856547894\n",
      "epoch: 0\t| Loss: 2.244502948522568\t| precision: 0.5473684210526316\n",
      "epoch: 0\t| Loss: 2.250367259979248\t| precision: 0.5252969643642763\n",
      "epoch: 0\t| Loss: 2.2386684048175813\t| precision: 0.5392964071856288\n",
      "epoch: 0\t| Loss: 2.252023973464966\t| precision: 0.5434494927216585\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.257897258400917\t| precision: 0.5291499777481086\n",
      "epoch: 0\t| Loss: 2.2449081707000733\t| precision: 0.528682882055989\n",
      "epoch: 0\t| Loss: 2.2502154397964476\t| precision: 0.5442978322337417\n",
      "epoch: 0\t| Loss: 2.2511334240436556\t| precision: 0.5245354930919486\n",
      "epoch: 0\t| Loss: 2.2324263894557954\t| precision: 0.5292646323161581\n",
      "epoch: 0\t| Loss: 2.2411377573013307\t| precision: 0.5467289719626168\n",
      "epoch: 0\t| Loss: 2.239858559370041\t| precision: 0.5311284046692607\n",
      "epoch: 0\t| Loss: 2.241851216554642\t| precision: 0.5464224872231687\n",
      "epoch: 0\t| Loss: 2.2280255651473997\t| precision: 0.5417766051011433\n",
      "epoch: 0\t| Loss: 2.2463733953237535\t| precision: 0.5207964601769911\n",
      "epoch: 0\t| Loss: 2.2459603476524355\t| precision: 0.5830491962981004\n",
      "epoch: 0\t| Loss: 2.237885285615921\t| precision: 0.5445086705202312\n",
      "epoch: 0\t| Loss: 2.2396516263484956\t| precision: 0.5188431200701139\n",
      "epoch: 0\t| Loss: 2.236637408733368\t| precision: 0.5578446909667195\n",
      "epoch: 0\t| Loss: 2.217656903266907\t| precision: 0.5177951388888888\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.2308294624090195\t| precision: 0.562580093976933\n",
      "epoch: 0\t| Loss: 2.2230857974290847\t| precision: 0.552297165200391\n",
      "epoch: 0\t| Loss: 2.2327999448776246\t| precision: 0.541170378042465\n",
      "epoch: 0\t| Loss: 2.213525463938713\t| precision: 0.5675156174915906\n",
      "epoch: 0\t| Loss: 2.2268575567007063\t| precision: 0.5362139917695473\n",
      "epoch: 0\t| Loss: 2.2167578107118606\t| precision: 0.5334997919267582\n",
      "epoch: 0\t| Loss: 2.2124524503946303\t| precision: 0.5330739299610895\n",
      "epoch: 0\t| Loss: 2.2087800443172454\t| precision: 0.5255362614913177\n",
      "epoch: 0\t| Loss: 2.2041748172044753\t| precision: 0.551010349926072\n",
      "epoch: 0\t| Loss: 2.2153124648332594\t| precision: 0.5438428366075707\n",
      "epoch: 0\t| Loss: 2.2171287047863006\t| precision: 0.535483870967742\n",
      "epoch: 0\t| Loss: 2.214294920563698\t| precision: 0.540084388185654\n",
      "epoch: 0\t| Loss: 2.197682731151581\t| precision: 0.5734265734265734\n",
      "epoch: 0\t| Loss: 2.2139681547880175\t| precision: 0.5556503198294243\n",
      "epoch: 0\t| Loss: 2.2200114595890046\t| precision: 0.5524475524475524\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.208339103460312\t| precision: 0.5383488031265267\n",
      "epoch: 0\t| Loss: 2.1915218883752825\t| precision: 0.5677858053548661\n",
      "epoch: 0\t| Loss: 2.203164038658142\t| precision: 0.5691275167785235\n",
      "epoch: 0\t| Loss: 2.2107982033491136\t| precision: 0.5491129381220251\n",
      "epoch: 0\t| Loss: 2.185221005678177\t| precision: 0.5510288065843622\n",
      "epoch: 0\t| Loss: 2.1941725808382033\t| precision: 0.5478306325143754\n",
      "epoch: 0\t| Loss: 2.2010872370004653\t| precision: 0.5512506130456106\n",
      "epoch: 0\t| Loss: 2.1966835290193556\t| precision: 0.5373684210526316\n",
      "epoch: 0\t| Loss: 2.1921519780159\t| precision: 0.5531914893617021\n",
      "epoch: 0\t| Loss: 2.1837944483757017\t| precision: 0.5676776822091444\n",
      "epoch: 0\t| Loss: 2.184612813591957\t| precision: 0.5329500221141088\n",
      "epoch: 0\t| Loss: 2.18540032684803\t| precision: 0.559774964838256\n",
      "epoch: 0\t| Loss: 2.1810099720954894\t| precision: 0.5594827586206896\n",
      "epoch: 0\t| Loss: 2.191047204732895\t| precision: 0.5258236865538736\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1790398025512694\t| precision: 0.5239307535641547\n",
      "epoch: 0\t| Loss: 2.186181145310402\t| precision: 0.5443091138177236\n",
      "epoch: 0\t| Loss: 2.179604250788689\t| precision: 0.5472460838807478\n",
      "epoch: 0\t| Loss: 2.1806767106056215\t| precision: 0.5286384976525822\n",
      "epoch: 0\t| Loss: 2.173088490962982\t| precision: 0.5607646791078744\n",
      "epoch: 0\t| Loss: 2.177080849409103\t| precision: 0.5586141433317513\n",
      "epoch: 0\t| Loss: 2.18418267250061\t| precision: 0.5506263048016702\n",
      "epoch: 0\t| Loss: 2.1698032414913175\t| precision: 0.5556612749762131\n",
      "epoch: 0\t| Loss: 2.1743799340724945\t| precision: 0.5414825724104074\n",
      "epoch: 0\t| Loss: 2.164912061095238\t| precision: 0.5355535553555355\n",
      "epoch: 0\t| Loss: 2.181839310526848\t| precision: 0.5302734375\n",
      "epoch: 0\t| Loss: 2.1721253329515458\t| precision: 0.5373514431239389\n",
      "epoch: 0\t| Loss: 2.161377383470535\t| precision: 0.5447189097103918\n",
      "epoch: 0\t| Loss: 2.170741472840309\t| precision: 0.5464106081390032\n",
      "epoch: 0\t| Loss: 2.175480959415436\t| precision: 0.5551265551265552\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1664756405353547\t| precision: 0.5505729945191828\n",
      "epoch: 0\t| Loss: 2.174590390920639\t| precision: 0.5432418436043501\n",
      "epoch: 0\t| Loss: 2.167384136915207\t| precision: 0.5492462311557789\n",
      "epoch: 0\t| Loss: 2.1714317828416823\t| precision: 0.5381120646138314\n",
      "epoch: 0\t| Loss: 2.16730791926384\t| precision: 0.5230024213075061\n",
      "epoch: 0\t| Loss: 2.1619575893878937\t| precision: 0.5472419442927362\n",
      "epoch: 0\t| Loss: 2.166934432387352\t| precision: 0.5527093596059113\n",
      "epoch: 0\t| Loss: 2.153407284617424\t| precision: 0.5653579676674365\n",
      "epoch: 0\t| Loss: 2.174815432429314\t| precision: 0.5274038461538462\n",
      "epoch: 0\t| Loss: 2.1467012047767637\t| precision: 0.5628390596745028\n",
      "epoch: 0\t| Loss: 2.1653194743394852\t| precision: 0.5454952935903182\n",
      "epoch: 0\t| Loss: 2.158474789261818\t| precision: 0.5725765946760422\n",
      "epoch: 0\t| Loss: 2.16339846432209\t| precision: 0.5415584415584416\n",
      "epoch: 0\t| Loss: 2.1534092658758164\t| precision: 0.540213000367242\n",
      "epoch: 0\t| Loss: 2.1510853457450865\t| precision: 0.5549780087964814\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.150627382397652\t| precision: 0.555878084179971\n",
      "epoch: 0\t| Loss: 2.150382165312767\t| precision: 0.5621149897330595\n",
      "epoch: 0\t| Loss: 2.1391634517908096\t| precision: 0.5628284758719542\n",
      "epoch: 0\t| Loss: 2.153219269514084\t| precision: 0.574146130566364\n",
      "epoch: 0\t| Loss: 2.1477390867471695\t| precision: 0.5810205908683975\n",
      "epoch: 0\t| Loss: 2.1480536282062532\t| precision: 0.5550500454959054\n",
      "epoch: 0\t| Loss: 2.1485640144348146\t| precision: 0.550314465408805\n",
      "epoch: 0\t| Loss: 2.148885754942894\t| precision: 0.5431119920713577\n",
      "epoch: 0\t| Loss: 2.1554071950912475\t| precision: 0.5423659435120753\n",
      "epoch: 0\t| Loss: 2.1336438310146333\t| precision: 0.5446685878962536\n",
      "epoch: 0\t| Loss: 2.1486750048398973\t| precision: 0.5744368266405485\n",
      "epoch: 0\t| Loss: 2.151193842291832\t| precision: 0.5022222222222222\n",
      "epoch: 0\t| Loss: 2.1385239613056184\t| precision: 0.527367506516073\n",
      "epoch: 0\t| Loss: 2.1377102947235107\t| precision: 0.5636792452830188\n",
      "epoch: 0\t| Loss: 2.14700807929039\t| precision: 0.541161178509532\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1460840135812758\t| precision: 0.5439377085650723\n",
      "epoch: 0\t| Loss: 2.1442367744445803\t| precision: 0.530397319291527\n",
      "epoch: 0\t| Loss: 2.1324658507108687\t| precision: 0.5640640640640641\n",
      "epoch: 0\t| Loss: 2.138673583269119\t| precision: 0.5316953316953317\n",
      "epoch: 0\t| Loss: 2.136576163172722\t| precision: 0.5724101479915433\n",
      "epoch: 0\t| Loss: 2.1438076132535935\t| precision: 0.5398686679174484\n",
      "epoch: 0\t| Loss: 2.1342403906583787\t| precision: 0.5270669291338582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.1394871443510057\t| precision: 0.5596094552929085\n",
      "epoch: 0\t| Loss: 2.1259844601154327\t| precision: 0.5592611595690098\n",
      "epoch: 0\t| Loss: 2.128884483575821\t| precision: 0.5694648993618066\n",
      "epoch: 0\t| Loss: 2.1309292393922807\t| precision: 0.5827017387427552\n",
      "epoch: 0\t| Loss: 2.1273590463399885\t| precision: 0.5445843828715365\n",
      "epoch: 0\t| Loss: 2.131042472720146\t| precision: 0.545255793616091\n",
      "epoch: 0\t| Loss: 2.1316230195760726\t| precision: 0.5758651286601597\n",
      "epoch: 0\t| Loss: 2.1358372336626053\t| precision: 0.5565789473684211\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.126919071674347\t| precision: 0.5543293718166383\n",
      "epoch: 0\t| Loss: 2.1286863684654236\t| precision: 0.51513671875\n",
      "epoch: 0\t| Loss: 2.135166125893593\t| precision: 0.5503292568203199\n",
      "epoch: 0\t| Loss: 2.1236518806219102\t| precision: 0.574852652259332\n",
      "epoch: 0\t| Loss: 2.1147201681137084\t| precision: 0.5385308697611537\n",
      "epoch: 0\t| Loss: 2.1317164409160614\t| precision: 0.5609514370664024\n",
      "epoch: 0\t| Loss: 2.120262259840965\t| precision: 0.567337807606264\n",
      "epoch: 0\t| Loss: 2.113965457081795\t| precision: 0.5386638611257233\n",
      "epoch: 0\t| Loss: 2.117256686091423\t| precision: 0.5511811023622047\n",
      "epoch: 0\t| Loss: 2.1156493747234344\t| precision: 0.5642201834862385\n",
      "epoch: 0\t| Loss: 2.118893154859543\t| precision: 0.5468227424749164\n",
      "epoch: 0\t| Loss: 2.121652371883392\t| precision: 0.5487309644670051\n",
      "epoch: 0\t| Loss: 2.1129603523015974\t| precision: 0.5767088607594937\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.114140755534172\t| precision: 0.5743371212121212\n",
      "epoch: 0\t| Loss: 2.114422684311867\t| precision: 0.5577523413111343\n",
      "epoch: 0\t| Loss: 2.120126420855522\t| precision: 0.5822643614054657\n",
      "epoch: 0\t| Loss: 2.102429627776146\t| precision: 0.5489181561618062\n",
      "epoch: 0\t| Loss: 2.1161256796121597\t| precision: 0.5506030414263241\n",
      "epoch: 0\t| Loss: 2.1095991384983064\t| precision: 0.5692307692307692\n",
      "epoch: 0\t| Loss: 2.108300747871399\t| precision: 0.5239169675090253\n",
      "epoch: 0\t| Loss: 2.1042074179649353\t| precision: 0.5432307043441585\n",
      "epoch: 0\t| Loss: 2.1189112907648084\t| precision: 0.5697724345212538\n",
      "epoch: 0\t| Loss: 2.099570867419243\t| precision: 0.5594262295081968\n",
      "epoch: 0\t| Loss: 2.114799422621727\t| precision: 0.5974630021141649\n",
      "epoch: 0\t| Loss: 2.104531991481781\t| precision: 0.558768656716418\n",
      "epoch: 0\t| Loss: 2.1147970283031463\t| precision: 0.5625\n",
      "epoch: 0\t| Loss: 2.104989759325981\t| precision: 0.5799399914273468\n",
      "epoch: 0\t| Loss: 2.1135167187452315\t| precision: 0.5612061206120612\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.103134427666664\t| precision: 0.564282580078982\n",
      "epoch: 0\t| Loss: 2.1183085894584655\t| precision: 0.5525200188412623\n",
      "epoch: 0\t| Loss: 2.104347858428955\t| precision: 0.5433944069431051\n",
      "epoch: 0\t| Loss: 2.104211947321892\t| precision: 0.5354535974973931\n",
      "epoch: 0\t| Loss: 2.104718761444092\t| precision: 0.5450999512432959\n",
      "epoch: 0\t| Loss: 2.108696280121803\t| precision: 0.5730137594318686\n",
      "epoch: 0\t| Loss: 2.0947745567560196\t| precision: 0.5591907016788635\n",
      "epoch: 0\t| Loss: 2.114193896651268\t| precision: 0.540090771558245\n",
      "epoch: 0\t| Loss: 2.1076639872789382\t| precision: 0.5566714490674318\n",
      "epoch: 0\t| Loss: 2.108566331863403\t| precision: 0.54753053637812\n",
      "epoch: 0\t| Loss: 2.0865616047382356\t| precision: 0.5450216450216451\n",
      "epoch: 0\t| Loss: 2.092540689110756\t| precision: 0.5598253275109171\n",
      "epoch: 0\t| Loss: 2.1013202077150344\t| precision: 0.5424270072992701\n",
      "epoch: 0\t| Loss: 2.0971683710813522\t| precision: 0.5587378640776699\n",
      "epoch: 0\t| Loss: 2.1056037801504135\t| precision: 0.5288912024986986\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.0911066943407057\t| precision: 0.5786848072562358\n",
      "epoch: 0\t| Loss: 2.0947978281974793\t| precision: 0.5597051597051597\n",
      "epoch: 0\t| Loss: 2.0917513316869734\t| precision: 0.571825764596849\n",
      "epoch: 0\t| Loss: 2.0953239005804063\t| precision: 0.5460176991150443\n",
      "epoch: 0\t| Loss: 2.100765428543091\t| precision: 0.5497017892644135\n",
      "epoch: 0\t| Loss: 2.088582080602646\t| precision: 0.590958605664488\n",
      "epoch: 0\t| Loss: 2.103151115179062\t| precision: 0.5466222645099905\n",
      "epoch: 0\t| Loss: 2.086449075937271\t| precision: 0.5838049629952111\n",
      "epoch: 0\t| Loss: 2.0928485453128816\t| precision: 0.5603190428713859\n",
      "epoch: 0\t| Loss: 2.091376275420189\t| precision: 0.5633165829145729\n",
      "epoch: 0\t| Loss: 2.09170080602169\t| precision: 0.5429223744292238\n",
      "epoch: 0\t| Loss: 2.0991942316293715\t| precision: 0.5583070688878884\n",
      "epoch: 0\t| Loss: 2.0920127493143084\t| precision: 0.5597643097643098\n",
      "epoch: 0\t| Loss: 2.0899745947122574\t| precision: 0.5631111111111111\n",
      "epoch: 0\t| Loss: 2.0887431472539904\t| precision: 0.5666977611940298\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.089894613623619\t| precision: 0.5277296360485269\n",
      "epoch: 0\t| Loss: 2.087399325966835\t| precision: 0.5741908364859184\n",
      "epoch: 0\t| Loss: 2.1021350014209745\t| precision: 0.5506626763574177\n",
      "epoch: 0\t| Loss: 2.0787646234035493\t| precision: 0.5409756097560976\n",
      "epoch: 0\t| Loss: 2.0861613363027574\t| precision: 0.5587761674718197\n",
      "epoch: 0\t| Loss: 2.079119978547096\t| precision: 0.5419489317352788\n",
      "epoch: 0\t| Loss: 2.079989893436432\t| precision: 0.5707522356654392\n",
      "epoch: 0\t| Loss: 2.0944472581148146\t| precision: 0.556479217603912\n",
      "epoch: 0\t| Loss: 2.0928973400592805\t| precision: 0.5506234413965088\n",
      "epoch: 0\t| Loss: 2.0879693120718\t| precision: 0.54828797190518\n",
      "epoch: 0\t| Loss: 2.0775291353464125\t| precision: 0.5729316116690578\n",
      "epoch: 0\t| Loss: 2.0920272320508957\t| precision: 0.5628571428571428\n",
      "epoch: 0\t| Loss: 2.07146123111248\t| precision: 0.5585193249863909\n",
      "epoch: 0\t| Loss: 2.069941970705986\t| precision: 0.529500756429652\n",
      "epoch: 0\t| Loss: 2.087392808198929\t| precision: 0.5415436524673134\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.0868127578496933\t| precision: 0.5405405405405406\n",
      "epoch: 0\t| Loss: 2.085672708749771\t| precision: 0.5456663560111836\n",
      "epoch: 0\t| Loss: 2.0856757229566574\t| precision: 0.5455549420209829\n",
      "epoch: 0\t| Loss: 2.0849870455265047\t| precision: 0.5532928942807626\n",
      "epoch: 0\t| Loss: 2.0816619497537614\t| precision: 0.5634233316352522\n",
      "epoch: 0\t| Loss: 2.085560515522957\t| precision: 0.5370132648694907\n",
      "epoch: 0\t| Loss: 2.0810335528850556\t| precision: 0.561491935483871\n",
      "epoch: 0\t| Loss: 2.085025544166565\t| precision: 0.5569339831634914\n",
      "epoch: 0\t| Loss: 2.080645669102669\t| precision: 0.528169014084507\n",
      "epoch: 0\t| Loss: 2.071937294006348\t| precision: 0.5470833333333334\n",
      "epoch: 0\t| Loss: 2.078807979822159\t| precision: 0.5492957746478874\n",
      "epoch: 0\t| Loss: 2.0647121822834014\t| precision: 0.5948275862068966\n",
      "epoch: 0\t| Loss: 2.0837781369686126\t| precision: 0.5426324503311258\n",
      "epoch: 0\t| Loss: 2.080139607191086\t| precision: 0.5535259133389975\n",
      "epoch: 0\t| Loss: 2.0660424143075944\t| precision: 0.5736961451247166\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.0789053255319594\t| precision: 0.509009009009009\n",
      "epoch: 0\t| Loss: 2.078457958102226\t| precision: 0.5399491094147583\n",
      "epoch: 0\t| Loss: 2.0822692757844923\t| precision: 0.5747236905333974\n",
      "epoch: 0\t| Loss: 2.079482633471489\t| precision: 0.5417236662106704\n",
      "epoch: 0\t| Loss: 2.065677365064621\t| precision: 0.5693187232015245\n",
      "epoch: 0\t| Loss: 2.073997786641121\t| precision: 0.5508433734939759\n",
      "epoch: 0\t| Loss: 2.089569262266159\t| precision: 0.5518324607329843\n",
      "epoch: 0\t| Loss: 2.0686089503765106\t| precision: 0.5341506129597198\n",
      "epoch: 0\t| Loss: 2.070832576751709\t| precision: 0.5760425183973835\n",
      "epoch: 0\t| Loss: 2.0685511749982832\t| precision: 0.5457413249211357\n",
      "epoch: 0\t| Loss: 2.0811534869670867\t| precision: 0.5705384957721407\n",
      "epoch: 0\t| Loss: 2.070205465555191\t| precision: 0.559685177087888\n",
      "epoch: 0\t| Loss: 2.071348597407341\t| precision: 0.5569948186528497\n",
      "epoch: 0\t| Loss: 2.063772603869438\t| precision: 0.5485908649173955\n",
      "epoch: 0\t| Loss: 2.06710647046566\t| precision: 0.5587628865979382\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.061418648958206\t| precision: 0.5763144461459928\n",
      "epoch: 0\t| Loss: 2.0717925345897674\t| precision: 0.5498920086393089\n",
      "epoch: 0\t| Loss: 2.0692226272821426\t| precision: 0.5576828669612175\n",
      "epoch: 0\t| Loss: 2.072823566198349\t| precision: 0.5441758241758242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.0694415152072905\t| precision: 0.5600765916706558\n",
      "epoch: 0\t| Loss: 2.0612795478105546\t| precision: 0.56\n",
      "epoch: 0\t| Loss: 2.0821907407045366\t| precision: 0.5660282258064516\n",
      "epoch: 0\t| Loss: 2.0586420714855196\t| precision: 0.5525369556532161\n",
      "epoch: 0\t| Loss: 2.0529007691144945\t| precision: 0.5467096180395766\n",
      "epoch: 0\t| Loss: 2.061504119634628\t| precision: 0.572345890410959\n",
      "epoch: 0\t| Loss: 2.056060215830803\t| precision: 0.5518248175182482\n",
      "epoch: 0\t| Loss: 2.071199862360954\t| precision: 0.5336951605608322\n",
      "epoch: 0\t| Loss: 2.0710393249988557\t| precision: 0.5865620402157921\n",
      "epoch: 0\t| Loss: 2.0547158682346343\t| precision: 0.5651013874066169\n",
      "epoch: 0\t| Loss: 2.062061892747879\t| precision: 0.5563869588455371\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.051661638021469\t| precision: 0.5888300433317285\n",
      "epoch: 0\t| Loss: 2.0591131055355074\t| precision: 0.5441098784997359\n",
      "epoch: 0\t| Loss: 2.064701561331749\t| precision: 0.5748175182481752\n",
      "epoch: 0\t| Loss: 2.0645558685064316\t| precision: 0.56957928802589\n",
      "epoch: 0\t| Loss: 2.0610813128948213\t| precision: 0.5300198807157057\n",
      "epoch: 0\t| Loss: 2.071609768271446\t| precision: 0.5721530659289995\n",
      "epoch: 0\t| Loss: 2.059111561179161\t| precision: 0.5517241379310345\n",
      "epoch: 0\t| Loss: 2.048848015666008\t| precision: 0.5316001792917974\n",
      "epoch: 0\t| Loss: 2.0603437530994415\t| precision: 0.5830152671755725\n",
      "epoch: 0\t| Loss: 2.052311190366745\t| precision: 0.5369127516778524\n",
      "epoch: 0\t| Loss: 2.0519298160076143\t| precision: 0.5787494123178185\n",
      "epoch: 0\t| Loss: 2.0530807888507843\t| precision: 0.5562913907284768\n",
      "epoch: 0\t| Loss: 2.051070306301117\t| precision: 0.5441098784997359\n",
      "epoch: 0\t| Loss: 2.058131701350212\t| precision: 0.5762286860581746\n",
      "epoch: 0\t| Loss: 2.0569263100624084\t| precision: 0.5566476978789446\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.0589433020353316\t| precision: 0.5588874402433724\n",
      "epoch: 0\t| Loss: 2.0560132592916487\t| precision: 0.5571494678389635\n",
      "epoch: 0\t| Loss: 2.0567918223142625\t| precision: 0.5761396702230844\n",
      "epoch: 0\t| Loss: 2.0592542493343355\t| precision: 0.5831495368328187\n",
      "epoch: 0\t| Loss: 2.05381951212883\t| precision: 0.5773087071240105\n",
      "epoch: 0\t| Loss: 2.0563428473472594\t| precision: 0.5403992395437263\n",
      "epoch: 0\t| Loss: 2.0627597504854203\t| precision: 0.5607436230004323\n",
      "epoch: 0\t| Loss: 2.057561601996422\t| precision: 0.5716145833333334\n",
      "epoch: 0\t| Loss: 2.0645371770858763\t| precision: 0.5490887713109935\n",
      "epoch: 0\t| Loss: 2.05056008040905\t| precision: 0.5749774164408311\n",
      "epoch: 0\t| Loss: 2.044959651827812\t| precision: 0.5617822691777675\n",
      "epoch: 0\t| Loss: 2.050974560379982\t| precision: 0.5593830334190232\n",
      "epoch: 0\t| Loss: 2.053368769288063\t| precision: 0.5701058201058201\n",
      "epoch: 0\t| Loss: 2.0493991619348524\t| precision: 0.5851254480286738\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.0428835058212282\t| precision: 0.5788745387453874\n",
      "epoch: 0\t| Loss: 2.0609717857837677\t| precision: 0.5445873081708834\n",
      "epoch: 0\t| Loss: 2.043478633761406\t| precision: 0.5658163265306122\n",
      "epoch: 0\t| Loss: 2.0500835609436034\t| precision: 0.5510688836104513\n",
      "epoch: 0\t| Loss: 2.0547263616323472\t| precision: 0.5706267539756782\n",
      "epoch: 0\t| Loss: 2.0614508682489396\t| precision: 0.5332369942196532\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "done epoch 0\n",
      "epoch: 1\t| Loss: 0.01093159317970276\t| precision: 0.5450097847358122\n",
      "epoch: 1\t| Loss: 2.0467577427625656\t| precision: 0.5438950554994955\n",
      "epoch: 1\t| Loss: 2.052346593141556\t| precision: 0.5542521994134897\n",
      "epoch: 1\t| Loss: 2.0542935162782667\t| precision: 0.5566964285714285\n",
      "epoch: 1\t| Loss: 2.054948719739914\t| precision: 0.5634346357935359\n",
      "epoch: 1\t| Loss: 2.0428121972084043\t| precision: 0.5369807497467072\n",
      "epoch: 1\t| Loss: 2.047699504494667\t| precision: 0.5465004793863855\n",
      "epoch: 1\t| Loss: 2.052821291089058\t| precision: 0.5758329189457981\n",
      "epoch: 1\t| Loss: 2.0467199116945265\t| precision: 0.559040590405904\n",
      "epoch: 1\t| Loss: 2.0473702335357666\t| precision: 0.5692017005196032\n",
      "epoch: 1\t| Loss: 2.052273304462433\t| precision: 0.5607822410147991\n",
      "epoch: 1\t| Loss: 2.043145701289177\t| precision: 0.5419296663660956\n",
      "epoch: 1\t| Loss: 2.0634384775161743\t| precision: 0.5599626691553896\n",
      "epoch: 1\t| Loss: 2.049147946238518\t| precision: 0.5469637396242901\n",
      "epoch: 1\t| Loss: 2.0332594138383864\t| precision: 0.5575842696629213\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.036784664392471\t| precision: 0.5531471446436015\n",
      "epoch: 1\t| Loss: 2.0525793820619582\t| precision: 0.5525455394675385\n",
      "epoch: 1\t| Loss: 2.0351149940490725\t| precision: 0.5547826086956522\n",
      "epoch: 1\t| Loss: 2.0547543984651564\t| precision: 0.5507662835249042\n",
      "epoch: 1\t| Loss: 2.042794176340103\t| precision: 0.5460526315789473\n",
      "epoch: 1\t| Loss: 2.0517754656076432\t| precision: 0.5512887953708574\n",
      "epoch: 1\t| Loss: 2.035052604675293\t| precision: 0.5699107522311943\n",
      "epoch: 1\t| Loss: 2.048983656167984\t| precision: 0.561618917689859\n",
      "epoch: 1\t| Loss: 2.029734528660774\t| precision: 0.5733743409490334\n",
      "epoch: 1\t| Loss: 2.032935569882393\t| precision: 0.5708235294117647\n",
      "epoch: 1\t| Loss: 2.0503545886278154\t| precision: 0.5626134301270418\n",
      "epoch: 1\t| Loss: 2.04343003153801\t| precision: 0.560227770477442\n",
      "epoch: 1\t| Loss: 2.0339095306396486\t| precision: 0.5676199709161416\n",
      "epoch: 1\t| Loss: 2.0340918987989425\t| precision: 0.5966958211856171\n",
      "epoch: 1\t| Loss: 2.042047555446625\t| precision: 0.542953667953668\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.032885289788246\t| precision: 0.572736520854527\n",
      "epoch: 1\t| Loss: 2.0424241721630096\t| precision: 0.5689505637467476\n",
      "epoch: 1\t| Loss: 2.033498608469963\t| precision: 0.5546218487394958\n",
      "epoch: 1\t| Loss: 2.038900979757309\t| precision: 0.5667138142385667\n",
      "epoch: 1\t| Loss: 2.0351593202352523\t| precision: 0.5504543280726925\n",
      "epoch: 1\t| Loss: 2.0421940475702285\t| precision: 0.5668617739756866\n",
      "epoch: 1\t| Loss: 2.037233542203903\t| precision: 0.5124399825403754\n",
      "epoch: 1\t| Loss: 2.0331557565927505\t| precision: 0.5779691749773346\n",
      "epoch: 1\t| Loss: 2.0306481528282165\t| precision: 0.5731021555763823\n",
      "epoch: 1\t| Loss: 2.0386990773677827\t| precision: 0.5710306406685237\n",
      "epoch: 1\t| Loss: 2.0369208878278733\t| precision: 0.567295046032442\n",
      "epoch: 1\t| Loss: 2.0418538159132003\t| precision: 0.5726573110004527\n",
      "epoch: 1\t| Loss: 2.0435513472557068\t| precision: 0.5311640696608616\n",
      "epoch: 1\t| Loss: 2.0360458981990814\t| precision: 0.5756771109930962\n",
      "epoch: 1\t| Loss: 2.0362948501110076\t| precision: 0.5667368421052632\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.0344492679834367\t| precision: 0.5822088955522239\n",
      "epoch: 1\t| Loss: 2.040471741557121\t| precision: 0.5488607594936709\n",
      "epoch: 1\t| Loss: 2.025071769952774\t| precision: 0.5572550640760645\n",
      "epoch: 1\t| Loss: 2.0341910737752915\t| precision: 0.5826660386245879\n",
      "epoch: 1\t| Loss: 2.0320590001344683\t| precision: 0.5902939804013065\n",
      "epoch: 1\t| Loss: 2.0238653099536896\t| precision: 0.5448307410795974\n",
      "epoch: 1\t| Loss: 2.028115102648735\t| precision: 0.5583825533848251\n",
      "epoch: 1\t| Loss: 2.0405845791101456\t| precision: 0.564567285908473\n",
      "epoch: 1\t| Loss: 2.023703005313873\t| precision: 0.539564787339268\n",
      "epoch: 1\t| Loss: 2.0327176505327227\t| precision: 0.5577963600590261\n",
      "epoch: 1\t| Loss: 2.0276745307445525\t| precision: 0.5770046669495121\n",
      "epoch: 1\t| Loss: 2.023744640946388\t| precision: 0.5572446555819477\n",
      "epoch: 1\t| Loss: 2.0293805742263795\t| precision: 0.5721822541966427\n",
      "epoch: 1\t| Loss: 2.030036598443985\t| precision: 0.5535494460402134\n",
      "epoch: 1\t| Loss: 2.0268296045064926\t| precision: 0.5878730459497868\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.020171660780907\t| precision: 0.5726756874727194\n",
      "epoch: 1\t| Loss: 2.0260997933149336\t| precision: 0.5457685664939551\n",
      "epoch: 1\t| Loss: 2.0304432106018067\t| precision: 0.5447227191413238\n",
      "epoch: 1\t| Loss: 2.034726101756096\t| precision: 0.5708609271523178\n",
      "epoch: 1\t| Loss: 2.034261096119881\t| precision: 0.5424551188743328\n",
      "epoch: 1\t| Loss: 2.0275645703077316\t| precision: 0.536067892503536\n",
      "epoch: 1\t| Loss: 2.024875205159187\t| precision: 0.5454545454545454\n",
      "epoch: 1\t| Loss: 2.0272819483280182\t| precision: 0.5789233576642335\n",
      "epoch: 1\t| Loss: 2.034706517457962\t| precision: 0.5478668054110302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.0257682007551194\t| precision: 0.6069699903194579\n",
      "epoch: 1\t| Loss: 2.027430747151375\t| precision: 0.5580009170105457\n",
      "epoch: 1\t| Loss: 2.026481945514679\t| precision: 0.5623365300784655\n",
      "epoch: 1\t| Loss: 2.0276019626855852\t| precision: 0.583219954648526\n",
      "epoch: 1\t| Loss: 2.0347988700866697\t| precision: 0.5813028344107409\n",
      "epoch: 1\t| Loss: 2.0306069844961168\t| precision: 0.5631789594053745\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.030687106847763\t| precision: 0.5309318715740016\n",
      "epoch: 1\t| Loss: 2.029826741218567\t| precision: 0.5505836575875487\n",
      "epoch: 1\t| Loss: 2.0201482993364333\t| precision: 0.5692786715101193\n",
      "epoch: 1\t| Loss: 2.0300117415189742\t| precision: 0.5324852339845525\n",
      "epoch: 1\t| Loss: 2.021408703327179\t| precision: 0.5782761653474054\n",
      "epoch: 1\t| Loss: 2.0361070758104325\t| precision: 0.5497211895910781\n",
      "epoch: 1\t| Loss: 2.0273842829465867\t| precision: 0.5658986175115207\n",
      "epoch: 1\t| Loss: 2.025838031768799\t| precision: 0.5469703131354209\n",
      "epoch: 1\t| Loss: 2.0230236887931823\t| precision: 0.5798816568047337\n",
      "epoch: 1\t| Loss: 2.01907865524292\t| precision: 0.576\n",
      "epoch: 1\t| Loss: 2.0279229521751403\t| precision: 0.5688622754491018\n",
      "epoch: 1\t| Loss: 2.0170640432834626\t| precision: 0.5780730897009967\n",
      "epoch: 1\t| Loss: 2.0314274507761003\t| precision: 0.5696798493408662\n",
      "epoch: 1\t| Loss: 2.0233900249004364\t| precision: 0.5556081400851869\n",
      "epoch: 1\t| Loss: 2.020058627128601\t| precision: 0.5608614232209738\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.0244858837127686\t| precision: 0.5574034334763949\n",
      "epoch: 1\t| Loss: 2.0190314483642577\t| precision: 0.5837221358216692\n",
      "epoch: 1\t| Loss: 2.028690581917763\t| precision: 0.5402822030040965\n",
      "epoch: 1\t| Loss: 2.019004901051521\t| precision: 0.5467068445975032\n",
      "epoch: 1\t| Loss: 2.0148340564966203\t| precision: 0.5706145706145707\n",
      "epoch: 1\t| Loss: 2.0114608824253084\t| precision: 0.6051732552464617\n",
      "epoch: 1\t| Loss: 2.028242652416229\t| precision: 0.576592082616179\n",
      "epoch: 1\t| Loss: 2.015507044196129\t| precision: 0.5863095238095238\n",
      "epoch: 1\t| Loss: 2.0294129955768585\t| precision: 0.573929064947029\n",
      "epoch: 1\t| Loss: 2.027656816840172\t| precision: 0.5436766623207301\n",
      "epoch: 1\t| Loss: 2.0207295423746108\t| precision: 0.5991080277502477\n",
      "epoch: 1\t| Loss: 2.015399143099785\t| precision: 0.5387931034482759\n",
      "epoch: 1\t| Loss: 2.0157587903738023\t| precision: 0.554698138901498\n",
      "epoch: 1\t| Loss: 2.0117236417531967\t| precision: 0.5545184556639796\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.0184138202667237\t| precision: 0.5555086534402701\n",
      "epoch: 1\t| Loss: 2.016347319483757\t| precision: 0.5389733840304183\n",
      "epoch: 1\t| Loss: 2.014813399910927\t| precision: 0.5608137992038921\n",
      "epoch: 1\t| Loss: 2.0186993324756624\t| precision: 0.5700636942675159\n",
      "epoch: 1\t| Loss: 2.020856060385704\t| precision: 0.5534069981583793\n",
      "epoch: 1\t| Loss: 2.024610164761543\t| precision: 0.5741176470588235\n",
      "epoch: 1\t| Loss: 2.0116442543268205\t| precision: 0.540810953133228\n",
      "epoch: 1\t| Loss: 2.003298826217651\t| precision: 0.5645093945720251\n",
      "epoch: 1\t| Loss: 2.023344646692276\t| precision: 0.5444743935309974\n",
      "epoch: 1\t| Loss: 2.0196449637413023\t| precision: 0.5696087352138307\n",
      "epoch: 1\t| Loss: 2.005528094768524\t| precision: 0.5939927498705334\n",
      "epoch: 1\t| Loss: 2.0200723206996916\t| precision: 0.5648662291771832\n",
      "epoch: 1\t| Loss: 2.011761263012886\t| precision: 0.5723890632003585\n",
      "epoch: 1\t| Loss: 2.010109454393387\t| precision: 0.5464612421762157\n",
      "epoch: 1\t| Loss: 2.01244636118412\t| precision: 0.5575642245480494\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.0167324489355085\t| precision: 0.5720995772663222\n",
      "epoch: 1\t| Loss: 2.0044721096754072\t| precision: 0.5698059630856602\n",
      "epoch: 1\t| Loss: 2.024968814253807\t| precision: 0.5344048216976394\n",
      "epoch: 1\t| Loss: 2.003864077925682\t| precision: 0.5805651958353991\n",
      "epoch: 1\t| Loss: 2.014568731188774\t| precision: 0.5632563256325632\n",
      "epoch: 1\t| Loss: 2.0089266085624695\t| precision: 0.5622337908187411\n",
      "epoch: 1\t| Loss: 2.0043541371822355\t| precision: 0.5540417801998183\n",
      "epoch: 1\t| Loss: 2.0163617616891862\t| precision: 0.5459527824620574\n",
      "epoch: 1\t| Loss: 2.0140478974580764\t| precision: 0.5966069745523092\n",
      "epoch: 1\t| Loss: 2.0152260839939116\t| precision: 0.5717691940867906\n",
      "epoch: 1\t| Loss: 2.0064514952898027\t| precision: 0.5560975609756098\n",
      "epoch: 1\t| Loss: 2.0029121530056\t| precision: 0.560059317844785\n",
      "epoch: 1\t| Loss: 2.014816690683365\t| precision: 0.5930232558139535\n",
      "epoch: 1\t| Loss: 2.02443932056427\t| precision: 0.5684870188003581\n",
      "epoch: 1\t| Loss: 2.007855406999588\t| precision: 0.5603181617322138\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.016927873492241\t| precision: 0.5439642324888226\n",
      "epoch: 1\t| Loss: 2.004860392212868\t| precision: 0.5808580858085809\n",
      "epoch: 1\t| Loss: 2.014886637926102\t| precision: 0.5681121331581253\n",
      "epoch: 1\t| Loss: 2.000205425620079\t| precision: 0.5902335456475584\n",
      "epoch: 1\t| Loss: 2.0077357763051986\t| precision: 0.5866099893730075\n",
      "epoch: 1\t| Loss: 2.0074451893568037\t| precision: 0.5634134842055634\n",
      "epoch: 1\t| Loss: 2.0144536197185516\t| precision: 0.5690168818272096\n",
      "epoch: 1\t| Loss: 2.0040179884433744\t| precision: 0.5736129314110966\n",
      "epoch: 1\t| Loss: 1.998377923965454\t| precision: 0.5683952633728052\n",
      "epoch: 1\t| Loss: 2.0010972303152084\t| precision: 0.5868613138686132\n",
      "epoch: 1\t| Loss: 2.006067441701889\t| precision: 0.5662337662337662\n",
      "epoch: 1\t| Loss: 2.0056725466251373\t| precision: 0.5754553339115351\n",
      "epoch: 1\t| Loss: 2.005685676932335\t| precision: 0.5522613065326634\n",
      "epoch: 1\t| Loss: 2.0023086994886397\t| precision: 0.5796366708914238\n",
      "epoch: 1\t| Loss: 2.002634584903717\t| precision: 0.5785085687818434\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.9959045392274857\t| precision: 0.5543175487465181\n",
      "epoch: 1\t| Loss: 2.0208788931369783\t| precision: 0.5614208189442526\n",
      "epoch: 1\t| Loss: 2.0098281615972517\t| precision: 0.5693730729701952\n",
      "epoch: 1\t| Loss: 2.005443521142006\t| precision: 0.5620915032679739\n",
      "epoch: 1\t| Loss: 2.0015926057100297\t| precision: 0.5835219556360344\n",
      "epoch: 1\t| Loss: 2.0120840281248094\t| precision: 0.5730994152046783\n",
      "epoch: 1\t| Loss: 2.0040901482105253\t| precision: 0.5844861660079052\n",
      "epoch: 1\t| Loss: 2.00456702709198\t| precision: 0.5922284644194756\n",
      "epoch: 1\t| Loss: 2.0031865572929384\t| precision: 0.5825515947467167\n",
      "epoch: 1\t| Loss: 2.004739902615547\t| precision: 0.5898722627737226\n",
      "epoch: 1\t| Loss: 2.0070289278030398\t| precision: 0.5380710659898477\n",
      "epoch: 1\t| Loss: 2.000480933189392\t| precision: 0.5602960383108402\n",
      "epoch: 1\t| Loss: 2.00242398917675\t| precision: 0.5880149812734082\n",
      "epoch: 1\t| Loss: 2.0027778989076612\t| precision: 0.5687041564792176\n",
      "epoch: 1\t| Loss: 1.9950281476974487\t| precision: 0.5568645219755324\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.001787532567978\t| precision: 0.5585546364823684\n",
      "epoch: 1\t| Loss: 2.0135932469367983\t| precision: 0.5695426685525695\n",
      "epoch: 1\t| Loss: 1.9955703520774841\t| precision: 0.6016873889875666\n",
      "epoch: 1\t| Loss: 2.007625579237938\t| precision: 0.5594618055555556\n",
      "epoch: 1\t| Loss: 2.0059516233205796\t| precision: 0.5731051344743276\n",
      "epoch: 1\t| Loss: 2.018984562754631\t| precision: 0.5676172953081877\n",
      "epoch: 1\t| Loss: 2.002396406531334\t| precision: 0.5577172503242542\n",
      "epoch: 1\t| Loss: 1.9993238019943238\t| precision: 0.5473870682019486\n",
      "epoch: 1\t| Loss: 2.0149896639585494\t| precision: 0.5600691443388073\n",
      "epoch: 1\t| Loss: 2.0047320663928985\t| precision: 0.5981122702434177\n",
      "epoch: 1\t| Loss: 2.007836084961891\t| precision: 0.5710253998118533\n",
      "epoch: 1\t| Loss: 1.9911104267835618\t| precision: 0.5951073389915127\n",
      "epoch: 1\t| Loss: 1.992853183746338\t| precision: 0.5507590132827325\n",
      "epoch: 1\t| Loss: 1.9991390603780745\t| precision: 0.5755395683453237\n",
      "epoch: 1\t| Loss: 1.9966263896226883\t| precision: 0.5913978494623656\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.9999886786937713\t| precision: 0.5487428319364799\n",
      "epoch: 1\t| Loss: 2.005112910866737\t| precision: 0.5686619718309859\n",
      "epoch: 1\t| Loss: 1.9950297379493713\t| precision: 0.5702259636685866\n",
      "epoch: 1\t| Loss: 1.9970450431108475\t| precision: 0.5585867620751341\n",
      "epoch: 1\t| Loss: 2.011418495774269\t| precision: 0.5691318327974276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.0145832270383837\t| precision: 0.5589902112313241\n",
      "epoch: 1\t| Loss: 2.0080696320533753\t| precision: 0.5487204724409449\n",
      "epoch: 1\t| Loss: 2.0030487579107286\t| precision: 0.6112232539362131\n",
      "epoch: 1\t| Loss: 2.0010087984800338\t| precision: 0.5521140609636185\n",
      "epoch: 1\t| Loss: 1.9976227235794068\t| precision: 0.5585152838427948\n",
      "epoch: 1\t| Loss: 1.9977854150533676\t| precision: 0.6119891008174387\n",
      "epoch: 1\t| Loss: 1.986173300743103\t| precision: 0.5812831644958997\n",
      "epoch: 1\t| Loss: 1.985068097114563\t| precision: 0.5539494062983996\n",
      "epoch: 1\t| Loss: 1.9951268023252486\t| precision: 0.5979425632233176\n",
      "epoch: 1\t| Loss: 2.01410526573658\t| precision: 0.5615976900866217\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.99600624024868\t| precision: 0.6007924715205547\n",
      "epoch: 1\t| Loss: 1.9848528826236724\t| precision: 0.5585464333781965\n",
      "epoch: 1\t| Loss: 2.0022016668319704\t| precision: 0.5704665704665705\n",
      "epoch: 1\t| Loss: 2.0013605684041975\t| precision: 0.5767032967032967\n",
      "epoch: 1\t| Loss: 1.9958468848466873\t| precision: 0.5920934411500449\n",
      "epoch: 1\t| Loss: 1.9936483371257783\t| precision: 0.5718144979738856\n",
      "epoch: 1\t| Loss: 1.9967718505859375\t| precision: 0.5720319099014547\n",
      "epoch: 1\t| Loss: 1.9934612250328063\t| precision: 0.5783018867924529\n",
      "epoch: 1\t| Loss: 1.9984886848926544\t| precision: 0.5683851725607159\n",
      "epoch: 1\t| Loss: 1.995741081237793\t| precision: 0.5434574976122254\n",
      "epoch: 1\t| Loss: 2.0098619079589843\t| precision: 0.5882608695652174\n",
      "epoch: 1\t| Loss: 1.994557369351387\t| precision: 0.5651808242220353\n",
      "epoch: 1\t| Loss: 1.990068188905716\t| precision: 0.5753356539035306\n",
      "epoch: 1\t| Loss: 1.991266312599182\t| precision: 0.5878037597432371\n",
      "epoch: 1\t| Loss: 1.9887595492601395\t| precision: 0.5584988962472406\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.9988308680057525\t| precision: 0.5560951918406993\n",
      "epoch: 1\t| Loss: 1.9954384928941726\t| precision: 0.596466759646676\n",
      "epoch: 1\t| Loss: 1.9967146706581116\t| precision: 0.5730633802816901\n",
      "epoch: 1\t| Loss: 1.994980090856552\t| precision: 0.5911552346570397\n",
      "epoch: 1\t| Loss: 1.988595237135887\t| precision: 0.5625535561268209\n",
      "epoch: 1\t| Loss: 1.9913714689016342\t| precision: 0.5845314823996034\n",
      "epoch: 1\t| Loss: 1.983771061897278\t| precision: 0.5625920471281296\n",
      "epoch: 1\t| Loss: 1.9900564473867417\t| precision: 0.57877639415268\n",
      "epoch: 1\t| Loss: 1.9945586729049682\t| precision: 0.5525629887054735\n",
      "epoch: 1\t| Loss: 1.989749396443367\t| precision: 0.5766283524904214\n",
      "epoch: 1\t| Loss: 1.997963815331459\t| precision: 0.5800273597811217\n",
      "epoch: 1\t| Loss: 1.9845918023586273\t| precision: 0.5836128413991375\n",
      "epoch: 1\t| Loss: 1.9998113632202148\t| precision: 0.5906374501992032\n",
      "epoch: 1\t| Loss: 1.9963557028770447\t| precision: 0.5797625193598348\n",
      "epoch: 1\t| Loss: 1.9883383882045746\t| precision: 0.5631324241091069\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.9948512625694275\t| precision: 0.5806159420289855\n",
      "epoch: 1\t| Loss: 1.9844084602594376\t| precision: 0.5787865560890441\n",
      "epoch: 1\t| Loss: 1.9884649515151978\t| precision: 0.6037827352085354\n",
      "epoch: 1\t| Loss: 1.9953253853321076\t| precision: 0.5564053537284895\n",
      "epoch: 1\t| Loss: 1.9920124870538711\t| precision: 0.5672029060716139\n",
      "epoch: 1\t| Loss: 1.994713042974472\t| precision: 0.5692850838481907\n",
      "epoch: 1\t| Loss: 1.9923380398750306\t| precision: 0.6323462414578588\n",
      "epoch: 1\t| Loss: 1.9871468818187714\t| precision: 0.5818266110338433\n",
      "epoch: 1\t| Loss: 1.9892530256509782\t| precision: 0.5715871254162043\n",
      "epoch: 1\t| Loss: 1.9943288141489028\t| precision: 0.5700197238658777\n",
      "epoch: 1\t| Loss: 1.9940113151073455\t| precision: 0.5360272638753651\n",
      "epoch: 1\t| Loss: 1.9907601487636566\t| precision: 0.6049651567944251\n",
      "epoch: 1\t| Loss: 1.9831470602750778\t| precision: 0.5412608918503332\n",
      "epoch: 1\t| Loss: 1.9896511763334275\t| precision: 0.5482160077145612\n",
      "epoch: 1\t| Loss: 1.9854879814386368\t| precision: 0.5978695073235686\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.992489697933197\t| precision: 0.53737658674189\n",
      "epoch: 1\t| Loss: 1.9919219940900803\t| precision: 0.5447154471544715\n",
      "epoch: 1\t| Loss: 1.9891861963272095\t| precision: 0.5542667771333886\n",
      "epoch: 1\t| Loss: 1.9802244526147843\t| precision: 0.5578093306288032\n",
      "epoch: 1\t| Loss: 1.9845083731412887\t| precision: 0.5646594274432379\n",
      "epoch: 1\t| Loss: 1.9806842589378357\t| precision: 0.566607460035524\n",
      "epoch: 1\t| Loss: 1.9930280542373657\t| precision: 0.5722698072805139\n",
      "epoch: 1\t| Loss: 1.9963614076375962\t| precision: 0.5426829268292683\n",
      "epoch: 1\t| Loss: 1.988604262471199\t| precision: 0.540401681457263\n",
      "epoch: 1\t| Loss: 1.982591821551323\t| precision: 0.531416400425985\n",
      "epoch: 1\t| Loss: 1.9959044116735458\t| precision: 0.5727630285152409\n",
      "epoch: 1\t| Loss: 1.9791125631332398\t| precision: 0.595309381237525\n",
      "epoch: 1\t| Loss: 1.9809199959039687\t| precision: 0.5495668912415784\n",
      "epoch: 1\t| Loss: 1.983484584093094\t| precision: 0.5566234851650648\n",
      "epoch: 1\t| Loss: 1.9881079560518264\t| precision: 0.5735361917934532\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.978329616189003\t| precision: 0.5726691042047533\n",
      "epoch: 1\t| Loss: 1.9839307302236557\t| precision: 0.5972434915773354\n",
      "epoch: 1\t| Loss: 1.9879154098033904\t| precision: 0.5835654596100278\n",
      "epoch: 1\t| Loss: 1.9879838585853578\t| precision: 0.5619637750238322\n",
      "epoch: 1\t| Loss: 1.9910012149810792\t| precision: 0.5718879191548002\n",
      "epoch: 1\t| Loss: 1.9734753173589707\t| precision: 0.5476864966949953\n",
      "epoch: 1\t| Loss: 1.9854488509893418\t| precision: 0.5542521994134897\n",
      "epoch: 1\t| Loss: 1.9836874085664749\t| precision: 0.5716272600834492\n",
      "epoch: 1\t| Loss: 1.9839581537246704\t| precision: 0.5485458612975391\n",
      "epoch: 1\t| Loss: 1.9855054289102554\t| precision: 0.5656509695290859\n",
      "epoch: 1\t| Loss: 1.9844672411680222\t| precision: 0.5660653339834227\n",
      "epoch: 1\t| Loss: 1.987372132539749\t| precision: 0.5610724925521351\n",
      "epoch: 1\t| Loss: 1.995077536702156\t| precision: 0.5797297297297297\n",
      "epoch: 1\t| Loss: 1.9818594807386398\t| precision: 0.5283018867924528\n",
      "epoch: 1\t| Loss: 1.9852521210908889\t| precision: 0.5767292716445259\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.983880569934845\t| precision: 0.5757722007722008\n",
      "epoch: 1\t| Loss: 1.9729594165086746\t| precision: 0.5483165719282903\n",
      "epoch: 1\t| Loss: 1.9832633048295976\t| precision: 0.5529680365296804\n",
      "epoch: 1\t| Loss: 1.9887434780597686\t| precision: 0.5907766990291262\n",
      "epoch: 1\t| Loss: 1.9754851770401\t| precision: 0.5735794074793589\n",
      "epoch: 1\t| Loss: 1.9773753505945206\t| precision: 0.5873674504379899\n",
      "epoch: 1\t| Loss: 1.9735221719741822\t| precision: 0.5871681415929203\n",
      "epoch: 1\t| Loss: 1.9795515209436416\t| precision: 0.5594787725935267\n",
      "epoch: 1\t| Loss: 1.9882867735624314\t| precision: 0.5846501128668171\n",
      "epoch: 1\t| Loss: 1.9781511181592941\t| precision: 0.5654981549815498\n",
      "epoch: 1\t| Loss: 1.984345492720604\t| precision: 0.5630885122410546\n",
      "epoch: 1\t| Loss: 1.9817008286714555\t| precision: 0.5698456667985754\n",
      "epoch: 1\t| Loss: 1.9772338896989823\t| precision: 0.5614366729678639\n",
      "epoch: 1\t| Loss: 1.9818615984916688\t| precision: 0.5834464043419267\n",
      "epoch: 1\t| Loss: 1.9680670350790024\t| precision: 0.5795356835769562\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.967953445315361\t| precision: 0.5595446584938704\n",
      "epoch: 1\t| Loss: 1.9853227257728576\t| precision: 0.5669911914696337\n",
      "epoch: 1\t| Loss: 1.978050593137741\t| precision: 0.5377800937988536\n",
      "epoch: 1\t| Loss: 1.9817087644338607\t| precision: 0.5174239082487869\n",
      "epoch: 1\t| Loss: 1.981119560599327\t| precision: 0.5622586872586872\n",
      "epoch: 1\t| Loss: 1.975803462266922\t| precision: 0.5611759619541721\n",
      "epoch: 1\t| Loss: 1.9742487049102784\t| precision: 0.5709876543209876\n",
      "epoch: 1\t| Loss: 1.9817807626724244\t| precision: 0.5883609183128671\n",
      "epoch: 1\t| Loss: 1.9703679072856903\t| precision: 0.5899692036955565\n",
      "epoch: 1\t| Loss: 1.972656107544899\t| precision: 0.5556088207094918\n",
      "epoch: 1\t| Loss: 1.9797217267751694\t| precision: 0.5910132966529115\n",
      "epoch: 1\t| Loss: 1.976252445578575\t| precision: 0.5561797752808989\n",
      "epoch: 1\t| Loss: 1.9768554335832595\t| precision: 0.5655775425678785\n",
      "epoch: 1\t| Loss: 1.9664023768901826\t| precision: 0.5686523024926067\n",
      "epoch: 1\t| Loss: 1.9803089165687562\t| precision: 0.5562239703840814\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 1.9813315641880036\t| precision: 0.5740384615384615\n",
      "epoch: 1\t| Loss: 1.9814665096998214\t| precision: 0.5662763466042154\n",
      "epoch: 1\t| Loss: 1.966296108365059\t| precision: 0.5715547703180212\n",
      "epoch: 1\t| Loss: 1.9748947381973267\t| precision: 0.5947856827220503\n",
      "epoch: 1\t| Loss: 1.9830837053060533\t| precision: 0.5665807560137457\n",
      "epoch: 1\t| Loss: 1.974072361588478\t| precision: 0.5684320922915574\n",
      "epoch: 1\t| Loss: 1.9780784898996353\t| precision: 0.5864839319470699\n",
      "epoch: 1\t| Loss: 1.9778564137220382\t| precision: 0.5803244374672946\n",
      "epoch: 1\t| Loss: 1.9634806495904922\t| precision: 0.5671852899575672\n",
      "epoch: 1\t| Loss: 1.9898029720783235\t| precision: 0.563006300630063\n",
      "epoch: 1\t| Loss: 1.9823061400651931\t| precision: 0.5785785785785785\n",
      "epoch: 1\t| Loss: 1.9726514929533006\t| precision: 0.5673902069661787\n",
      "epoch: 1\t| Loss: 1.9665208441019058\t| precision: 0.5988323603002502\n",
      "epoch: 1\t| Loss: 1.9790889102220535\t| precision: 0.5551932929669307\n",
      "epoch: 1\t| Loss: 1.9745371729135512\t| precision: 0.5763546798029556\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.9673820686340333\t| precision: 0.5953961930057547\n",
      "epoch: 1\t| Loss: 1.9714497476816177\t| precision: 0.5390810042633822\n",
      "epoch: 1\t| Loss: 1.9760591989755631\t| precision: 0.5804967801287948\n",
      "epoch: 1\t| Loss: 1.9651929593086244\t| precision: 0.5647396082178691\n",
      "epoch: 1\t| Loss: 1.9635335969924927\t| precision: 0.5664367816091954\n",
      "epoch: 1\t| Loss: 1.9820956158638001\t| precision: 0.5542911332385017\n",
      "epoch: 1\t| Loss: 1.9762978690862656\t| precision: 0.5686644643699859\n",
      "epoch: 1\t| Loss: 1.9736700975894927\t| precision: 0.5426653883029722\n",
      "epoch: 1\t| Loss: 1.9755602848529816\t| precision: 0.5988603988603989\n",
      "epoch: 1\t| Loss: 1.974227357506752\t| precision: 0.5891358024691358\n",
      "epoch: 1\t| Loss: 1.9782138365507125\t| precision: 0.5924579736483416\n",
      "epoch: 1\t| Loss: 1.9761237049102782\t| precision: 0.5451754385964912\n",
      "epoch: 1\t| Loss: 1.978760882616043\t| precision: 0.5686274509803921\n",
      "epoch: 1\t| Loss: 1.9711377382278443\t| precision: 0.5680449618627057\n",
      "epoch: 1\t| Loss: 1.96754064142704\t| precision: 0.592\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.9560384964942932\t| precision: 0.5613548028872848\n",
      "epoch: 1\t| Loss: 1.9731026083230971\t| precision: 0.5767151767151767\n",
      "epoch: 1\t| Loss: 1.9732793170213698\t| precision: 0.5629770992366412\n",
      "epoch: 1\t| Loss: 1.9779623878002166\t| precision: 0.5540812223562526\n",
      "epoch: 1\t| Loss: 1.9733730602264403\t| precision: 0.5651483050847458\n",
      "epoch: 1\t| Loss: 1.9689235198497772\t| precision: 0.5578575037897928\n",
      "epoch: 1\t| Loss: 1.977687502503395\t| precision: 0.5617828773168579\n",
      "epoch: 1\t| Loss: 1.9766292989253997\t| precision: 0.5806065442936952\n",
      "epoch: 1\t| Loss: 1.9821486055850983\t| precision: 0.5668693009118541\n",
      "epoch: 1\t| Loss: 1.9707775902748108\t| precision: 0.5767554479418886\n",
      "epoch: 1\t| Loss: 1.9827866816520692\t| precision: 0.5645719940623454\n",
      "epoch: 1\t| Loss: 1.9688183498382568\t| precision: 0.5733634311512416\n",
      "epoch: 1\t| Loss: 1.9734015440940857\t| precision: 0.5797101449275363\n",
      "epoch: 1\t| Loss: 1.959603276848793\t| precision: 0.5596416784535596\n",
      "epoch: 1\t| Loss: 1.9653700023889542\t| precision: 0.5989252564728872\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.9694184100627898\t| precision: 0.5599571734475375\n",
      "epoch: 1\t| Loss: 1.9746010148525237\t| precision: 0.5514705882352942\n",
      "epoch: 1\t| Loss: 1.9645553201436996\t| precision: 0.5695331695331696\n",
      "epoch: 1\t| Loss: 1.971234934926033\t| precision: 0.5780299959464937\n",
      "epoch: 1\t| Loss: 1.9791433787345887\t| precision: 0.5576407506702413\n",
      "epoch: 1\t| Loss: 1.9734288418293\t| precision: 0.5914775358962483\n",
      "epoch: 1\t| Loss: 1.9691621267795563\t| precision: 0.5692450208429829\n",
      "epoch: 1\t| Loss: 1.9767586117982865\t| precision: 0.5777439024390244\n",
      "epoch: 1\t| Loss: 1.9692370200157165\t| precision: 0.5811051693404634\n",
      "epoch: 1\t| Loss: 1.9734048461914062\t| precision: 0.5559666975023126\n",
      "epoch: 1\t| Loss: 1.9673186022043228\t| precision: 0.5727058823529412\n",
      "epoch: 1\t| Loss: 1.9677517163753508\t| precision: 0.5968882602545968\n",
      "epoch: 1\t| Loss: 1.9621256738901138\t| precision: 0.573697270471464\n",
      "epoch: 1\t| Loss: 1.9668035912513733\t| precision: 0.5552064631956912\n",
      "epoch: 1\t| Loss: 1.9716563159227372\t| precision: 0.577005577005577\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.966757339835167\t| precision: 0.5746302106678619\n",
      "epoch: 1\t| Loss: 1.97869664311409\t| precision: 0.5993117010816126\n",
      "epoch: 1\t| Loss: 1.9642933475971223\t| precision: 0.5717326521924223\n",
      "epoch: 1\t| Loss: 1.9598081922531128\t| precision: 0.5797165633303809\n",
      "epoch: 1\t| Loss: 1.9675050014257431\t| precision: 0.5657059110893992\n",
      "epoch: 1\t| Loss: 1.9764304894208908\t| precision: 0.5667405764966741\n",
      "epoch: 1\t| Loss: 1.9780537223815917\t| precision: 0.5512995896032832\n",
      "epoch: 1\t| Loss: 1.9593430584669114\t| precision: 0.5747295968534907\n",
      "epoch: 1\t| Loss: 1.9611373823881149\t| precision: 0.5780010576414596\n",
      "epoch: 1\t| Loss: 1.9697985464334489\t| precision: 0.595104538500765\n",
      "epoch: 1\t| Loss: 1.9738372296094895\t| precision: 0.5660770031217481\n",
      "epoch: 1\t| Loss: 1.9613365375995635\t| precision: 0.5611582626060909\n",
      "epoch: 1\t| Loss: 1.9591139668226243\t| precision: 0.5572552447552448\n",
      "epoch: 1\t| Loss: 1.9780442649126053\t| precision: 0.552349930200093\n",
      "epoch: 1\t| Loss: 1.9559880590438843\t| precision: 0.5679120879120879\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.977213968038559\t| precision: 0.560287081339713\n",
      "epoch: 1\t| Loss: 1.970195215344429\t| precision: 0.5578947368421052\n",
      "epoch: 1\t| Loss: 1.970638068318367\t| precision: 0.5640918580375783\n",
      "epoch: 1\t| Loss: 1.9639759570360185\t| precision: 0.5863026376677464\n",
      "epoch: 1\t| Loss: 1.962643525004387\t| precision: 0.5669642857142857\n",
      "epoch: 1\t| Loss: 1.9634291929006578\t| precision: 0.5645719940623454\n",
      "epoch: 1\t| Loss: 1.9691348952054977\t| precision: 0.5649560795191864\n",
      "epoch: 1\t| Loss: 1.9632505995035172\t| precision: 0.5610619469026549\n",
      "epoch: 1\t| Loss: 1.9655384004116059\t| precision: 0.5899100899100899\n",
      "epoch: 1\t| Loss: 1.956831294298172\t| precision: 0.5544303797468354\n",
      "epoch: 1\t| Loss: 1.9749879795312881\t| precision: 0.5466169419286094\n",
      "epoch: 1\t| Loss: 1.973380796313286\t| precision: 0.5723298783235692\n",
      "epoch: 1\t| Loss: 1.9635745644569398\t| precision: 0.5610924772400575\n",
      "epoch: 1\t| Loss: 1.9543879854679107\t| precision: 0.5795344325897187\n",
      "epoch: 1\t| Loss: 1.9683764654397964\t| precision: 0.55819592628516\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 1.962923750281334\t| precision: 0.5957828622700763\n",
      "epoch: 1\t| Loss: 1.974163601398468\t| precision: 0.5663632007666507\n",
      "epoch: 1\t| Loss: 1.9595995491743088\t| precision: 0.5362806343104277\n",
      "epoch: 1\t| Loss: 1.9701436293125152\t| precision: 0.5779418150238819\n",
      "epoch: 1\t| Loss: 1.9523148691654206\t| precision: 0.5588089330024814\n",
      "epoch: 1\t| Loss: 1.9658345878124237\t| precision: 0.5633802816901409\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "done epoch 1\n",
      "epoch: 2\t| Loss: 0.01000278115272522\t| precision: 0.568654019873532\n",
      "epoch: 2\t| Loss: 1.9663081294298173\t| precision: 0.5658119658119658\n",
      "epoch: 2\t| Loss: 1.9665841782093048\t| precision: 0.5525618374558304\n",
      "epoch: 2\t| Loss: 1.968120791912079\t| precision: 0.5927857453281182\n",
      "epoch: 2\t| Loss: 1.972271266579628\t| precision: 0.5714948932219127\n",
      "epoch: 2\t| Loss: 1.9630222779512405\t| precision: 0.5594441585177561\n",
      "epoch: 2\t| Loss: 1.951484466791153\t| precision: 0.5706887622993267\n",
      "epoch: 2\t| Loss: 1.9588582265377044\t| precision: 0.574160447761194\n",
      "epoch: 2\t| Loss: 1.9581339329481124\t| precision: 0.57466732380483\n",
      "epoch: 2\t| Loss: 1.9634856700897216\t| precision: 0.5328689595124075\n",
      "epoch: 2\t| Loss: 1.9674464160203933\t| precision: 0.5511887433284813\n",
      "epoch: 2\t| Loss: 1.9585082584619522\t| precision: 0.5994962216624685\n",
      "epoch: 2\t| Loss: 1.9688148498535156\t| precision: 0.5715691096901131\n",
      "epoch: 2\t| Loss: 1.9476649165153503\t| precision: 0.5673208580556823\n",
      "epoch: 2\t| Loss: 1.9729087525606155\t| precision: 0.5799293998991427\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.968395403623581\t| precision: 0.5782891445722862\n",
      "epoch: 2\t| Loss: 1.9621354246139526\t| precision: 0.5576540755467196\n",
      "epoch: 2\t| Loss: 1.9577539938688278\t| precision: 0.5610260653702938\n",
      "epoch: 2\t| Loss: 1.9758812642097474\t| precision: 0.5848499754058042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 1.958699929714203\t| precision: 0.572007200720072\n",
      "epoch: 2\t| Loss: 1.955981684923172\t| precision: 0.5607863424728401\n",
      "epoch: 2\t| Loss: 1.9609556269645692\t| precision: 0.56784140969163\n",
      "epoch: 2\t| Loss: 1.953951245546341\t| precision: 0.5872093023255814\n",
      "epoch: 2\t| Loss: 1.9683180725574494\t| precision: 0.5560578661844484\n",
      "epoch: 2\t| Loss: 1.9641692101955415\t| precision: 0.5737951807228916\n",
      "epoch: 2\t| Loss: 1.9595992201566697\t| precision: 0.5580246913580247\n",
      "epoch: 2\t| Loss: 1.950548529624939\t| precision: 0.570353982300885\n",
      "epoch: 2\t| Loss: 1.9543643879890442\t| precision: 0.5632183908045977\n",
      "epoch: 2\t| Loss: 1.966978240609169\t| precision: 0.5614489003880984\n",
      "epoch: 2\t| Loss: 1.9626203930377961\t| precision: 0.5727029438001784\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9641633969545365\t| precision: 0.582860092927207\n",
      "epoch: 2\t| Loss: 1.959433863759041\t| precision: 0.5735364112327463\n",
      "epoch: 2\t| Loss: 1.9600973850488663\t| precision: 0.5536927956502039\n",
      "epoch: 2\t| Loss: 1.9560286438465118\t| precision: 0.5500754906894816\n",
      "epoch: 2\t| Loss: 1.956661673784256\t| precision: 0.5744779582366589\n",
      "epoch: 2\t| Loss: 1.9578295212984085\t| precision: 0.5619834710743802\n",
      "epoch: 2\t| Loss: 1.9482370626926422\t| precision: 0.6036981509245377\n",
      "epoch: 2\t| Loss: 1.9567584472894668\t| precision: 0.5511695906432749\n",
      "epoch: 2\t| Loss: 1.9540403777360915\t| precision: 0.573921028466483\n",
      "epoch: 2\t| Loss: 1.9631536251306534\t| precision: 0.5844226579520697\n",
      "epoch: 2\t| Loss: 1.9560253882408143\t| precision: 0.5846313603322949\n",
      "epoch: 2\t| Loss: 1.9570319604873658\t| precision: 0.5570798628123469\n",
      "epoch: 2\t| Loss: 1.9560234063863755\t| precision: 0.5835117773019272\n",
      "epoch: 2\t| Loss: 1.9601591622829437\t| precision: 0.5645869947275922\n",
      "epoch: 2\t| Loss: 1.9575510704517365\t| precision: 0.5904255319148937\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.959320254921913\t| precision: 0.5620567375886525\n",
      "epoch: 2\t| Loss: 1.966455615758896\t| precision: 0.5416666666666666\n",
      "epoch: 2\t| Loss: 1.9609009051322936\t| precision: 0.5801457194899818\n",
      "epoch: 2\t| Loss: 1.9579781436920165\t| precision: 0.555662188099808\n",
      "epoch: 2\t| Loss: 1.9631584399938584\t| precision: 0.5537078651685393\n",
      "epoch: 2\t| Loss: 1.9570902556180954\t| precision: 0.5765124555160143\n",
      "epoch: 2\t| Loss: 1.958512498140335\t| precision: 0.5926456542502387\n",
      "epoch: 2\t| Loss: 1.9668719601631164\t| precision: 0.559009227780476\n",
      "epoch: 2\t| Loss: 1.9539148592948914\t| precision: 0.5468564650059312\n",
      "epoch: 2\t| Loss: 1.9624693596363068\t| precision: 0.5641293013555787\n",
      "epoch: 2\t| Loss: 1.9541109037399291\t| precision: 0.6019507663725034\n",
      "epoch: 2\t| Loss: 1.9527873659133912\t| precision: 0.5468409586056645\n",
      "epoch: 2\t| Loss: 1.9618894600868224\t| precision: 0.5841874084919473\n",
      "epoch: 2\t| Loss: 1.9473556721210479\t| precision: 0.5810028929604629\n",
      "epoch: 2\t| Loss: 1.9529070198535918\t| precision: 0.5606714628297362\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9453440845012664\t| precision: 0.6063829787234043\n",
      "epoch: 2\t| Loss: 1.9689907431602478\t| precision: 0.5770733825445071\n",
      "epoch: 2\t| Loss: 1.9534004867076873\t| precision: 0.5476419634263715\n",
      "epoch: 2\t| Loss: 1.939757890701294\t| precision: 0.5459081836327345\n",
      "epoch: 2\t| Loss: 1.9530546230077743\t| precision: 0.5545645330535152\n",
      "epoch: 2\t| Loss: 1.9530691355466843\t| precision: 0.5617095508068033\n",
      "epoch: 2\t| Loss: 1.9481645315885543\t| precision: 0.5626007522837184\n",
      "epoch: 2\t| Loss: 1.960694352388382\t| precision: 0.5502547475683187\n",
      "epoch: 2\t| Loss: 1.9468459951877595\t| precision: 0.5697848924462231\n",
      "epoch: 2\t| Loss: 1.955631737112999\t| precision: 0.576808721506442\n",
      "epoch: 2\t| Loss: 1.956725640296936\t| precision: 0.5771315919172005\n",
      "epoch: 2\t| Loss: 1.9690525376796721\t| precision: 0.5685117967332124\n",
      "epoch: 2\t| Loss: 1.9594348573684692\t| precision: 0.57135703555333\n",
      "epoch: 2\t| Loss: 1.9610717302560807\t| precision: 0.5867158671586716\n",
      "epoch: 2\t| Loss: 1.9616694927215577\t| precision: 0.5485576923076924\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.965192112326622\t| precision: 0.5728202630297126\n",
      "epoch: 2\t| Loss: 1.9591953426599502\t| precision: 0.5755159474671669\n",
      "epoch: 2\t| Loss: 1.9455908077955246\t| precision: 0.5644114921223355\n",
      "epoch: 2\t| Loss: 1.9553201913833618\t| precision: 0.5618181818181818\n",
      "epoch: 2\t| Loss: 1.9541353052854538\t| precision: 0.5560821484992101\n",
      "epoch: 2\t| Loss: 1.9566439110040665\t| precision: 0.5674300254452926\n",
      "epoch: 2\t| Loss: 1.9495417803525925\t| precision: 0.5623398804440649\n",
      "epoch: 2\t| Loss: 1.955185561776161\t| precision: 0.5786864931846345\n",
      "epoch: 2\t| Loss: 1.9523895770311355\t| precision: 0.564755838641189\n",
      "epoch: 2\t| Loss: 1.9576783257722854\t| precision: 0.5603328710124826\n",
      "epoch: 2\t| Loss: 1.9568701434135436\t| precision: 0.5870307167235495\n",
      "epoch: 2\t| Loss: 1.9478186821937562\t| precision: 0.5879959308240081\n",
      "epoch: 2\t| Loss: 1.9478370064496995\t| precision: 0.5957240038872692\n",
      "epoch: 2\t| Loss: 1.9562770128250122\t| precision: 0.5618336886993603\n",
      "epoch: 2\t| Loss: 1.9509894049167633\t| precision: 0.5822245875465674\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9620710510015487\t| precision: 0.5744327614831212\n",
      "epoch: 2\t| Loss: 1.9613684964179994\t| precision: 0.5740167918692002\n",
      "epoch: 2\t| Loss: 1.937589396238327\t| precision: 0.5663716814159292\n",
      "epoch: 2\t| Loss: 1.9609396171569824\t| precision: 0.5847953216374269\n",
      "epoch: 2\t| Loss: 1.9666335225105285\t| precision: 0.5857637172516065\n",
      "epoch: 2\t| Loss: 1.9406023782491684\t| precision: 0.5760986066452305\n",
      "epoch: 2\t| Loss: 1.9548707854747773\t| precision: 0.5468509984639017\n",
      "epoch: 2\t| Loss: 1.9458685970306397\t| precision: 0.5697030699547055\n",
      "epoch: 2\t| Loss: 1.9512694495916367\t| precision: 0.5491470349309504\n",
      "epoch: 2\t| Loss: 1.9503627049922942\t| precision: 0.5848968105065666\n",
      "epoch: 2\t| Loss: 1.949517816901207\t| precision: 0.5863849765258216\n",
      "epoch: 2\t| Loss: 1.944790753722191\t| precision: 0.5842418235877106\n",
      "epoch: 2\t| Loss: 1.9483936774730681\t| precision: 0.5616151022548506\n",
      "epoch: 2\t| Loss: 1.954811568260193\t| precision: 0.5734302023871303\n",
      "epoch: 2\t| Loss: 1.9473075276613236\t| precision: 0.5768482490272373\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.957892432808876\t| precision: 0.5558217537134643\n",
      "epoch: 2\t| Loss: 1.946844619512558\t| precision: 0.5868881903906601\n",
      "epoch: 2\t| Loss: 1.9475419044494628\t| precision: 0.5734576120727918\n",
      "epoch: 2\t| Loss: 1.953964871764183\t| precision: 0.5433610975012249\n",
      "epoch: 2\t| Loss: 1.9518634432554245\t| precision: 0.552504816955684\n",
      "epoch: 2\t| Loss: 1.9457850748300551\t| precision: 0.5841975308641976\n",
      "epoch: 2\t| Loss: 1.9525104427337647\t| precision: 0.5553016453382084\n",
      "epoch: 2\t| Loss: 1.9396379178762435\t| precision: 0.537712895377129\n",
      "epoch: 2\t| Loss: 1.9639137864112854\t| precision: 0.5613959500215424\n",
      "epoch: 2\t| Loss: 1.9391840922832488\t| precision: 0.5769407441433165\n",
      "epoch: 2\t| Loss: 1.9521670871973038\t| precision: 0.5780027611596871\n",
      "epoch: 2\t| Loss: 1.9506503117084504\t| precision: 0.5401330376940133\n",
      "epoch: 2\t| Loss: 1.9393125754594802\t| precision: 0.5878061490359562\n",
      "epoch: 2\t| Loss: 1.9501433718204497\t| precision: 0.5727969348659003\n",
      "epoch: 2\t| Loss: 1.952090927362442\t| precision: 0.5771379458530297\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9407364875078201\t| precision: 0.6026457618814307\n",
      "epoch: 2\t| Loss: 1.9419410037994385\t| precision: 0.5778513961192617\n",
      "epoch: 2\t| Loss: 1.9559471476078034\t| precision: 0.584958217270195\n",
      "epoch: 2\t| Loss: 1.9524288707971573\t| precision: 0.5604681404421327\n",
      "epoch: 2\t| Loss: 1.9494065678119659\t| precision: 0.5370544090056285\n",
      "epoch: 2\t| Loss: 1.958833794593811\t| precision: 0.5460151802656547\n",
      "epoch: 2\t| Loss: 1.9473635101318358\t| precision: 0.5563736769443166\n",
      "epoch: 2\t| Loss: 1.9450168257951737\t| precision: 0.6000998502246631\n",
      "epoch: 2\t| Loss: 1.9525549024343491\t| precision: 0.5816416593115622\n",
      "epoch: 2\t| Loss: 1.9439264833927155\t| precision: 0.5713000449842555\n",
      "epoch: 2\t| Loss: 1.9401837110519409\t| precision: 0.5636444841447075\n",
      "epoch: 2\t| Loss: 1.949739279150963\t| precision: 0.5653526970954357\n",
      "epoch: 2\t| Loss: 1.9401400530338286\t| precision: 0.5654028436018957\n",
      "epoch: 2\t| Loss: 1.9468298655748368\t| precision: 0.5832476875642343\n",
      "epoch: 2\t| Loss: 1.9418185716867447\t| precision: 0.5578859060402684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.955673599243164\t| precision: 0.6006959547629404\n",
      "epoch: 2\t| Loss: 1.9565956050157547\t| precision: 0.5500954198473282\n",
      "epoch: 2\t| Loss: 1.9496503514051438\t| precision: 0.5780716723549488\n",
      "epoch: 2\t| Loss: 1.9515593993663787\t| precision: 0.5846303501945526\n",
      "epoch: 2\t| Loss: 1.948143093585968\t| precision: 0.5699233716475096\n",
      "epoch: 2\t| Loss: 1.9539622014760971\t| precision: 0.6068931068931069\n",
      "epoch: 2\t| Loss: 1.9598819202184676\t| precision: 0.5810313075506446\n",
      "epoch: 2\t| Loss: 1.9459964120388031\t| precision: 0.5639686684073107\n",
      "epoch: 2\t| Loss: 1.949462366104126\t| precision: 0.5534283170080142\n",
      "epoch: 2\t| Loss: 1.9407049775123597\t| precision: 0.5683797909407665\n",
      "epoch: 2\t| Loss: 1.944610225558281\t| precision: 0.5597652081109925\n",
      "epoch: 2\t| Loss: 1.9484237629175185\t| precision: 0.5801495016611296\n",
      "epoch: 2\t| Loss: 1.9520273458957673\t| precision: 0.5838769804287046\n",
      "epoch: 2\t| Loss: 1.9444179236888885\t| precision: 0.5719267654751525\n",
      "epoch: 2\t| Loss: 1.9544267070293426\t| precision: 0.5670849420849421\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.93899698138237\t| precision: 0.5791057609630267\n",
      "epoch: 2\t| Loss: 1.9397865861654282\t| precision: 0.5848670756646217\n",
      "epoch: 2\t| Loss: 1.9442768913507462\t| precision: 0.5509009009009009\n",
      "epoch: 2\t| Loss: 1.9394609946012498\t| precision: 0.591648590021692\n",
      "epoch: 2\t| Loss: 1.9423689478635788\t| precision: 0.5779049295774648\n",
      "epoch: 2\t| Loss: 1.9417985087633134\t| precision: 0.561049011177988\n",
      "epoch: 2\t| Loss: 1.9510411643981933\t| precision: 0.5744394618834081\n",
      "epoch: 2\t| Loss: 1.9506114739179612\t| precision: 0.54925516578568\n",
      "epoch: 2\t| Loss: 1.9493828970193863\t| precision: 0.5625841184387618\n",
      "epoch: 2\t| Loss: 1.9429710793495178\t| precision: 0.5641583297085689\n",
      "epoch: 2\t| Loss: 1.9399606907367706\t| precision: 0.555349698934692\n",
      "epoch: 2\t| Loss: 1.9487127512693405\t| precision: 0.580829756795422\n",
      "epoch: 2\t| Loss: 1.9464729064702988\t| precision: 0.5769805680119582\n",
      "epoch: 2\t| Loss: 1.9417337948083877\t| precision: 0.5897574123989219\n",
      "epoch: 2\t| Loss: 1.943725882768631\t| precision: 0.5648893360160966\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9376474124193193\t| precision: 0.5870736086175943\n",
      "epoch: 2\t| Loss: 1.9533672374486923\t| precision: 0.5838739573679332\n",
      "epoch: 2\t| Loss: 1.925331438779831\t| precision: 0.56\n",
      "epoch: 2\t| Loss: 1.9436766576766968\t| precision: 0.5587606837606838\n",
      "epoch: 2\t| Loss: 1.9435653805732727\t| precision: 0.5707865168539326\n",
      "epoch: 2\t| Loss: 1.943220477104187\t| precision: 0.5727602278612118\n",
      "epoch: 2\t| Loss: 1.9469688999652863\t| precision: 0.566066066066066\n",
      "epoch: 2\t| Loss: 1.945060192346573\t| precision: 0.5780189959294437\n",
      "epoch: 2\t| Loss: 1.937911269068718\t| precision: 0.5642722117202268\n",
      "epoch: 2\t| Loss: 1.9563416808843612\t| precision: 0.5411460577337355\n",
      "epoch: 2\t| Loss: 1.9414056789875032\t| precision: 0.598145285935085\n",
      "epoch: 2\t| Loss: 1.9524912309646607\t| precision: 0.5779547359597653\n",
      "epoch: 2\t| Loss: 1.9433259558677674\t| precision: 0.5767148014440433\n",
      "epoch: 2\t| Loss: 1.9297226417064666\t| precision: 0.5712223291626564\n",
      "epoch: 2\t| Loss: 1.9497978258132935\t| precision: 0.5706018518518519\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9499093079566956\t| precision: 0.5648967551622419\n",
      "epoch: 2\t| Loss: 1.9365461313724517\t| precision: 0.5712871287128712\n",
      "epoch: 2\t| Loss: 1.9442186105251311\t| precision: 0.5753052917232022\n",
      "epoch: 2\t| Loss: 1.9363452571630477\t| precision: 0.5892307692307692\n",
      "epoch: 2\t| Loss: 1.936213775873184\t| precision: 0.5767441860465117\n",
      "epoch: 2\t| Loss: 1.949030140042305\t| precision: 0.567591763652641\n",
      "epoch: 2\t| Loss: 1.944416694045067\t| precision: 0.5775492716366752\n",
      "epoch: 2\t| Loss: 1.9428382104635238\t| precision: 0.5739171374764596\n",
      "epoch: 2\t| Loss: 1.9319953602552413\t| precision: 0.572849328692193\n",
      "epoch: 2\t| Loss: 1.9508267086744309\t| precision: 0.5759803921568627\n",
      "epoch: 2\t| Loss: 1.937393079996109\t| precision: 0.5575304022450889\n",
      "epoch: 2\t| Loss: 1.951529951095581\t| precision: 0.5950975941897413\n",
      "epoch: 2\t| Loss: 1.9508353185653686\t| precision: 0.5485714285714286\n",
      "epoch: 2\t| Loss: 1.936835772395134\t| precision: 0.5768321513002365\n",
      "epoch: 2\t| Loss: 1.9478117251396179\t| precision: 0.5745920745920746\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9467582261562348\t| precision: 0.5768052516411378\n",
      "epoch: 2\t| Loss: 1.93905009329319\t| precision: 0.5794161306284018\n",
      "epoch: 2\t| Loss: 1.945124592781067\t| precision: 0.5619913834370512\n",
      "epoch: 2\t| Loss: 1.9420325237512588\t| precision: 0.6107566089334548\n",
      "epoch: 2\t| Loss: 1.946678466796875\t| precision: 0.5851528384279476\n",
      "epoch: 2\t| Loss: 1.963864980340004\t| precision: 0.5953488372093023\n",
      "epoch: 2\t| Loss: 1.9355881607532501\t| precision: 0.5750120598166908\n",
      "epoch: 2\t| Loss: 1.9349258464574814\t| precision: 0.5694760820045558\n",
      "epoch: 2\t| Loss: 1.944709350466728\t| precision: 0.5901890648952478\n",
      "epoch: 2\t| Loss: 1.9380195438861847\t| precision: 0.5696887686062246\n",
      "epoch: 2\t| Loss: 1.925572207570076\t| precision: 0.5898876404494382\n",
      "epoch: 2\t| Loss: 1.9448500061035157\t| precision: 0.5547134059687352\n",
      "epoch: 2\t| Loss: 1.9331501173973082\t| precision: 0.572002007024586\n",
      "epoch: 2\t| Loss: 1.943369346857071\t| precision: 0.5909926470588235\n",
      "epoch: 2\t| Loss: 1.937924361228943\t| precision: 0.5710814094775213\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9359626269340515\t| precision: 0.5733091239219247\n",
      "epoch: 2\t| Loss: 1.9336894738674164\t| precision: 0.5747619047619048\n",
      "epoch: 2\t| Loss: 1.9430013138055802\t| precision: 0.559295701708959\n",
      "epoch: 2\t| Loss: 1.9474320775270462\t| precision: 0.5465367965367965\n",
      "epoch: 2\t| Loss: 1.9404148083925248\t| precision: 0.5628187297171998\n",
      "epoch: 2\t| Loss: 1.9426702517271042\t| precision: 0.5747538048343778\n",
      "epoch: 2\t| Loss: 1.9346031814813613\t| precision: 0.5882951653944021\n",
      "epoch: 2\t| Loss: 1.9350241428613664\t| precision: 0.5345768880800728\n",
      "epoch: 2\t| Loss: 1.9321618735790254\t| precision: 0.5567961165048544\n",
      "epoch: 2\t| Loss: 1.934916267991066\t| precision: 0.5649380449747591\n",
      "epoch: 2\t| Loss: 1.9351729714870454\t| precision: 0.5925744992672203\n",
      "epoch: 2\t| Loss: 1.930507269501686\t| precision: 0.5956873315363881\n",
      "epoch: 2\t| Loss: 1.9366169542074203\t| precision: 0.5506683915480811\n",
      "epoch: 2\t| Loss: 1.9355584943294526\t| precision: 0.5987229862475442\n",
      "epoch: 2\t| Loss: 1.9395833837985992\t| precision: 0.5453707968678029\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9359104174375534\t| precision: 0.5595238095238095\n",
      "epoch: 2\t| Loss: 1.9340539866685866\t| precision: 0.5758789497107254\n",
      "epoch: 2\t| Loss: 1.93234163582325\t| precision: 0.5540665434380776\n",
      "epoch: 2\t| Loss: 1.9374271446466447\t| precision: 0.5433104631217839\n",
      "epoch: 2\t| Loss: 1.9343840318918228\t| precision: 0.5935903390617743\n",
      "epoch: 2\t| Loss: 1.9337329304218291\t| precision: 0.5899912203687445\n",
      "epoch: 2\t| Loss: 1.9379750221967698\t| precision: 0.5817990531299316\n",
      "epoch: 2\t| Loss: 1.9402286416292192\t| precision: 0.578125\n",
      "epoch: 2\t| Loss: 1.9347074794769288\t| precision: 0.587369420702754\n",
      "epoch: 2\t| Loss: 1.9362795895338059\t| precision: 0.5571428571428572\n",
      "epoch: 2\t| Loss: 1.9222931337356568\t| precision: 0.5895729339988908\n",
      "epoch: 2\t| Loss: 1.9368126142024993\t| precision: 0.5817767653758542\n",
      "epoch: 2\t| Loss: 1.9347530275583267\t| precision: 0.5924453280318092\n",
      "epoch: 2\t| Loss: 1.9431396675109864\t| precision: 0.5487465181058496\n",
      "epoch: 2\t| Loss: 1.9361518102884292\t| precision: 0.59375\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9351905518770218\t| precision: 0.5820153637596024\n",
      "epoch: 2\t| Loss: 1.9299490576982499\t| precision: 0.5647058823529412\n",
      "epoch: 2\t| Loss: 1.936062849164009\t| precision: 0.6017811704834606\n",
      "epoch: 2\t| Loss: 1.9335904383659364\t| precision: 0.5685936151855048\n",
      "epoch: 2\t| Loss: 1.93789874792099\t| precision: 0.5765993265993266\n",
      "epoch: 2\t| Loss: 1.9357933950424195\t| precision: 0.550890004564126\n",
      "epoch: 2\t| Loss: 1.944610225558281\t| precision: 0.5486399262332873\n",
      "epoch: 2\t| Loss: 1.9404775989055634\t| precision: 0.5606502709462275\n",
      "epoch: 2\t| Loss: 1.9342073547840117\t| precision: 0.6\n",
      "epoch: 2\t| Loss: 1.93522247672081\t| precision: 0.5299607072691552\n",
      "epoch: 2\t| Loss: 1.930576924085617\t| precision: 0.5713097713097713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 1.9272576349973678\t| precision: 0.5744784085395439\n",
      "epoch: 2\t| Loss: 1.9345759373903275\t| precision: 0.6039387308533917\n",
      "epoch: 2\t| Loss: 1.9201806551218032\t| precision: 0.579454926624738\n",
      "epoch: 2\t| Loss: 1.9531423234939576\t| precision: 0.5786463298379408\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9316466408967972\t| precision: 0.611969111969112\n",
      "epoch: 2\t| Loss: 1.9394033640623092\t| precision: 0.5383872525350072\n",
      "epoch: 2\t| Loss: 1.935233570933342\t| precision: 0.5599812558575445\n",
      "epoch: 2\t| Loss: 1.9279505199193954\t| precision: 0.5696353855349672\n",
      "epoch: 2\t| Loss: 1.9402993083000184\t| precision: 0.5442043222003929\n",
      "epoch: 2\t| Loss: 1.9317735987901687\t| precision: 0.5917381974248928\n",
      "epoch: 2\t| Loss: 1.9327347266674042\t| precision: 0.5788546255506608\n",
      "epoch: 2\t| Loss: 1.924860216975212\t| precision: 0.6000846381718155\n",
      "epoch: 2\t| Loss: 1.9339675408601762\t| precision: 0.5681505277650298\n",
      "epoch: 2\t| Loss: 1.937672073841095\t| precision: 0.5568769389865563\n",
      "epoch: 2\t| Loss: 1.925046479701996\t| precision: 0.5818275684047496\n",
      "epoch: 2\t| Loss: 1.9126769816875457\t| precision: 0.5624309392265193\n",
      "epoch: 2\t| Loss: 1.9415462499856948\t| precision: 0.5455807496529385\n",
      "epoch: 2\t| Loss: 1.9281421476602554\t| precision: 0.5443410093091622\n",
      "epoch: 2\t| Loss: 1.9338577145338058\t| precision: 0.5959488272921108\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9457431799173355\t| precision: 0.5776765375854214\n",
      "epoch: 2\t| Loss: 1.9431795638799667\t| precision: 0.5628558913710031\n",
      "epoch: 2\t| Loss: 1.931127716898918\t| precision: 0.5765524625267666\n",
      "epoch: 2\t| Loss: 1.933897603750229\t| precision: 0.5614997713763146\n",
      "epoch: 2\t| Loss: 1.920587157011032\t| precision: 0.5758587786259542\n",
      "epoch: 2\t| Loss: 1.9410992270708085\t| precision: 0.5641025641025641\n",
      "epoch: 2\t| Loss: 1.9397782051563264\t| precision: 0.580943121050073\n",
      "epoch: 2\t| Loss: 1.9250722140073777\t| precision: 0.5827966881324746\n",
      "epoch: 2\t| Loss: 1.9345096606016159\t| precision: 0.5734939759036145\n",
      "epoch: 2\t| Loss: 1.9360370701551437\t| precision: 0.5807590467784642\n",
      "epoch: 2\t| Loss: 1.9322878760099411\t| precision: 0.5676674364896074\n",
      "epoch: 2\t| Loss: 1.9316983258724212\t| precision: 0.5615\n",
      "epoch: 2\t| Loss: 1.9198373025655746\t| precision: 0.584835876098012\n",
      "epoch: 2\t| Loss: 1.9369638741016388\t| precision: 0.5690269994905757\n",
      "epoch: 2\t| Loss: 1.9345375782251357\t| precision: 0.5665322580645161\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9380958789587022\t| precision: 0.6011470281543274\n",
      "epoch: 2\t| Loss: 1.938813677430153\t| precision: 0.5521500457456542\n",
      "epoch: 2\t| Loss: 1.9218221974372864\t| precision: 0.5801457194899818\n",
      "epoch: 2\t| Loss: 1.925900753736496\t| precision: 0.5959688502061383\n",
      "epoch: 2\t| Loss: 1.946600417494774\t| precision: 0.580960130187144\n",
      "epoch: 2\t| Loss: 1.9315606379508972\t| precision: 0.5454081632653062\n",
      "epoch: 2\t| Loss: 1.929609524011612\t| precision: 0.597544338335607\n",
      "epoch: 2\t| Loss: 1.9376199012994766\t| precision: 0.5809302325581396\n",
      "epoch: 2\t| Loss: 1.9362933707237244\t| precision: 0.5490286771507863\n",
      "epoch: 2\t| Loss: 1.9292798960208892\t| precision: 0.5701415701415702\n",
      "epoch: 2\t| Loss: 1.9331319558620452\t| precision: 0.5848142164781907\n",
      "epoch: 2\t| Loss: 1.928487045764923\t| precision: 0.5683987274655355\n",
      "epoch: 2\t| Loss: 1.9266302961111068\t| precision: 0.5489769264257728\n",
      "epoch: 2\t| Loss: 1.9378264105319978\t| precision: 0.5582608695652174\n",
      "epoch: 2\t| Loss: 1.9355728399753571\t| precision: 0.5601300108342362\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.92312624335289\t| precision: 0.5906515580736544\n",
      "epoch: 2\t| Loss: 1.9322612011432647\t| precision: 0.5971643035863219\n",
      "epoch: 2\t| Loss: 1.9319465762376786\t| precision: 0.5740308267164876\n",
      "epoch: 2\t| Loss: 1.9247751837968827\t| precision: 0.5670759436107321\n",
      "epoch: 2\t| Loss: 1.9260606640577316\t| precision: 0.5761148129164531\n",
      "epoch: 2\t| Loss: 1.9208829057216645\t| precision: 0.5968763191219923\n",
      "epoch: 2\t| Loss: 1.9322420769929887\t| precision: 0.5294380017841214\n",
      "epoch: 2\t| Loss: 1.9328870397806168\t| precision: 0.5604878048780488\n",
      "epoch: 2\t| Loss: 1.934889354109764\t| precision: 0.5832933269323092\n",
      "epoch: 2\t| Loss: 1.9375809168815612\t| precision: 0.5780984719864176\n",
      "epoch: 2\t| Loss: 1.922937980890274\t| precision: 0.5328345802161264\n",
      "epoch: 2\t| Loss: 1.9387910151481629\t| precision: 0.5948690539818279\n",
      "epoch: 2\t| Loss: 1.936833558678627\t| precision: 0.5406542056074767\n",
      "epoch: 2\t| Loss: 1.9386463129520417\t| precision: 0.5716535433070866\n",
      "epoch: 2\t| Loss: 1.9240160363912582\t| precision: 0.5592216582064298\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9173672157526016\t| precision: 0.5985111662531017\n",
      "epoch: 2\t| Loss: 1.9308442431688309\t| precision: 0.5571212883744339\n",
      "epoch: 2\t| Loss: 1.9274396783113479\t| precision: 0.599250936329588\n",
      "epoch: 2\t| Loss: 1.9321043008565904\t| precision: 0.6155055002619172\n",
      "epoch: 2\t| Loss: 1.922261580824852\t| precision: 0.6034958601655934\n",
      "epoch: 2\t| Loss: 1.9262525707483291\t| precision: 0.555397051830718\n",
      "epoch: 2\t| Loss: 1.919970865249634\t| precision: 0.6000899685110211\n",
      "epoch: 2\t| Loss: 1.9239658373594284\t| precision: 0.5518394648829431\n",
      "epoch: 2\t| Loss: 1.9190765064954758\t| precision: 0.5630252100840336\n",
      "epoch: 2\t| Loss: 1.9242378330230714\t| precision: 0.5792972459639126\n",
      "epoch: 2\t| Loss: 1.9319255757331848\t| precision: 0.5888143176733781\n",
      "epoch: 2\t| Loss: 1.9231604772806168\t| precision: 0.5874616396317405\n",
      "epoch: 2\t| Loss: 1.9218538784980774\t| precision: 0.5694444444444444\n",
      "epoch: 2\t| Loss: 1.9240135425329208\t| precision: 0.5846792801107522\n",
      "epoch: 2\t| Loss: 1.9226204657554626\t| precision: 0.5545156761815629\n",
      "mid epoch ..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 1.9339417445659637\t| precision: 0.5552742616033756\n",
      "epoch: 2\t| Loss: 1.9195305734872818\t| precision: 0.5581915846016114\n",
      "epoch: 2\t| Loss: 1.9301059865951538\t| precision: 0.5856481481481481\n",
      "epoch: 2\t| Loss: 1.9231468057632446\t| precision: 0.562198649951784\n",
      "epoch: 2\t| Loss: 1.9282919734716415\t| precision: 0.5911780527165142\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d5ecc2e5605a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'to__save: output_model_file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"** ** * Saving fine - tuned model ** ** * \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d5ecc2e5605a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, e, validload, optim)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_accumulation_steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_accumulation_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtemp_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/well/rahimi/users/gra027/conda/envs/MLGPvision/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/well/rahimi/users/gra027/conda/envs/MLGPvision/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(model, e, validload, optim):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(validload):\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "        age_ids, year_ids,input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "#         print(batch)\n",
    "        loss, pred, label = model(input_ids, age_ids, segment_ids, posi_ids,year_ids,  attention_mask=attMask,\n",
    "                                  masked_lm_labels=masked_label)\n",
    "\n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if step % 200 == 0:\n",
    "            print(\"epoch: {}\\t| Loss: {}\\t| precision: {}\".format(e, temp_loss / 200, cal_acc(label, pred)))\n",
    "            temp_loss = 0\n",
    "\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        if (step+1 )%3000 ==0:\n",
    "            print(\"mid epoch: \" +str(step *148) + \"..... ** ** * Saving fine - tuned model ** ** * \")\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "print('starting epoch0')\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_newcut1985_2020_DM.bin\")\n",
    "\n",
    "print('to__save: output_model_file')\n",
    "for e in range(50):\n",
    "    train(model, e, trainload, optim)\n",
    "\n",
    "    print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "    #         create_folder(global_params['output_dir'])\n",
    "    print('done epoch', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python MLGPvision",
   "language": "python",
   "name": "mlgpvision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
