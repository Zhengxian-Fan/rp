{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting run....\n",
      "starting run....\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "print('starting run....')\n",
    "\n",
    "import sys\n",
    "\n",
    "print('starting run....')\n",
    "\n",
    "sys.path.insert(0,'/gpfs3/well/rahimi/users/gra027/JNb/')\n",
    "\n",
    "from general_model_newCutCPRD.ModelPkg.BEHRTraw import *\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from general_model_newCutCPRD.ModelPkg import utils\n",
    "from general_model_newCutCPRD.ModelPkg.MLMRaw import *\n",
    "\n",
    "from general_model_newCutCPRD.ModelPkg.DataProc import *\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from general_model_newCutCPRD.pytorch_pretrained_bert  import optimizer\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3662\n",
      "read data....\n",
      "dict_keys(['bert.embeddings.word_embeddings.weight', 'bert.embeddings.segment_embeddings.weight', 'bert.embeddings.age_embeddings.weight', 'bert.embeddings.year_embeddings.weight', 'bert.embeddings.posi_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.embeddings.catmap.weight', 'bert.embeddings.catmap.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight'])\n"
     ]
    }
   ],
   "source": [
    "file_config = {\n",
    "        'vocab': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/GeneralVocDM_25k',\n",
    "    'oldvocab': '/gpfs3/well/rahimi/users/gra027/JNb/ExpHypCancer/Data/AllSubclass/DMBp',\n",
    "    'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_for_pretraining_28M_1985_2020__unique_per6m_70pc_sample.parquet/',\n",
    "\n",
    "#     'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_for_pretraining_28M_1985_2020__unique_per6m_50pc_sample___10kdebug.parquet/',\n",
    "    #\n",
    "    'yearVocab':  '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/yearVoc_1985_2021',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 148,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:1',\n",
    "    'output_dir':'/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/SavedModels/',\n",
    "    'output_name': 'MLM_CEHR_newcut1985_2020_DM.bin',\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 250,\n",
    "    'max_age': 110,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    'yearOn':True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "YearVocab = utils.load_obj(file_config['yearVocab'])\n",
    "create_folder(global_params['output_dir'])\n",
    "BertVocab = utils.load_obj(file_config['vocab'])\n",
    "print(len(BertVocab['token2idx']))\n",
    "\n",
    "ageVocab, _ = utils.age_vocab(max_age=global_params['max_age'], year=global_params['age_year'], symbol=global_params['age_symbol'])\n",
    "fulldata = pd.read_parquet(file_config['fulld'])\n",
    "print('read data....')\n",
    "\n",
    "trainSet = MLMLoader(token2idx=BertVocab['token2idx'], dataframe=fulldata, max_len=global_params['max_len_seq'], max_age=global_params['max_age'], year=global_params['age_year'], age_symbol=global_params['age_symbol'],year2idx = YearVocab['token2idx'] )\n",
    "trainload = DataLoader(dataset=trainSet, batch_size=global_params['batch_size'], shuffle=True)\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 150, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()), # number of vocab for age embedding\n",
    "\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.15, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 6, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.15, # multi-head attention dropout rate\n",
    "    'intermediate_size': 108, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range,\n",
    "    'yearOn':True,\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()),\n",
    "    'concat_embeddings':True,\n",
    "\n",
    "}\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForMaskedLM(conf)\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_CEHR_newcut1985_2020_DM__6msummary.bin\")\n",
    "model = toLoad(model, output_model_file)\n",
    "model = model.to(global_params['device'])\n",
    "optim = optimizer.adam(params=list(model.named_parameters()), config=optim_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# YearVocab = {'token2idx': {'PAD': 0,\n",
    "#   '1987': 1,\n",
    "#   '1988': 2,\n",
    "#   '1989': 3,\n",
    "#   '1990': 4,\n",
    "#   '1991': 5,\n",
    "#   '1992': 6,\n",
    "#   '1993': 7,\n",
    "#   '1994': 8,\n",
    "#   '1995': 9,\n",
    "#   '1996': 10,\n",
    "#   '1997': 11,\n",
    "#   '1998': 12,\n",
    "#   '1999': 13,\n",
    "#   '2000': 14,\n",
    "#   '2001': 15,\n",
    "#   '2002': 16,\n",
    "#   '2003': 17,\n",
    "#   '2004': 18,\n",
    "#   '2005': 19,\n",
    "#   '2006': 20,\n",
    "#   '2007': 21,\n",
    "#   '2008': 22,\n",
    "#   '2009': 23,\n",
    "#   '2010': 24,\n",
    "#   '2011': 25,\n",
    "#   '2012': 26,\n",
    "#   '2013': 27,\n",
    "#   '2014': 28,\n",
    "#   '2015': 29,\n",
    "#  '2016': 30,\n",
    "#   '2017': 31,\n",
    "#   '2018': 32,\n",
    "#   '2019': 33,\n",
    "#   '2020': 34,\n",
    "#  '2021': 35,\n",
    "\n",
    "#  'UNK': 36\n",
    "\n",
    "# },\n",
    "#  'idx2token': {0: 'PAD',\n",
    "#   1: '1987',\n",
    "#   2: '1988',\n",
    "#   3: '1989',\n",
    "#   4: '1990',\n",
    "#   5: '1991',\n",
    "#   6: '1992',\n",
    "#   7: '1993',\n",
    "#   8: '1994',\n",
    "#   9: '1995',\n",
    "#   10: '1996',\n",
    "#   11: '1997',\n",
    "#   12: '1998',\n",
    "#   13: '1999',\n",
    "#   14: '2000',\n",
    "#   15: '2001',\n",
    "#   16: '2002',\n",
    "#   17: '2003',\n",
    "#   18: '2004',\n",
    "#   19: '2005',\n",
    "#   20: '2006',\n",
    "#   21: '2007',\n",
    "#   22: '2008',\n",
    "#   23: '2009',\n",
    "#   24: '2010',\n",
    "#   25: '2011',\n",
    "#   26: '2012',\n",
    "#   27: '2013',\n",
    "#   28: '2014',\n",
    "#   29: '2015',\n",
    "#                30: '2016',\n",
    "#   31: '2017',\n",
    "#   32: '2018',\n",
    "#   33: '2019',\n",
    "#   34: '2020',\n",
    "#   35: '2021',\n",
    "\n",
    "#   36: 'UNK'}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = utils.load_obj(file_config['vocab'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# olddic = os.path.join('/gpfs3/well/rahimi/users/gra027/JNb/ExpHypCancer/ModelBins/',\n",
    "#                                              \"BEHRT_mlm_DMBp.bin\")\n",
    "# dd = torch.load(olddic,  map_location='cpu')\n",
    "# modeld = model.state_dict()\n",
    "# modeld['bert.embeddings.age_embeddings.weight'] = dd['bert.embeddings.age_embeddings.weight']\n",
    "# modeld['bert.embeddings.year_embeddings.weight'][:31] = dd['bert.embeddings.year_embeddings.weight']\n",
    "# modeld['bert.embeddings.posi_embeddings.weight'] = dd['bert.embeddings.posi_embeddings.weight']\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for x in BertVocab['idx2token']:\n",
    "#     if 'bnf' not in BertVocab['idx2token'][x] and 'vtm' not in BertVocab['idx2token'][x] and 'BANDAGE' not in BertVocab['idx2token'][x]:\n",
    "#         if BertVocab['idx2token'][x] in BertVocabold['token2idx']:\n",
    "#             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x]]\n",
    "\n",
    "#             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#             count = count+1\n",
    "#         elif     BertVocab['idx2token'][x][:-1] in BertVocabold['token2idx']:\n",
    "#             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x][:-1]]\n",
    "#             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#             count = count+1\n",
    "\n",
    "#         else:\n",
    "#             for y in BertVocabold['token2idx']:\n",
    "#                 if BertVocab['idx2token'][x][:-1] in y :\n",
    "#                     oldindx = BertVocabold['token2idx'][y]\n",
    "#     #                 print(BertVocab['idx2token'][x], y )\n",
    "#                     modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#                     count = count+1\n",
    "#                     break\n",
    "#     else:\n",
    "#         if 'bnf' in BertVocab['idx2token'][x]:\n",
    "            \n",
    "#             text2check = BertVocab['idx2token'][x][4:8]\n",
    "#             if text2check in BertVocabold['token2idx']:\n",
    "#                 oldindx = BertVocabold['token2idx'][text2check]\n",
    "#                 modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#                 count = count+1\n",
    "\n",
    "#             else:\n",
    "#                 for y in BertVocabold['token2idx']:\n",
    "#                     if text2check in y :\n",
    "# #                         print(y, text2check)\n",
    "#                         count = count+1\n",
    "#                         oldindx = BertVocabold['token2idx'][y]\n",
    "#                         modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "\n",
    "#                         break\n",
    "                \n",
    "                \n",
    "# for x in modeld:\n",
    "#     if x not in ['cls.predictions.bias','cls.predictions.decoder.weight','bert.embeddings.segment_embeddings.weight' , 'bert.embeddings.word_embeddings.weight','bert.embeddings.age_embeddings.weight', 'bert.embeddings.year_embeddings.weight', 'bert.embeddings.posi_embeddings.weight']:\n",
    "#         if x in dd:\n",
    "#             print(x)\n",
    "#             modeld[x] = dd[x]\n",
    "# model.load_state_dict(modeld)\n",
    "# print(count, len(BertVocab['idx2token']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch0\n",
      "to__save:  /gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/SavedModels/MLM_CEHR_newcut1985_2020_DM__6msummary.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/rahimi/users/gra027/JNb/Graph/ModelPkg/utils.py:606: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  truepred = logs(torch.tensor(truepred))\n",
      "/gpfs3/well/rahimi/users/gra027/JNb/Graph/ModelPkg/pytorch_pretrained_bert/optimization.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378065146/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 0.010283447504043579\t| precision: 0.5664845173041895\n",
      "epoch: 0\t| Loss: 2.179081581234932\t| precision: 0.554858934169279\n",
      "epoch: 0\t| Loss: 2.1613850086927413\t| precision: 0.5531073446327683\n",
      "epoch: 0\t| Loss: 2.1575544053316116\t| precision: 0.5715116279069767\n",
      "epoch: 0\t| Loss: 2.158020183444023\t| precision: 0.5599352051835853\n",
      "epoch: 0\t| Loss: 2.1569062238931656\t| precision: 0.5517808219178082\n",
      "epoch: 0\t| Loss: 2.1545570570230486\t| precision: 0.5959367945823928\n",
      "epoch: 0\t| Loss: 2.1640449553728103\t| precision: 0.5925526173772261\n",
      "epoch: 0\t| Loss: 2.15715895652771\t| precision: 0.5657276995305164\n",
      "epoch: 0\t| Loss: 2.1563590109348296\t| precision: 0.5679691799669785\n",
      "epoch: 0\t| Loss: 2.1551244515180588\t| precision: 0.5514303104077907\n",
      "epoch: 0\t| Loss: 2.1622695398330687\t| precision: 0.5536332179930796\n",
      "epoch: 0\t| Loss: 2.154433307647705\t| precision: 0.5560190703218116\n",
      "epoch: 0\t| Loss: 2.163051890730858\t| precision: 0.581201044386423\n",
      "epoch: 0\t| Loss: 2.1563811814785003\t| precision: 0.5373711340206185\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1554773724079133\t| precision: 0.558165548098434\n",
      "epoch: 0\t| Loss: 2.1474198651313783\t| precision: 0.5524079320113314\n",
      "epoch: 0\t| Loss: 2.148509343266487\t| precision: 0.5640569395017794\n",
      "epoch: 0\t| Loss: 2.1505274897813798\t| precision: 0.5392528424472117\n",
      "epoch: 0\t| Loss: 2.143909201622009\t| precision: 0.5387348969438521\n",
      "epoch: 0\t| Loss: 2.1510628885030747\t| precision: 0.5525634386328327\n",
      "epoch: 0\t| Loss: 2.1626087659597397\t| precision: 0.5669988925802879\n",
      "epoch: 0\t| Loss: 2.15461922287941\t| precision: 0.5801913337084975\n",
      "epoch: 0\t| Loss: 2.154044665694237\t| precision: 0.5157194679564692\n",
      "epoch: 0\t| Loss: 2.155979400277138\t| precision: 0.5553224960671211\n",
      "epoch: 0\t| Loss: 2.1419111680984497\t| precision: 0.5501460564751705\n",
      "epoch: 0\t| Loss: 2.1540250939130785\t| precision: 0.5563524590163934\n",
      "epoch: 0\t| Loss: 2.152009208202362\t| precision: 0.575013638843426\n",
      "epoch: 0\t| Loss: 2.1509744626283647\t| precision: 0.5724852071005917\n",
      "epoch: 0\t| Loss: 2.1504850399494173\t| precision: 0.5132394366197183\n",
      "mid epoch: 887852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1595900559425356\t| precision: 0.5767567567567567\n",
      "epoch: 0\t| Loss: 2.144466302394867\t| precision: 0.5470085470085471\n",
      "epoch: 0\t| Loss: 2.1528853994607924\t| precision: 0.5425884955752213\n",
      "epoch: 0\t| Loss: 2.149280371069908\t| precision: 0.5740740740740741\n",
      "epoch: 0\t| Loss: 2.148291452527046\t| precision: 0.5532915360501567\n",
      "epoch: 0\t| Loss: 2.1490292751789095\t| precision: 0.5626191989828353\n",
      "epoch: 0\t| Loss: 2.1617623031139375\t| precision: 0.5577219086711134\n",
      "epoch: 0\t| Loss: 2.153371523618698\t| precision: 0.5738813735691988\n",
      "epoch: 0\t| Loss: 2.149289043545723\t| precision: 0.5606060606060606\n",
      "epoch: 0\t| Loss: 2.1554878878593446\t| precision: 0.5406626506024096\n",
      "epoch: 0\t| Loss: 2.1595909821987154\t| precision: 0.5663265306122449\n",
      "epoch: 0\t| Loss: 2.1527373176813125\t| precision: 0.5499405469678954\n",
      "epoch: 0\t| Loss: 2.144501517415047\t| precision: 0.5670157068062828\n",
      "epoch: 0\t| Loss: 2.153639094233513\t| precision: 0.5619047619047619\n",
      "epoch: 0\t| Loss: 2.1502031856775283\t| precision: 0.567287784679089\n",
      "mid epoch: 1331852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.16749862909317\t| precision: 0.5588078765300691\n",
      "epoch: 0\t| Loss: 2.1548056107759477\t| precision: 0.5718733783082511\n",
      "epoch: 0\t| Loss: 2.158481856584549\t| precision: 0.5724313975749841\n",
      "epoch: 0\t| Loss: 2.150038157701492\t| precision: 0.5640183847669075\n",
      "epoch: 0\t| Loss: 2.153145983219147\t| precision: 0.5456989247311828\n",
      "epoch: 0\t| Loss: 2.16060569524765\t| precision: 0.5295081967213114\n",
      "epoch: 0\t| Loss: 2.148203440308571\t| precision: 0.5360094451003542\n",
      "epoch: 0\t| Loss: 2.149003051519394\t| precision: 0.5636177823198774\n",
      "epoch: 0\t| Loss: 2.1508163416385653\t| precision: 0.5690826727066818\n",
      "epoch: 0\t| Loss: 2.1509016305208206\t| precision: 0.5647536820721178\n",
      "epoch: 0\t| Loss: 2.1487756991386413\t| precision: 0.5647442872687704\n",
      "epoch: 0\t| Loss: 2.1563984680175783\t| precision: 0.5616844602609727\n",
      "epoch: 0\t| Loss: 2.153431438803673\t| precision: 0.5498971193415638\n",
      "epoch: 0\t| Loss: 2.1455017524957656\t| precision: 0.5586166471277842\n",
      "epoch: 0\t| Loss: 2.154035500884056\t| precision: 0.5569693464430306\n",
      "mid epoch: 1775852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1639231276512145\t| precision: 0.5562950637825845\n",
      "epoch: 0\t| Loss: 2.158413350582123\t| precision: 0.5470961444607125\n",
      "epoch: 0\t| Loss: 2.1387627071142195\t| precision: 0.5332207207207207\n",
      "epoch: 0\t| Loss: 2.153730159401894\t| precision: 0.547945205479452\n",
      "epoch: 0\t| Loss: 2.1612884253263474\t| precision: 0.5671045117075957\n",
      "epoch: 0\t| Loss: 2.1429448717832567\t| precision: 0.5581127733026467\n",
      "epoch: 0\t| Loss: 2.1513626945018767\t| precision: 0.566189111747851\n",
      "epoch: 0\t| Loss: 2.152358574271202\t| precision: 0.5498377375985165\n",
      "epoch: 0\t| Loss: 2.1527758330106734\t| precision: 0.5538873994638069\n",
      "epoch: 0\t| Loss: 2.156072967648506\t| precision: 0.5597667638483965\n",
      "epoch: 0\t| Loss: 2.1441924244165422\t| precision: 0.5503426462836057\n",
      "epoch: 0\t| Loss: 2.1455986595153806\t| precision: 0.5536818437324339\n",
      "epoch: 0\t| Loss: 2.146759783625603\t| precision: 0.5457646389994315\n",
      "epoch: 0\t| Loss: 2.159008183479309\t| precision: 0.5363321799307958\n",
      "epoch: 0\t| Loss: 2.1487081736326217\t| precision: 0.5586034912718204\n",
      "mid epoch: 2219852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1625115263462065\t| precision: 0.538420204191295\n",
      "epoch: 0\t| Loss: 2.160244680047035\t| precision: 0.5742444152431012\n",
      "epoch: 0\t| Loss: 2.1520339429378508\t| precision: 0.5620649651972158\n",
      "epoch: 0\t| Loss: 2.157613260149956\t| precision: 0.5716603569497025\n",
      "epoch: 0\t| Loss: 2.1499499750137328\t| precision: 0.5763052208835341\n",
      "epoch: 0\t| Loss: 2.145337458252907\t| precision: 0.5410169491525424\n",
      "epoch: 0\t| Loss: 2.153197823166847\t| precision: 0.545995670995671\n",
      "epoch: 0\t| Loss: 2.1551102775335313\t| precision: 0.5657276995305164\n",
      "epoch: 0\t| Loss: 2.1497072023153305\t| precision: 0.536683997689197\n",
      "epoch: 0\t| Loss: 2.156313654780388\t| precision: 0.5685852608929532\n",
      "epoch: 0\t| Loss: 2.16324489235878\t| precision: 0.5807398708162067\n",
      "epoch: 0\t| Loss: 2.157769718170166\t| precision: 0.5666293393057111\n",
      "epoch: 0\t| Loss: 2.1552208888530733\t| precision: 0.5570578691184424\n",
      "epoch: 0\t| Loss: 2.159539961218834\t| precision: 0.5810344827586207\n",
      "epoch: 0\t| Loss: 2.1515495371818543\t| precision: 0.5460992907801419\n",
      "mid epoch: 2663852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1481758362054824\t| precision: 0.5556179775280898\n",
      "epoch: 0\t| Loss: 2.1563907355070113\t| precision: 0.564741641337386\n",
      "epoch: 0\t| Loss: 2.1502483171224593\t| precision: 0.5554335894621295\n",
      "epoch: 0\t| Loss: 2.1519477194547654\t| precision: 0.5494778717056191\n",
      "epoch: 0\t| Loss: 2.156382927894592\t| precision: 0.5590729225551159\n",
      "epoch: 0\t| Loss: 2.157841071486473\t| precision: 0.5511494252873563\n",
      "epoch: 0\t| Loss: 2.1491365987062454\t| precision: 0.5529646902065289\n",
      "epoch: 0\t| Loss: 2.1468575161695482\t| precision: 0.5441085749537322\n",
      "epoch: 0\t| Loss: 2.1480033558607103\t| precision: 0.5633440514469453\n",
      "epoch: 0\t| Loss: 2.1559178465604782\t| precision: 0.5386584741423451\n",
      "epoch: 0\t| Loss: 2.1495863115787506\t| precision: 0.5529595015576324\n",
      "epoch: 0\t| Loss: 2.1532251477241515\t| precision: 0.5541976620616366\n",
      "epoch: 0\t| Loss: 2.1484163480997087\t| precision: 0.5452422650321074\n",
      "epoch: 0\t| Loss: 2.1499954050779344\t| precision: 0.567552289429056\n",
      "epoch: 0\t| Loss: 2.14841215133667\t| precision: 0.5472610096670247\n",
      "mid epoch: 3107852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.150188239812851\t| precision: 0.5425884955752213\n",
      "epoch: 0\t| Loss: 2.153033162355423\t| precision: 0.5423216444981862\n",
      "epoch: 0\t| Loss: 2.147235325574875\t| precision: 0.5714285714285714\n",
      "epoch: 0\t| Loss: 2.161150588989258\t| precision: 0.5638474295190713\n",
      "epoch: 0\t| Loss: 2.156983953714371\t| precision: 0.5708245243128964\n",
      "epoch: 0\t| Loss: 2.1535030323266984\t| precision: 0.5592885375494071\n",
      "epoch: 0\t| Loss: 2.1502978879213335\t| precision: 0.5749559082892416\n",
      "epoch: 0\t| Loss: 2.149313017129898\t| precision: 0.5633351149118119\n",
      "epoch: 0\t| Loss: 2.1419913387298584\t| precision: 0.5628415300546448\n",
      "epoch: 0\t| Loss: 2.1510255581140516\t| precision: 0.5719360568383659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.156807242631912\t| precision: 0.5688295936931473\n",
      "epoch: 0\t| Loss: 2.153807436823845\t| precision: 0.5513482501434309\n",
      "epoch: 0\t| Loss: 2.1519216054677965\t| precision: 0.5717476270240089\n",
      "epoch: 0\t| Loss: 2.1494312292337416\t| precision: 0.5725101921956901\n",
      "epoch: 0\t| Loss: 2.1606084686517715\t| precision: 0.5653315824031516\n",
      "mid epoch: 3551852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.151894587278366\t| precision: 0.5431530494821634\n",
      "epoch: 0\t| Loss: 2.1465711134672163\t| precision: 0.5569767441860465\n",
      "epoch: 0\t| Loss: 2.151559729576111\t| precision: 0.5669431279620853\n",
      "epoch: 0\t| Loss: 2.155016375184059\t| precision: 0.5544499723604202\n",
      "epoch: 0\t| Loss: 2.14570140004158\t| precision: 0.56299938537185\n",
      "epoch: 0\t| Loss: 2.1588864988088607\t| precision: 0.5729755178907722\n",
      "epoch: 0\t| Loss: 2.1582480973005294\t| precision: 0.5772038180797305\n",
      "epoch: 0\t| Loss: 2.150919804573059\t| precision: 0.5926605504587156\n",
      "epoch: 0\t| Loss: 2.1516034132242203\t| precision: 0.5560298826040555\n",
      "epoch: 0\t| Loss: 2.1535950791835785\t| precision: 0.5460448642266824\n",
      "epoch: 0\t| Loss: 2.1497514909505844\t| precision: 0.5772984078068824\n",
      "epoch: 0\t| Loss: 2.1449891269207\t| precision: 0.5623459832429768\n",
      "epoch: 0\t| Loss: 2.153727658390999\t| precision: 0.554294175715696\n",
      "epoch: 0\t| Loss: 2.150382789969444\t| precision: 0.5470031545741325\n",
      "epoch: 0\t| Loss: 2.1460813784599306\t| precision: 0.5473747736873869\n",
      "mid epoch: 3995852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1538654237985613\t| precision: 0.5868680999418943\n",
      "epoch: 0\t| Loss: 2.157144059538841\t| precision: 0.571263704558569\n",
      "epoch: 0\t| Loss: 2.1504372107982634\t| precision: 0.5741254858411994\n",
      "epoch: 0\t| Loss: 2.1489338690042494\t| precision: 0.5612835349815887\n",
      "epoch: 0\t| Loss: 2.14643906891346\t| precision: 0.5546218487394958\n",
      "epoch: 0\t| Loss: 2.1623150312900545\t| precision: 0.537746806039489\n",
      "epoch: 0\t| Loss: 2.145288189649582\t| precision: 0.5413758723828515\n",
      "epoch: 0\t| Loss: 2.1451762026548384\t| precision: 0.5568181818181818\n",
      "epoch: 0\t| Loss: 2.150370628237724\t| precision: 0.566320645905421\n",
      "epoch: 0\t| Loss: 2.16101021528244\t| precision: 0.5878378378378378\n",
      "epoch: 0\t| Loss: 2.158124601840973\t| precision: 0.5305889079473985\n",
      "epoch: 0\t| Loss: 2.1480284237861635\t| precision: 0.5731462925851704\n",
      "epoch: 0\t| Loss: 2.14569417655468\t| precision: 0.5503826530612245\n",
      "epoch: 0\t| Loss: 2.1542791754007338\t| precision: 0.56\n",
      "epoch: 0\t| Loss: 2.1481019753217696\t| precision: 0.5741538461538461\n",
      "mid epoch: 4439852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.155518668293953\t| precision: 0.5699263932702419\n",
      "epoch: 0\t| Loss: 2.152586333155632\t| precision: 0.5336225596529284\n",
      "epoch: 0\t| Loss: 2.146853061914444\t| precision: 0.5552776388194097\n",
      "epoch: 0\t| Loss: 2.15811692237854\t| precision: 0.5613948919449901\n",
      "epoch: 0\t| Loss: 2.150657185316086\t| precision: 0.5539340101522843\n",
      "epoch: 0\t| Loss: 2.147147578597069\t| precision: 0.5471877979027645\n",
      "epoch: 0\t| Loss: 2.1556693321466445\t| precision: 0.5535390199637024\n",
      "epoch: 0\t| Loss: 2.14981165766716\t| precision: 0.5596184419713831\n",
      "epoch: 0\t| Loss: 2.1476201099157333\t| precision: 0.5618311533888228\n",
      "epoch: 0\t| Loss: 2.1571033614873887\t| precision: 0.5532574974146846\n",
      "epoch: 0\t| Loss: 2.146839151382446\t| precision: 0.5726775956284152\n",
      "epoch: 0\t| Loss: 2.1558028048276903\t| precision: 0.573791348600509\n",
      "epoch: 0\t| Loss: 2.1475289767980574\t| precision: 0.5716568993074055\n",
      "epoch: 0\t| Loss: 2.1558224034309386\t| precision: 0.5728693898134141\n",
      "epoch: 0\t| Loss: 2.148239113688469\t| precision: 0.577313769751693\n",
      "mid epoch: 4883852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.147499327659607\t| precision: 0.5586061246040127\n",
      "epoch: 0\t| Loss: 2.1540789169073107\t| precision: 0.5657209033005212\n",
      "epoch: 0\t| Loss: 2.161045374274254\t| precision: 0.555613850996852\n",
      "epoch: 0\t| Loss: 2.1482958644628525\t| precision: 0.5568790946992257\n",
      "epoch: 0\t| Loss: 2.1440288507938385\t| precision: 0.5565693430656934\n",
      "epoch: 0\t| Loss: 2.1446632313728333\t| precision: 0.5268941584731058\n",
      "epoch: 0\t| Loss: 2.153345784544945\t| precision: 0.5649651972157773\n",
      "epoch: 0\t| Loss: 2.1559626281261446\t| precision: 0.5564915758176412\n",
      "epoch: 0\t| Loss: 2.147854586839676\t| precision: 0.5616031667491341\n",
      "epoch: 0\t| Loss: 2.155558683872223\t| precision: 0.5267857142857143\n",
      "epoch: 0\t| Loss: 2.163487885594368\t| precision: 0.5563076923076923\n",
      "epoch: 0\t| Loss: 2.152294116616249\t| precision: 0.5709624796084829\n",
      "epoch: 0\t| Loss: 2.150494644641876\t| precision: 0.5539053905390539\n",
      "epoch: 0\t| Loss: 2.1436678510904312\t| precision: 0.5621156211562116\n",
      "epoch: 0\t| Loss: 2.158918997049332\t| precision: 0.5649178590355061\n",
      "mid epoch: 5327852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1483892577886583\t| precision: 0.5575485799701047\n",
      "epoch: 0\t| Loss: 2.1533715397119524\t| precision: 0.5381035485747527\n",
      "epoch: 0\t| Loss: 2.1395549672842025\t| precision: 0.5715020576131687\n",
      "epoch: 0\t| Loss: 2.14886534512043\t| precision: 0.5455580865603644\n",
      "epoch: 0\t| Loss: 2.1490240532159803\t| precision: 0.5903846153846154\n",
      "epoch: 0\t| Loss: 2.146438202857971\t| precision: 0.5639097744360902\n",
      "epoch: 0\t| Loss: 2.1525904673337934\t| precision: 0.576943140323422\n",
      "epoch: 0\t| Loss: 2.1521478235721587\t| precision: 0.5721925133689839\n",
      "epoch: 0\t| Loss: 2.142914417386055\t| precision: 0.5431565967940813\n",
      "epoch: 0\t| Loss: 2.1579376178979874\t| precision: 0.5645901639344262\n",
      "epoch: 0\t| Loss: 2.1424826788902283\t| precision: 0.5675257731958763\n",
      "epoch: 0\t| Loss: 2.1463597452640535\t| precision: 0.5299019607843137\n",
      "epoch: 0\t| Loss: 2.154087124466896\t| precision: 0.5810732833237161\n",
      "epoch: 0\t| Loss: 2.1535032528638838\t| precision: 0.5722635494155154\n",
      "epoch: 0\t| Loss: 2.1482732474803923\t| precision: 0.5597852611029771\n",
      "mid epoch: 5771852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1466265523433687\t| precision: 0.5811965811965812\n",
      "epoch: 0\t| Loss: 2.1380092233419417\t| precision: 0.5510306686777275\n",
      "epoch: 0\t| Loss: 2.1564848279953\t| precision: 0.5543753161355589\n",
      "epoch: 0\t| Loss: 2.144267364144325\t| precision: 0.567194685743485\n",
      "epoch: 0\t| Loss: 2.135228006839752\t| precision: 0.5734886300610095\n",
      "epoch: 0\t| Loss: 2.155240275859833\t| precision: 0.5780010576414596\n",
      "epoch: 0\t| Loss: 2.161740971803665\t| precision: 0.5575221238938053\n",
      "epoch: 0\t| Loss: 2.1493296921253204\t| precision: 0.5534855769230769\n",
      "epoch: 0\t| Loss: 2.1450106787681578\t| precision: 0.5706682436428149\n",
      "epoch: 0\t| Loss: 2.148716452717781\t| precision: 0.587027027027027\n",
      "epoch: 0\t| Loss: 2.1503057968616486\t| precision: 0.5557819663779928\n",
      "epoch: 0\t| Loss: 2.154145488142967\t| precision: 0.5746013667425968\n",
      "epoch: 0\t| Loss: 2.1576088219881058\t| precision: 0.5441176470588235\n",
      "epoch: 0\t| Loss: 2.1514059007167816\t| precision: 0.5534084809447128\n",
      "epoch: 0\t| Loss: 2.1481907922029495\t| precision: 0.5774400935125658\n",
      "mid epoch: 6215852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1532005435228347\t| precision: 0.5567651632970451\n",
      "epoch: 0\t| Loss: 2.1598814314603807\t| precision: 0.5756302521008403\n",
      "epoch: 0\t| Loss: 2.1464307487010954\t| precision: 0.5745784695201037\n",
      "epoch: 0\t| Loss: 2.1488075029850005\t| precision: 0.5270114942528735\n",
      "epoch: 0\t| Loss: 2.150633688569069\t| precision: 0.5693950177935944\n",
      "epoch: 0\t| Loss: 2.147919764518738\t| precision: 0.5731462925851704\n",
      "epoch: 0\t| Loss: 2.147294713258743\t| precision: 0.5582104728012202\n",
      "epoch: 0\t| Loss: 2.14577991604805\t| precision: 0.5382498624105668\n",
      "epoch: 0\t| Loss: 2.1609820204973222\t| precision: 0.5768725361366623\n",
      "epoch: 0\t| Loss: 2.1445224052667617\t| precision: 0.5621301775147929\n",
      "epoch: 0\t| Loss: 2.1539816880226135\t| precision: 0.5543531726512543\n",
      "epoch: 0\t| Loss: 2.152254905104637\t| precision: 0.55214399152991\n",
      "epoch: 0\t| Loss: 2.1524206709861757\t| precision: 0.5771929824561404\n",
      "epoch: 0\t| Loss: 2.1396977531909944\t| precision: 0.5650632911392405\n",
      "epoch: 0\t| Loss: 2.1591691875457766\t| precision: 0.5660377358490566\n",
      "mid epoch: 6659852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1527040565013884\t| precision: 0.5612366230677764\n",
      "epoch: 0\t| Loss: 2.143013365864754\t| precision: 0.5553060078607523\n",
      "epoch: 0\t| Loss: 2.1494119518995287\t| precision: 0.5505425471159338\n",
      "epoch: 0\t| Loss: 2.1506891506910324\t| precision: 0.5844421699078812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.1511091810464857\t| precision: 0.5469644902634594\n",
      "epoch: 0\t| Loss: 2.1541340208053588\t| precision: 0.5554903112155021\n",
      "epoch: 0\t| Loss: 2.15660695374012\t| precision: 0.5612903225806452\n",
      "epoch: 0\t| Loss: 2.1325894367694853\t| precision: 0.5646295294753921\n",
      "epoch: 0\t| Loss: 2.14892192363739\t| precision: 0.539895165987187\n",
      "epoch: 0\t| Loss: 2.1352819633483886\t| precision: 0.5616365568544102\n",
      "epoch: 0\t| Loss: 2.1450619959831236\t| precision: 0.5631768953068592\n",
      "epoch: 0\t| Loss: 2.152791855931282\t| precision: 0.5748407643312102\n",
      "epoch: 0\t| Loss: 2.149012005329132\t| precision: 0.5480225988700564\n",
      "epoch: 0\t| Loss: 2.14685540497303\t| precision: 0.531658817373103\n",
      "epoch: 0\t| Loss: 2.143188470005989\t| precision: 0.5639810426540285\n",
      "mid epoch: 7103852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1475246500968934\t| precision: 0.5794594594594594\n",
      "epoch: 0\t| Loss: 2.156558933854103\t| precision: 0.5750724637681159\n",
      "epoch: 0\t| Loss: 2.1558963572978973\t| precision: 0.5534550195567145\n",
      "epoch: 0\t| Loss: 2.1550768637657165\t| precision: 0.5745207173778603\n",
      "epoch: 0\t| Loss: 2.1513807654380797\t| precision: 0.5627397260273973\n",
      "epoch: 0\t| Loss: 2.155090140104294\t| precision: 0.5629984051036683\n",
      "epoch: 0\t| Loss: 2.146611590385437\t| precision: 0.5461496450027308\n",
      "epoch: 0\t| Loss: 2.144010852575302\t| precision: 0.5568976478067387\n",
      "epoch: 0\t| Loss: 2.1490936988592146\t| precision: 0.5445052212829438\n",
      "epoch: 0\t| Loss: 2.1545133888721466\t| precision: 0.560999510044096\n",
      "epoch: 0\t| Loss: 2.1493881803750994\t| precision: 0.5403174603174603\n",
      "epoch: 0\t| Loss: 2.145575090050697\t| precision: 0.5618528610354223\n",
      "epoch: 0\t| Loss: 2.14928231716156\t| precision: 0.5416909620991254\n",
      "epoch: 0\t| Loss: 2.1399220114946367\t| precision: 0.5685279187817259\n",
      "epoch: 0\t| Loss: 2.151506576538086\t| precision: 0.5907545887151597\n",
      "mid epoch: 7547852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1414026433229445\t| precision: 0.5633889919604206\n",
      "epoch: 0\t| Loss: 2.152143794298172\t| precision: 0.5584965590259396\n",
      "epoch: 0\t| Loss: 2.1505954837799073\t| precision: 0.5839874411302983\n",
      "epoch: 0\t| Loss: 2.127728922367096\t| precision: 0.5713516424340334\n",
      "epoch: 0\t| Loss: 2.1412454479932785\t| precision: 0.5524017467248908\n",
      "epoch: 0\t| Loss: 2.139027948975563\t| precision: 0.5725971370143149\n",
      "epoch: 0\t| Loss: 2.1467455977201464\t| precision: 0.547918683446273\n",
      "epoch: 0\t| Loss: 2.1395933562517166\t| precision: 0.575503355704698\n",
      "epoch: 0\t| Loss: 2.149149782657623\t| precision: 0.5762806236080178\n",
      "epoch: 0\t| Loss: 2.1500688445568086\t| precision: 0.5435523114355231\n",
      "epoch: 0\t| Loss: 2.14961432993412\t| precision: 0.5607687959299039\n",
      "epoch: 0\t| Loss: 2.146173347234726\t| precision: 0.5623003194888179\n",
      "epoch: 0\t| Loss: 2.1574084359407424\t| precision: 0.5573192239858906\n",
      "epoch: 0\t| Loss: 2.1456960314512252\t| precision: 0.5495951417004049\n",
      "epoch: 0\t| Loss: 2.1541083925962448\t| precision: 0.5480338476854156\n",
      "mid epoch: 7991852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1390331840515135\t| precision: 0.5382857142857143\n",
      "epoch: 0\t| Loss: 2.154079779982567\t| precision: 0.5543637250121892\n",
      "epoch: 0\t| Loss: 2.1484179705381394\t| precision: 0.5997673065735893\n",
      "epoch: 0\t| Loss: 2.136751563549042\t| precision: 0.5742971887550201\n",
      "epoch: 0\t| Loss: 2.1506953835487366\t| precision: 0.5680507497116494\n",
      "epoch: 0\t| Loss: 2.1480197578668596\t| precision: 0.5298804780876494\n",
      "epoch: 0\t| Loss: 2.1476535427570345\t| precision: 0.5736074270557029\n",
      "epoch: 0\t| Loss: 2.1495628333091736\t| precision: 0.580542264752791\n",
      "epoch: 0\t| Loss: 2.1472527796030043\t| precision: 0.5556879094699225\n",
      "epoch: 0\t| Loss: 2.1520236146450045\t| precision: 0.5595947556615017\n",
      "epoch: 0\t| Loss: 2.1460943698883055\t| precision: 0.5501813784764208\n",
      "epoch: 0\t| Loss: 2.157348933815956\t| precision: 0.5609167671893848\n",
      "epoch: 0\t| Loss: 2.142778671979904\t| precision: 0.5896691816598956\n",
      "epoch: 0\t| Loss: 2.1462352418899537\t| precision: 0.5525866969869244\n",
      "epoch: 0\t| Loss: 2.1568061250448225\t| precision: 0.5560796645702306\n",
      "mid epoch: 8435852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1489929831027985\t| precision: 0.5510930350788003\n",
      "epoch: 0\t| Loss: 2.1499848502874372\t| precision: 0.5522914218566393\n",
      "epoch: 0\t| Loss: 2.148704441189766\t| precision: 0.5558568329718004\n",
      "epoch: 0\t| Loss: 2.148149641752243\t| precision: 0.5595475715236194\n",
      "epoch: 0\t| Loss: 2.138488371372223\t| precision: 0.5389792484576557\n",
      "epoch: 0\t| Loss: 2.14965439081192\t| precision: 0.5438302624232273\n",
      "epoch: 0\t| Loss: 2.1476151329278945\t| precision: 0.5923125393824826\n",
      "epoch: 0\t| Loss: 2.1525793582201005\t| precision: 0.5489762036524627\n",
      "epoch: 0\t| Loss: 2.1407365763187407\t| precision: 0.560378408458542\n",
      "epoch: 0\t| Loss: 2.1460179883241652\t| precision: 0.5615853658536586\n",
      "epoch: 0\t| Loss: 2.1482490521669386\t| precision: 0.5511627906976744\n",
      "epoch: 0\t| Loss: 2.1555043601989747\t| precision: 0.558645707376058\n",
      "epoch: 0\t| Loss: 2.141304722428322\t| precision: 0.5808909730363423\n",
      "epoch: 0\t| Loss: 2.1412725901603697\t| precision: 0.572560975609756\n",
      "epoch: 0\t| Loss: 2.1459929955005648\t| precision: 0.5696\n",
      "mid epoch: 8879852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1546708762645723\t| precision: 0.5695171611401978\n",
      "epoch: 0\t| Loss: 2.139103310704231\t| precision: 0.571858087793145\n",
      "epoch: 0\t| Loss: 2.1438562059402466\t| precision: 0.5587724377533295\n",
      "epoch: 0\t| Loss: 2.1443230968713762\t| precision: 0.5838627700127065\n",
      "epoch: 0\t| Loss: 2.1420334988832472\t| precision: 0.5692695214105793\n",
      "epoch: 0\t| Loss: 2.1536656326055525\t| precision: 0.5418227215980025\n",
      "epoch: 0\t| Loss: 2.1506647926568987\t| precision: 0.578031538879826\n",
      "epoch: 0\t| Loss: 2.1403316116333007\t| precision: 0.5508431272355646\n",
      "epoch: 0\t| Loss: 2.152538647055626\t| precision: 0.5622022233986236\n",
      "epoch: 0\t| Loss: 2.152303408384323\t| precision: 0.5425904317386231\n",
      "epoch: 0\t| Loss: 2.1469902282953264\t| precision: 0.5638569604086845\n",
      "epoch: 0\t| Loss: 2.152327359318733\t| precision: 0.5417417417417417\n",
      "epoch: 0\t| Loss: 2.154147388935089\t| precision: 0.5705361790733993\n",
      "epoch: 0\t| Loss: 2.1533380967378615\t| precision: 0.5596192384769539\n",
      "epoch: 0\t| Loss: 2.1431917405128478\t| precision: 0.586897671900379\n",
      "mid epoch: 9323852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1618989700078965\t| precision: 0.5511720983419096\n",
      "epoch: 0\t| Loss: 2.146900613307953\t| precision: 0.5640895218718209\n",
      "epoch: 0\t| Loss: 2.153223950266838\t| precision: 0.5573681348559\n",
      "epoch: 0\t| Loss: 2.149267435669899\t| precision: 0.5411324786324786\n",
      "epoch: 0\t| Loss: 2.1448610401153565\t| precision: 0.5639567936327459\n",
      "epoch: 0\t| Loss: 2.151811378002167\t| precision: 0.5656894679695983\n",
      "epoch: 0\t| Loss: 2.1537694638967513\t| precision: 0.5572949279899813\n",
      "epoch: 0\t| Loss: 2.1431133466959\t| precision: 0.543217665615142\n",
      "epoch: 0\t| Loss: 2.14686038851738\t| precision: 0.5590838105153566\n",
      "epoch: 0\t| Loss: 2.153180550932884\t| precision: 0.5680511182108626\n",
      "epoch: 0\t| Loss: 2.1468943256139754\t| precision: 0.557347670250896\n",
      "epoch: 0\t| Loss: 2.148143221735954\t| precision: 0.5493218249075216\n",
      "epoch: 0\t| Loss: 2.14375963807106\t| precision: 0.5528992878942014\n",
      "epoch: 0\t| Loss: 2.1494368928670884\t| precision: 0.5576407506702413\n",
      "epoch: 0\t| Loss: 2.15825807094574\t| precision: 0.555421686746988\n",
      "mid epoch: 9767852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1427631425857543\t| precision: 0.554525627044711\n",
      "epoch: 0\t| Loss: 2.150207146406174\t| precision: 0.5534031413612566\n",
      "epoch: 0\t| Loss: 2.1517421334981917\t| precision: 0.5631469979296067\n",
      "epoch: 0\t| Loss: 2.1517445820569994\t| precision: 0.5542234332425068\n",
      "epoch: 0\t| Loss: 2.139091147780418\t| precision: 0.5501577287066246\n",
      "epoch: 0\t| Loss: 2.150247583985329\t| precision: 0.534376746785914\n",
      "epoch: 0\t| Loss: 2.1507797092199326\t| precision: 0.5635018495684341\n",
      "epoch: 0\t| Loss: 2.1453900891542435\t| precision: 0.5587743732590529\n",
      "epoch: 0\t| Loss: 2.1496105182170866\t| precision: 0.5731857318573186\n",
      "epoch: 0\t| Loss: 2.1447644770145415\t| precision: 0.5297941495124594\n",
      "epoch: 0\t| Loss: 2.148436824083328\t| precision: 0.5824634655532359\n",
      "epoch: 0\t| Loss: 2.144345682263374\t| precision: 0.5764533463605276\n",
      "epoch: 0\t| Loss: 2.1370826971530916\t| precision: 0.5389331698344574\n",
      "epoch: 0\t| Loss: 2.152011709213257\t| precision: 0.5472081218274112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.1466317039728167\t| precision: 0.5646656905807711\n",
      "mid epoch: 10211852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.143929013609886\t| precision: 0.5780254777070064\n",
      "epoch: 0\t| Loss: 2.1466441309452056\t| precision: 0.5673758865248227\n",
      "epoch: 0\t| Loss: 2.1409251260757447\t| precision: 0.5454545454545454\n",
      "epoch: 0\t| Loss: 2.151812358498573\t| precision: 0.5573192239858906\n",
      "epoch: 0\t| Loss: 2.1512683886289596\t| precision: 0.5797260273972603\n",
      "epoch: 0\t| Loss: 2.1536584949493407\t| precision: 0.5909859154929578\n",
      "epoch: 0\t| Loss: 2.157635272741318\t| precision: 0.5361749859786876\n",
      "epoch: 0\t| Loss: 2.1425432097911834\t| precision: 0.5327536231884058\n",
      "epoch: 0\t| Loss: 2.1497211611270903\t| precision: 0.5396825396825397\n",
      "epoch: 0\t| Loss: 2.153931212425232\t| precision: 0.5697808535178778\n",
      "epoch: 0\t| Loss: 2.156094344854355\t| precision: 0.5558297347316471\n",
      "epoch: 0\t| Loss: 2.146078940629959\t| precision: 0.5620915032679739\n",
      "epoch: 0\t| Loss: 2.143744857311249\t| precision: 0.5791324736225087\n",
      "epoch: 0\t| Loss: 2.1414811342954634\t| precision: 0.5606500290191526\n",
      "epoch: 0\t| Loss: 2.1482245510816576\t| precision: 0.5616519174041298\n",
      "mid epoch: 10655852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.146256077289581\t| precision: 0.5602836879432624\n",
      "epoch: 0\t| Loss: 2.1556699681282043\t| precision: 0.5375959079283887\n",
      "epoch: 0\t| Loss: 2.1492613142728807\t| precision: 0.5398288347597103\n",
      "epoch: 0\t| Loss: 2.1425763112306595\t| precision: 0.5501618122977346\n",
      "epoch: 0\t| Loss: 2.1465037518739702\t| precision: 0.5532994923857868\n",
      "epoch: 0\t| Loss: 2.1414965283870697\t| precision: 0.5680647534952171\n",
      "epoch: 0\t| Loss: 2.140586505532265\t| precision: 0.5813253012048193\n",
      "epoch: 0\t| Loss: 2.150320125222206\t| precision: 0.5414430530709601\n",
      "epoch: 0\t| Loss: 2.1436064994335173\t| precision: 0.5559701492537313\n",
      "epoch: 0\t| Loss: 2.147501067519188\t| precision: 0.5640394088669951\n",
      "epoch: 0\t| Loss: 2.1406760734319685\t| precision: 0.5500963391136802\n",
      "epoch: 0\t| Loss: 2.1393350911140443\t| precision: 0.5616438356164384\n",
      "epoch: 0\t| Loss: 2.1432219064235687\t| precision: 0.5667621776504298\n",
      "epoch: 0\t| Loss: 2.1581231123209\t| precision: 0.5390070921985816\n",
      "epoch: 0\t| Loss: 2.1439686858654023\t| precision: 0.5717171717171717\n",
      "mid epoch: 11099852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1392831945419313\t| precision: 0.5637204522096608\n",
      "epoch: 0\t| Loss: 2.1443752270936964\t| precision: 0.5722793719545208\n",
      "epoch: 0\t| Loss: 2.13864897608757\t| precision: 0.5723491505565319\n",
      "epoch: 0\t| Loss: 2.1459947699308395\t| precision: 0.576092971776425\n",
      "epoch: 0\t| Loss: 2.154950442910194\t| precision: 0.5449172576832151\n",
      "epoch: 0\t| Loss: 2.152441537976265\t| precision: 0.5676767676767677\n",
      "epoch: 0\t| Loss: 2.148060480356216\t| precision: 0.5390904445579969\n",
      "epoch: 0\t| Loss: 2.1415522760152816\t| precision: 0.5819672131147541\n",
      "epoch: 0\t| Loss: 2.1507821679115295\t| precision: 0.5927152317880795\n",
      "epoch: 0\t| Loss: 2.14951183617115\t| precision: 0.5311791383219955\n",
      "epoch: 0\t| Loss: 2.148629999756813\t| precision: 0.5496876774559909\n",
      "epoch: 0\t| Loss: 2.1381743288040163\t| precision: 0.5680534155110426\n",
      "epoch: 0\t| Loss: 2.149762657880783\t| precision: 0.53966649823143\n",
      "epoch: 0\t| Loss: 2.145026606321335\t| precision: 0.5457446808510639\n",
      "epoch: 0\t| Loss: 2.158045441508293\t| precision: 0.5480371900826446\n",
      "mid epoch: 11543852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.143549306988716\t| precision: 0.5808153477218225\n",
      "epoch: 0\t| Loss: 2.148690192699432\t| precision: 0.586888657648283\n",
      "epoch: 0\t| Loss: 2.139711124300957\t| precision: 0.5379382522239665\n",
      "epoch: 0\t| Loss: 2.1471599280834197\t| precision: 0.5718816067653277\n",
      "epoch: 0\t| Loss: 2.1422365599870683\t| precision: 0.537639405204461\n",
      "epoch: 0\t| Loss: 2.157763006091118\t| precision: 0.5608072225172597\n",
      "epoch: 0\t| Loss: 2.1427480471134186\t| precision: 0.5514929920780012\n",
      "epoch: 0\t| Loss: 2.140031997561455\t| precision: 0.54239663629993\n",
      "epoch: 0\t| Loss: 2.1473630982637406\t| precision: 0.5447556287753982\n",
      "epoch: 0\t| Loss: 2.1424411237239838\t| precision: 0.5535261288685946\n",
      "epoch: 0\t| Loss: 2.162966905236244\t| precision: 0.5627906976744186\n",
      "epoch: 0\t| Loss: 2.143848705291748\t| precision: 0.5372871403405755\n",
      "epoch: 0\t| Loss: 2.141103767156601\t| precision: 0.5751050420168067\n",
      "epoch: 0\t| Loss: 2.1465840297937393\t| precision: 0.5945366898768077\n",
      "epoch: 0\t| Loss: 2.1406693089008333\t| precision: 0.5446973903387007\n",
      "mid epoch: 11987852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1542468518018723\t| precision: 0.5420460632029994\n",
      "epoch: 0\t| Loss: 2.1427859973907473\t| precision: 0.5850945494994438\n",
      "epoch: 0\t| Loss: 2.14923809170723\t| precision: 0.5407911001236094\n",
      "epoch: 0\t| Loss: 2.146896169781685\t| precision: 0.5764895330112721\n",
      "epoch: 0\t| Loss: 2.147424347400665\t| precision: 0.5664297430289775\n",
      "epoch: 0\t| Loss: 2.146130291223526\t| precision: 0.5811777655476059\n",
      "epoch: 0\t| Loss: 2.144150670170784\t| precision: 0.5488301119023398\n",
      "epoch: 0\t| Loss: 2.13914304792881\t| precision: 0.5493890020366599\n",
      "epoch: 0\t| Loss: 2.1478707259893417\t| precision: 0.5629389519178822\n",
      "epoch: 0\t| Loss: 2.1358035629987717\t| precision: 0.5482717520858165\n",
      "epoch: 0\t| Loss: 2.1415083265304564\t| precision: 0.580896686159844\n",
      "epoch: 0\t| Loss: 2.1443690943717955\t| precision: 0.5524625267665952\n",
      "epoch: 0\t| Loss: 2.150177640914917\t| precision: 0.5586124401913876\n",
      "mid epoch: 12431852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.151946104764938\t| precision: 0.56656346749226\n",
      "epoch: 0\t| Loss: 2.1429263961315157\t| precision: 0.5743243243243243\n",
      "epoch: 0\t| Loss: 2.1486258274316787\t| precision: 0.5341701534170153\n",
      "epoch: 0\t| Loss: 2.1548735028505326\t| precision: 0.5777917189460476\n",
      "epoch: 0\t| Loss: 2.1504476255178453\t| precision: 0.5375777884155098\n",
      "epoch: 0\t| Loss: 2.1472371339797975\t| precision: 0.5522648083623694\n",
      "epoch: 0\t| Loss: 2.1477741742134095\t| precision: 0.5727170236753101\n",
      "epoch: 0\t| Loss: 2.145345754623413\t| precision: 0.5472350230414746\n",
      "epoch: 0\t| Loss: 2.144850566983223\t| precision: 0.5643985419198055\n",
      "epoch: 0\t| Loss: 2.141821804046631\t| precision: 0.5708131394722671\n",
      "epoch: 0\t| Loss: 2.1598781967163085\t| precision: 0.5643896976483762\n",
      "epoch: 0\t| Loss: 2.15658531665802\t| precision: 0.5452475811041548\n",
      "epoch: 0\t| Loss: 2.144954395890236\t| precision: 0.5687318489835431\n",
      "epoch: 0\t| Loss: 2.149453988671303\t| precision: 0.5624619134673979\n",
      "epoch: 0\t| Loss: 2.147367842793465\t| precision: 0.5750816104461371\n",
      "mid epoch: 12875852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.148297643065453\t| precision: 0.5761124121779859\n",
      "epoch: 0\t| Loss: 2.147790814042091\t| precision: 0.5233539673607203\n",
      "epoch: 0\t| Loss: 2.1526627933979032\t| precision: 0.5697606538237011\n",
      "epoch: 0\t| Loss: 2.14366799890995\t| precision: 0.5722336065573771\n",
      "epoch: 0\t| Loss: 2.1360428273677825\t| precision: 0.5762304921968787\n",
      "epoch: 0\t| Loss: 2.1485544615983962\t| precision: 0.5518896220755849\n",
      "epoch: 0\t| Loss: 2.1392791414260866\t| precision: 0.5647425897035881\n",
      "epoch: 0\t| Loss: 2.1471340411901476\t| precision: 0.5661971830985916\n",
      "epoch: 0\t| Loss: 2.1472973453998567\t| precision: 0.5545502927088877\n",
      "epoch: 0\t| Loss: 2.1487435406446456\t| precision: 0.5642954856361149\n",
      "epoch: 0\t| Loss: 2.1446097499132155\t| precision: 0.5448811256671519\n",
      "epoch: 0\t| Loss: 2.1364121520519257\t| precision: 0.5552030456852792\n",
      "epoch: 0\t| Loss: 2.150482481122017\t| precision: 0.5679275439531166\n",
      "epoch: 0\t| Loss: 2.1480097299814225\t| precision: 0.5706440339490764\n",
      "epoch: 0\t| Loss: 2.15109467625618\t| precision: 0.5348550277606415\n",
      "mid epoch: 13319852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.148503289222717\t| precision: 0.5633314054366686\n",
      "epoch: 0\t| Loss: 2.147056456208229\t| precision: 0.5730607336139507\n",
      "epoch: 0\t| Loss: 2.1473317819833757\t| precision: 0.5606157793457345\n",
      "epoch: 0\t| Loss: 2.153668440580368\t| precision: 0.5321366531557614\n",
      "epoch: 0\t| Loss: 2.1506158363819123\t| precision: 0.5547631176770249\n",
      "epoch: 0\t| Loss: 2.1420786488056183\t| precision: 0.5688172043010753\n",
      "epoch: 0\t| Loss: 2.142614821791649\t| precision: 0.5495049504950495\n",
      "epoch: 0\t| Loss: 2.1428752398490905\t| precision: 0.5190615835777126\n",
      "epoch: 0\t| Loss: 2.147534528970718\t| precision: 0.5477333333333333\n",
      "epoch: 0\t| Loss: 2.140232890844345\t| precision: 0.5279383429672447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 2.1364437687397\t| precision: 0.5432801822323462\n",
      "epoch: 0\t| Loss: 2.150751605629921\t| precision: 0.5492447129909366\n",
      "epoch: 0\t| Loss: 2.144923213720322\t| precision: 0.5328425821064553\n",
      "epoch: 0\t| Loss: 2.138412075638771\t| precision: 0.5862516212710766\n",
      "epoch: 0\t| Loss: 2.147397063374519\t| precision: 0.5571764705882353\n",
      "mid epoch: 13763852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1456904470920564\t| precision: 0.5651672433679354\n",
      "epoch: 0\t| Loss: 2.1529630041122436\t| precision: 0.559787849566056\n",
      "epoch: 0\t| Loss: 2.153595666885376\t| precision: 0.5463855421686747\n",
      "epoch: 0\t| Loss: 2.1477474510669707\t| precision: 0.5808035714285714\n",
      "epoch: 0\t| Loss: 2.139820844531059\t| precision: 0.5462184873949579\n",
      "epoch: 0\t| Loss: 2.1425115877389906\t| precision: 0.5528228060368922\n",
      "epoch: 0\t| Loss: 2.143101435303688\t| precision: 0.5584269662921348\n",
      "epoch: 0\t| Loss: 2.1501821261644363\t| precision: 0.5772663222170032\n",
      "epoch: 0\t| Loss: 2.1451332497596742\t| precision: 0.5569744597249509\n",
      "epoch: 0\t| Loss: 2.1433865201473234\t| precision: 0.5656810470189045\n",
      "epoch: 0\t| Loss: 2.1451987159252166\t| precision: 0.5725893824485374\n",
      "epoch: 0\t| Loss: 2.157441322803497\t| precision: 0.543906333155934\n",
      "epoch: 0\t| Loss: 2.1481186503171923\t| precision: 0.5680317040951123\n",
      "epoch: 0\t| Loss: 2.14330894947052\t| precision: 0.5466346153846153\n",
      "epoch: 0\t| Loss: 2.1526664620637894\t| precision: 0.5628664495114006\n",
      "mid epoch: 14207852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.152906636595726\t| precision: 0.5641632170313424\n",
      "epoch: 0\t| Loss: 2.1467447698116304\t| precision: 0.5691651432053626\n",
      "epoch: 0\t| Loss: 2.142994395494461\t| precision: 0.5481890730509515\n",
      "epoch: 0\t| Loss: 2.153565909862518\t| precision: 0.5779554540262707\n",
      "epoch: 0\t| Loss: 2.1523881554603577\t| precision: 0.5751816657350475\n",
      "epoch: 0\t| Loss: 2.1421866470575335\t| precision: 0.553821313240043\n",
      "epoch: 0\t| Loss: 2.1404050993919372\t| precision: 0.5880845611787315\n",
      "epoch: 0\t| Loss: 2.151598395705223\t| precision: 0.5524787851719518\n",
      "epoch: 0\t| Loss: 2.149189041852951\t| precision: 0.5750133475707421\n",
      "epoch: 0\t| Loss: 2.139722864031792\t| precision: 0.5565675934803451\n",
      "epoch: 0\t| Loss: 2.1489668542146685\t| precision: 0.5635391923990499\n",
      "epoch: 0\t| Loss: 2.1449983167648314\t| precision: 0.5660954712362302\n",
      "epoch: 0\t| Loss: 2.143537160754204\t| precision: 0.5480955455132344\n",
      "epoch: 0\t| Loss: 2.1461853301525116\t| precision: 0.565914489311164\n",
      "epoch: 0\t| Loss: 2.1487427771091463\t| precision: 0.5516871987145152\n",
      "mid epoch: 14651852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1463293826580045\t| precision: 0.5630105017502918\n",
      "epoch: 0\t| Loss: 2.1429143464565277\t| precision: 0.5491891891891892\n",
      "epoch: 0\t| Loss: 2.156216242313385\t| precision: 0.5303600214938206\n",
      "epoch: 0\t| Loss: 2.1401694321632387\t| precision: 0.5632241813602015\n",
      "epoch: 0\t| Loss: 2.1515030419826506\t| precision: 0.5776019476567255\n",
      "epoch: 0\t| Loss: 2.1491029864549636\t| precision: 0.5712158808933002\n",
      "epoch: 0\t| Loss: 2.1529401570558546\t| precision: 0.5616207090602139\n",
      "epoch: 0\t| Loss: 2.1468980211019515\t| precision: 0.5726457399103139\n",
      "epoch: 0\t| Loss: 2.144095302820206\t| precision: 0.5586470267321331\n",
      "epoch: 0\t| Loss: 2.1385008192062376\t| precision: 0.5611957134799774\n",
      "epoch: 0\t| Loss: 2.144728870987892\t| precision: 0.570308898109728\n",
      "epoch: 0\t| Loss: 2.147803304195404\t| precision: 0.5557404326123128\n",
      "epoch: 0\t| Loss: 2.149598163366318\t| precision: 0.5431530494821634\n",
      "epoch: 0\t| Loss: 2.1560992050170897\t| precision: 0.5482885085574573\n",
      "epoch: 0\t| Loss: 2.147357432246208\t| precision: 0.5502742230347349\n",
      "mid epoch: 15095852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.152429006695747\t| precision: 0.5628066732090284\n",
      "epoch: 0\t| Loss: 2.151731697320938\t| precision: 0.5607529948659441\n",
      "epoch: 0\t| Loss: 2.146949713230133\t| precision: 0.538750587130108\n",
      "epoch: 0\t| Loss: 2.153985986113548\t| precision: 0.5483359746434231\n",
      "epoch: 0\t| Loss: 2.143217254281044\t| precision: 0.5361842105263158\n",
      "epoch: 0\t| Loss: 2.154082635641098\t| precision: 0.5708131394722671\n",
      "epoch: 0\t| Loss: 2.1520790499448776\t| precision: 0.5585585585585585\n",
      "epoch: 0\t| Loss: 2.1499055361747743\t| precision: 0.5589931830099633\n",
      "epoch: 0\t| Loss: 2.141760596036911\t| precision: 0.5551820728291317\n",
      "epoch: 0\t| Loss: 2.150459426641464\t| precision: 0.5558528428093645\n",
      "epoch: 0\t| Loss: 2.1405246728658676\t| precision: 0.5630455868089234\n",
      "epoch: 0\t| Loss: 2.1333488100767135\t| precision: 0.5392699115044248\n",
      "epoch: 0\t| Loss: 2.1358938598632813\t| precision: 0.5558252427184466\n",
      "epoch: 0\t| Loss: 2.1406055808067324\t| precision: 0.569620253164557\n",
      "epoch: 0\t| Loss: 2.1630506217479706\t| precision: 0.5499729875742841\n",
      "mid epoch: 15539852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1368324434757233\t| precision: 0.5794037940379404\n",
      "epoch: 0\t| Loss: 2.145736998319626\t| precision: 0.5643511039310716\n",
      "epoch: 0\t| Loss: 2.1436915838718416\t| precision: 0.5538128718226069\n",
      "epoch: 0\t| Loss: 2.1479421758651736\t| precision: 0.56150027578599\n",
      "epoch: 0\t| Loss: 2.1418577033281325\t| precision: 0.5695468914646997\n",
      "epoch: 0\t| Loss: 2.1547684890031813\t| precision: 0.5362694300518135\n",
      "epoch: 0\t| Loss: 2.1475303465127946\t| precision: 0.5356020942408377\n",
      "epoch: 0\t| Loss: 2.148964840173721\t| precision: 0.566027397260274\n",
      "epoch: 0\t| Loss: 2.1325263667106626\t| precision: 0.535818287711124\n",
      "epoch: 0\t| Loss: 2.1552966809272767\t| precision: 0.5622047244094488\n",
      "epoch: 0\t| Loss: 2.1512535750865935\t| precision: 0.5678884873515746\n",
      "epoch: 0\t| Loss: 2.1462099277973175\t| precision: 0.5644562334217507\n",
      "epoch: 0\t| Loss: 2.137214496731758\t| precision: 0.5568181818181818\n",
      "epoch: 0\t| Loss: 2.129959541559219\t| precision: 0.5670103092783505\n",
      "epoch: 0\t| Loss: 2.14708144903183\t| precision: 0.5694581280788177\n",
      "mid epoch: 15983852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.1445968610048296\t| precision: 0.5530487804878049\n",
      "epoch: 0\t| Loss: 2.147886840701103\t| precision: 0.5604575163398693\n",
      "epoch: 0\t| Loss: 2.1546219891309737\t| precision: 0.5728854229154169\n",
      "epoch: 0\t| Loss: 2.145978106856346\t| precision: 0.5660860655737705\n",
      "epoch: 0\t| Loss: 2.148358787894249\t| precision: 0.5658174097664543\n",
      "epoch: 0\t| Loss: 2.145367431640625\t| precision: 0.5705521472392638\n",
      "epoch: 0\t| Loss: 2.1484837698936463\t| precision: 0.5499172642029785\n",
      "epoch: 0\t| Loss: 2.1492263066768644\t| precision: 0.5858250276854928\n",
      "epoch: 0\t| Loss: 2.139646234512329\t| precision: 0.5640599001663894\n",
      "epoch: 0\t| Loss: 2.146806759238243\t| precision: 0.5469343461747151\n",
      "epoch: 0\t| Loss: 2.1447433680295944\t| precision: 0.5525581395348838\n",
      "epoch: 0\t| Loss: 2.1467316329479216\t| precision: 0.5547945205479452\n",
      "epoch: 0\t| Loss: 2.1440533173084257\t| precision: 0.5531177829099307\n",
      "epoch: 0\t| Loss: 2.130211501121521\t| precision: 0.5609195402298851\n",
      "epoch: 0\t| Loss: 2.1533090150356293\t| precision: 0.5698689956331878\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "done epoch 0\n",
      "epoch: 1\t| Loss: 0.01099313735961914\t| precision: 0.5511588468061052\n",
      "epoch: 1\t| Loss: 2.137311379313469\t| precision: 0.5574365175332527\n",
      "epoch: 1\t| Loss: 2.147504686713219\t| precision: 0.5583284968078932\n",
      "epoch: 1\t| Loss: 2.1489435809850694\t| precision: 0.5786004056795132\n",
      "epoch: 1\t| Loss: 2.139996083974838\t| precision: 0.5718954248366013\n",
      "epoch: 1\t| Loss: 2.1358509641885757\t| precision: 0.5719044975013882\n",
      "epoch: 1\t| Loss: 2.1603843492269514\t| precision: 0.5683987274655355\n",
      "epoch: 1\t| Loss: 2.152230402827263\t| precision: 0.5645161290322581\n",
      "epoch: 1\t| Loss: 2.142160131931305\t| precision: 0.593613024420789\n",
      "epoch: 1\t| Loss: 2.1457929754257203\t| precision: 0.5771771771771772\n",
      "epoch: 1\t| Loss: 2.145828938484192\t| precision: 0.5511363636363636\n",
      "epoch: 1\t| Loss: 2.1517000722885133\t| precision: 0.5865327820437094\n",
      "epoch: 1\t| Loss: 2.14726903796196\t| precision: 0.6\n",
      "epoch: 1\t| Loss: 2.1525896614789963\t| precision: 0.5494618144541261\n",
      "epoch: 1\t| Loss: 2.1419627219438553\t| precision: 0.5789473684210527\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1525911545753478\t| precision: 0.5792192881745121\n",
      "epoch: 1\t| Loss: 2.1441588467359542\t| precision: 0.5545722713864307\n",
      "epoch: 1\t| Loss: 2.1473461490869523\t| precision: 0.5467706013363028\n",
      "epoch: 1\t| Loss: 2.145569252371788\t| precision: 0.5424039048200122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.141742575764656\t| precision: 0.5385906040268457\n",
      "epoch: 1\t| Loss: 2.1355660665035248\t| precision: 0.541371158392435\n",
      "epoch: 1\t| Loss: 2.1496556210517883\t| precision: 0.5493537015276145\n",
      "epoch: 1\t| Loss: 2.1442068123817446\t| precision: 0.5979447655748233\n",
      "epoch: 1\t| Loss: 2.1378131699562073\t| precision: 0.5515850144092219\n",
      "epoch: 1\t| Loss: 2.1470706814527514\t| precision: 0.5701204819277108\n",
      "epoch: 1\t| Loss: 2.13610587477684\t| precision: 0.5506849315068493\n",
      "epoch: 1\t| Loss: 2.1453567260503767\t| precision: 0.5346878097125867\n",
      "epoch: 1\t| Loss: 2.1297277933359147\t| precision: 0.5758873929008568\n",
      "epoch: 1\t| Loss: 2.1394795423746107\t| precision: 0.5327706057596823\n",
      "epoch: 1\t| Loss: 2.1543879371881487\t| precision: 0.5443262411347518\n",
      "mid epoch: 887852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.141809510588646\t| precision: 0.5630208333333333\n",
      "epoch: 1\t| Loss: 2.1491871309280395\t| precision: 0.5858250276854928\n",
      "epoch: 1\t| Loss: 2.1493734723329543\t| precision: 0.5490380103237916\n",
      "epoch: 1\t| Loss: 2.148566171526909\t| precision: 0.5598991172761665\n",
      "epoch: 1\t| Loss: 2.1423377108573916\t| precision: 0.5563118811881188\n",
      "epoch: 1\t| Loss: 2.14481956243515\t| precision: 0.5780615206035984\n",
      "epoch: 1\t| Loss: 2.1438055908679963\t| precision: 0.5714285714285714\n",
      "epoch: 1\t| Loss: 2.1411093318462373\t| precision: 0.5365355135411344\n",
      "epoch: 1\t| Loss: 2.148714895248413\t| precision: 0.5744558991981672\n",
      "epoch: 1\t| Loss: 2.149151269197464\t| precision: 0.5940919037199125\n",
      "epoch: 1\t| Loss: 2.1473563927412034\t| precision: 0.5624012638230648\n",
      "epoch: 1\t| Loss: 2.1419022303819655\t| precision: 0.5836273817925194\n",
      "epoch: 1\t| Loss: 2.143749097585678\t| precision: 0.5679374389051809\n",
      "epoch: 1\t| Loss: 2.15297384262085\t| precision: 0.5606860158311345\n",
      "epoch: 1\t| Loss: 2.14271512567997\t| precision: 0.5642985611510791\n",
      "mid epoch: 1331852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1483100062608718\t| precision: 0.5827338129496403\n",
      "epoch: 1\t| Loss: 2.14952609539032\t| precision: 0.5989492119089317\n",
      "epoch: 1\t| Loss: 2.1430521112680436\t| precision: 0.5577536694320357\n",
      "epoch: 1\t| Loss: 2.1485343903303145\t| precision: 0.552733296521259\n",
      "epoch: 1\t| Loss: 2.1477650570869447\t| precision: 0.552185089974293\n",
      "epoch: 1\t| Loss: 2.151677443981171\t| precision: 0.5728051391862955\n",
      "epoch: 1\t| Loss: 2.144220101237297\t| precision: 0.5641187618445989\n",
      "epoch: 1\t| Loss: 2.14649349629879\t| precision: 0.5561758548499651\n",
      "epoch: 1\t| Loss: 2.150869401693344\t| precision: 0.5314401622718052\n",
      "epoch: 1\t| Loss: 2.135443738102913\t| precision: 0.5619742007851934\n",
      "epoch: 1\t| Loss: 2.137490334510803\t| precision: 0.5663764961915125\n",
      "epoch: 1\t| Loss: 2.14683178961277\t| precision: 0.5599051008303677\n",
      "epoch: 1\t| Loss: 2.1501507252454757\t| precision: 0.5518590998043053\n",
      "epoch: 1\t| Loss: 2.1401076608896257\t| precision: 0.5556986477784932\n",
      "epoch: 1\t| Loss: 2.143133800625801\t| precision: 0.5665310865775712\n",
      "mid epoch: 1775852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1573347449302673\t| precision: 0.559286463798531\n",
      "epoch: 1\t| Loss: 2.139096776843071\t| precision: 0.5497780596068484\n",
      "epoch: 1\t| Loss: 2.1461223804950715\t| precision: 0.5588382573860792\n",
      "epoch: 1\t| Loss: 2.14886754989624\t| precision: 0.5427485750474984\n",
      "epoch: 1\t| Loss: 2.150004182457924\t| precision: 0.563265306122449\n",
      "epoch: 1\t| Loss: 2.1460737001895906\t| precision: 0.554046997389034\n",
      "epoch: 1\t| Loss: 2.1487277758121492\t| precision: 0.5436793422404933\n",
      "epoch: 1\t| Loss: 2.1506414610147475\t| precision: 0.5761234071093226\n",
      "epoch: 1\t| Loss: 2.136636720299721\t| precision: 0.5466983938132064\n",
      "epoch: 1\t| Loss: 2.150188834667206\t| precision: 0.5514584479911943\n",
      "epoch: 1\t| Loss: 2.1486768913269043\t| precision: 0.5742268041237113\n",
      "epoch: 1\t| Loss: 2.149470002651215\t| precision: 0.535607420706164\n",
      "epoch: 1\t| Loss: 2.1609516870975494\t| precision: 0.5583941605839416\n",
      "epoch: 1\t| Loss: 2.1362724256515504\t| precision: 0.5587259455872594\n",
      "epoch: 1\t| Loss: 2.141678589582443\t| precision: 0.5561993047508691\n",
      "mid epoch: 2219852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.14243354678154\t| precision: 0.5864799025578563\n",
      "epoch: 1\t| Loss: 2.13758851647377\t| precision: 0.5521143537820131\n",
      "epoch: 1\t| Loss: 2.150291351079941\t| precision: 0.5704014379868184\n",
      "epoch: 1\t| Loss: 2.1499349266290664\t| precision: 0.5546075085324232\n",
      "epoch: 1\t| Loss: 2.1480226427316667\t| precision: 0.5822532402791625\n",
      "epoch: 1\t| Loss: 2.1372701877355578\t| precision: 0.556498673740053\n",
      "epoch: 1\t| Loss: 2.148056827187538\t| precision: 0.5369211514392991\n",
      "epoch: 1\t| Loss: 2.1311083000898363\t| precision: 0.5605849582172702\n",
      "epoch: 1\t| Loss: 2.143820288181305\t| precision: 0.5626822157434402\n",
      "epoch: 1\t| Loss: 2.147986198067665\t| precision: 0.5487571701720841\n",
      "epoch: 1\t| Loss: 2.14445361495018\t| precision: 0.56\n",
      "epoch: 1\t| Loss: 2.1474003863334654\t| precision: 0.5445\n",
      "epoch: 1\t| Loss: 2.1441599398851396\t| precision: 0.5664772727272728\n",
      "epoch: 1\t| Loss: 2.149852334856987\t| precision: 0.553130384836301\n",
      "epoch: 1\t| Loss: 2.141945441365242\t| precision: 0.5733137829912024\n",
      "mid epoch: 2663852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.142908593416214\t| precision: 0.5565832426550599\n",
      "epoch: 1\t| Loss: 2.1360302662849424\t| precision: 0.5705165409170052\n",
      "epoch: 1\t| Loss: 2.1499295377731324\t| precision: 0.5613480779357557\n",
      "epoch: 1\t| Loss: 2.1403176748752593\t| precision: 0.567820392890552\n",
      "epoch: 1\t| Loss: 2.1430414658784867\t| precision: 0.5586249232658073\n",
      "epoch: 1\t| Loss: 2.146752659678459\t| precision: 0.5782778864970646\n",
      "epoch: 1\t| Loss: 2.1610587674379347\t| precision: 0.5712682379349046\n",
      "epoch: 1\t| Loss: 2.1489573520421983\t| precision: 0.5540785498489426\n",
      "epoch: 1\t| Loss: 2.1479640823602675\t| precision: 0.5464121132323897\n",
      "epoch: 1\t| Loss: 2.1430781477689744\t| precision: 0.5396463205932687\n",
      "epoch: 1\t| Loss: 2.150264852643013\t| precision: 0.5290102389078498\n",
      "epoch: 1\t| Loss: 2.1401044791936874\t| precision: 0.5600717274357442\n",
      "epoch: 1\t| Loss: 2.142697192430496\t| precision: 0.5856885147324113\n",
      "epoch: 1\t| Loss: 2.139062979221344\t| precision: 0.5415104445634709\n",
      "epoch: 1\t| Loss: 2.135663794875145\t| precision: 0.5425848719475879\n",
      "mid epoch: 3107852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.147392824292183\t| precision: 0.5576012624934246\n",
      "epoch: 1\t| Loss: 2.14480406999588\t| precision: 0.5456090651558073\n",
      "epoch: 1\t| Loss: 2.145988331437111\t| precision: 0.5705705705705706\n",
      "epoch: 1\t| Loss: 2.136264318227768\t| precision: 0.5672773215413771\n",
      "epoch: 1\t| Loss: 2.143336281776428\t| precision: 0.5475578406169666\n",
      "epoch: 1\t| Loss: 2.138773592710495\t| precision: 0.570976253298153\n",
      "epoch: 1\t| Loss: 2.130593339204788\t| precision: 0.5504385964912281\n",
      "epoch: 1\t| Loss: 2.1513065004348757\t| precision: 0.5481268011527377\n",
      "epoch: 1\t| Loss: 2.1420910227298737\t| precision: 0.5700888450148075\n",
      "epoch: 1\t| Loss: 2.1364885360002517\t| precision: 0.561218836565097\n",
      "epoch: 1\t| Loss: 2.1421101105213167\t| precision: 0.5661458333333333\n",
      "epoch: 1\t| Loss: 2.1493421310186385\t| precision: 0.5287421741605008\n",
      "epoch: 1\t| Loss: 2.133243633508682\t| precision: 0.5529536987759447\n",
      "epoch: 1\t| Loss: 2.1370976966619493\t| precision: 0.564650059311981\n",
      "epoch: 1\t| Loss: 2.1387031078338623\t| precision: 0.5667206915180983\n",
      "mid epoch: 3551852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1397334814071653\t| precision: 0.5764002175095161\n",
      "epoch: 1\t| Loss: 2.144629997611046\t| precision: 0.5621621621621622\n",
      "epoch: 1\t| Loss: 2.136819039583206\t| precision: 0.5712742980561555\n",
      "epoch: 1\t| Loss: 2.1417979764938355\t| precision: 0.5892857142857143\n",
      "epoch: 1\t| Loss: 2.1487960624694824\t| precision: 0.5231481481481481\n",
      "epoch: 1\t| Loss: 2.138605568408966\t| precision: 0.549671977507029\n",
      "epoch: 1\t| Loss: 2.1471593874692916\t| precision: 0.5662909286097692\n",
      "epoch: 1\t| Loss: 2.1418653988838194\t| precision: 0.5481481481481482\n",
      "epoch: 1\t| Loss: 2.1541848361492155\t| precision: 0.5610211706102117\n",
      "epoch: 1\t| Loss: 2.1428041434288025\t| precision: 0.5326340326340326\n",
      "epoch: 1\t| Loss: 2.149473272562027\t| precision: 0.545502367175171\n",
      "epoch: 1\t| Loss: 2.1470708864927293\t| precision: 0.5645756457564576\n",
      "epoch: 1\t| Loss: 2.137746297121048\t| precision: 0.57451078775715\n",
      "epoch: 1\t| Loss: 2.144786211848259\t| precision: 0.5562403697996918\n",
      "epoch: 1\t| Loss: 2.159589581489563\t| precision: 0.5585987261146497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid epoch: 3995852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.148022732138634\t| precision: 0.5529953917050692\n",
      "epoch: 1\t| Loss: 2.1444052982330324\t| precision: 0.5548481880509305\n",
      "epoch: 1\t| Loss: 2.1541629779338836\t| precision: 0.5542771385692846\n",
      "epoch: 1\t| Loss: 2.146866183876991\t| precision: 0.5662715517241379\n",
      "epoch: 1\t| Loss: 2.1445914304256437\t| precision: 0.5640473627556513\n",
      "epoch: 1\t| Loss: 2.152029165029526\t| precision: 0.5614127569847127\n",
      "epoch: 1\t| Loss: 2.1463613086938857\t| precision: 0.5794621026894865\n",
      "epoch: 1\t| Loss: 2.139496122598648\t| precision: 0.5616355983162958\n",
      "epoch: 1\t| Loss: 2.1469060361385344\t| precision: 0.5636942675159236\n",
      "epoch: 1\t| Loss: 2.152548822760582\t| precision: 0.5558282208588957\n",
      "epoch: 1\t| Loss: 2.148968247771263\t| precision: 0.5612403100775194\n",
      "epoch: 1\t| Loss: 2.146366021633148\t| precision: 0.5382059800664452\n",
      "epoch: 1\t| Loss: 2.147557553052902\t| precision: 0.5646470955652717\n",
      "epoch: 1\t| Loss: 2.147819801568985\t| precision: 0.5770263016639828\n",
      "epoch: 1\t| Loss: 2.139211239218712\t| precision: 0.5699937225360954\n",
      "mid epoch: 4439852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1465349769592286\t| precision: 0.5540242557883132\n",
      "epoch: 1\t| Loss: 2.1499494981765745\t| precision: 0.5516372795969773\n",
      "epoch: 1\t| Loss: 2.147911280989647\t| precision: 0.5796407185628742\n",
      "epoch: 1\t| Loss: 2.1468231105804443\t| precision: 0.5844919786096257\n",
      "epoch: 1\t| Loss: 2.1501567709445952\t| precision: 0.557630392788152\n",
      "epoch: 1\t| Loss: 2.1434763294458388\t| precision: 0.5606523955147809\n",
      "epoch: 1\t| Loss: 2.1466726416349413\t| precision: 0.572744014732965\n",
      "epoch: 1\t| Loss: 2.1363568043708803\t| precision: 0.5525231719876416\n",
      "epoch: 1\t| Loss: 2.1543067491054533\t| precision: 0.5561594202898551\n",
      "epoch: 1\t| Loss: 2.1432812309265135\t| precision: 0.5578947368421052\n",
      "epoch: 1\t| Loss: 2.156570167541504\t| precision: 0.5628672150411281\n",
      "epoch: 1\t| Loss: 2.137914229631424\t| precision: 0.5753344968004653\n",
      "epoch: 1\t| Loss: 2.141255162358284\t| precision: 0.553907586993725\n",
      "epoch: 1\t| Loss: 2.1406179589033125\t| precision: 0.5714285714285714\n",
      "epoch: 1\t| Loss: 2.1408029836416245\t| precision: 0.5478841870824054\n",
      "mid epoch: 4883852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.141348531842232\t| precision: 0.5860058309037901\n",
      "epoch: 1\t| Loss: 2.143395326733589\t| precision: 0.537321063394683\n",
      "epoch: 1\t| Loss: 2.1435880780220034\t| precision: 0.5642775881683731\n",
      "epoch: 1\t| Loss: 2.143630282878876\t| precision: 0.5612698412698413\n",
      "epoch: 1\t| Loss: 2.1438799196481706\t| precision: 0.5463917525773195\n",
      "epoch: 1\t| Loss: 2.1410888487100603\t| precision: 0.5630829651448208\n",
      "epoch: 1\t| Loss: 2.135210864543915\t| precision: 0.5610040863981319\n",
      "epoch: 1\t| Loss: 2.1556006973981856\t| precision: 0.5242265032107414\n",
      "epoch: 1\t| Loss: 2.1425271612405776\t| precision: 0.5666986564299424\n",
      "epoch: 1\t| Loss: 2.1399865978956223\t| precision: 0.5481064483111566\n",
      "epoch: 1\t| Loss: 2.1436262166500093\t| precision: 0.5506673257538309\n",
      "epoch: 1\t| Loss: 2.1403329730033875\t| precision: 0.5550161812297735\n",
      "epoch: 1\t| Loss: 2.148615624308586\t| precision: 0.5451595457003786\n",
      "epoch: 1\t| Loss: 2.129165846705437\t| precision: 0.5775370581527937\n",
      "epoch: 1\t| Loss: 2.1481114375591277\t| precision: 0.5408486383787207\n",
      "mid epoch: 5327852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.138796455860138\t| precision: 0.5671566142929549\n",
      "epoch: 1\t| Loss: 2.143508740067482\t| precision: 0.546971864568431\n",
      "epoch: 1\t| Loss: 2.1450759583711623\t| precision: 0.5528717122920022\n",
      "epoch: 1\t| Loss: 2.149766266345978\t| precision: 0.5615819209039548\n",
      "epoch: 1\t| Loss: 2.1410960084199906\t| precision: 0.5392670157068062\n",
      "epoch: 1\t| Loss: 2.1425519996881484\t| precision: 0.5686375321336761\n",
      "epoch: 1\t| Loss: 2.1413720947504045\t| precision: 0.5749867514573397\n",
      "epoch: 1\t| Loss: 2.1361160212755204\t| precision: 0.5659197498697238\n",
      "epoch: 1\t| Loss: 2.1379188668727873\t| precision: 0.5436529191215854\n",
      "epoch: 1\t| Loss: 2.145540692806244\t| precision: 0.5351227869788692\n",
      "epoch: 1\t| Loss: 2.1434963023662568\t| precision: 0.5732217573221757\n",
      "epoch: 1\t| Loss: 2.1409666764736177\t| precision: 0.5542560103963613\n",
      "epoch: 1\t| Loss: 2.1425375086069107\t| precision: 0.557001239157373\n",
      "epoch: 1\t| Loss: 2.143853581547737\t| precision: 0.56223942823109\n",
      "epoch: 1\t| Loss: 2.146300498843193\t| precision: 0.541780447842709\n",
      "mid epoch: 5771852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1470113015174865\t| precision: 0.5782092772384034\n",
      "epoch: 1\t| Loss: 2.1414573842287066\t| precision: 0.563856638061585\n",
      "epoch: 1\t| Loss: 2.147000157237053\t| precision: 0.5650580329871716\n",
      "epoch: 1\t| Loss: 2.147295756936073\t| precision: 0.5545243619489559\n",
      "epoch: 1\t| Loss: 2.140862739086151\t| precision: 0.5481150793650794\n",
      "epoch: 1\t| Loss: 2.146278078556061\t| precision: 0.5443243243243243\n",
      "epoch: 1\t| Loss: 2.1256936514377593\t| precision: 0.5254437869822485\n",
      "epoch: 1\t| Loss: 2.1368951916694643\t| precision: 0.5729230769230769\n",
      "epoch: 1\t| Loss: 2.143292511105537\t| precision: 0.546242774566474\n",
      "epoch: 1\t| Loss: 2.1371182292699813\t| precision: 0.5951417004048583\n",
      "epoch: 1\t| Loss: 2.144388297200203\t| precision: 0.5629213483146067\n",
      "epoch: 1\t| Loss: 2.1325648587942125\t| precision: 0.5613254203758655\n",
      "epoch: 1\t| Loss: 2.153643829226494\t| precision: 0.571875\n",
      "epoch: 1\t| Loss: 2.142840239405632\t| precision: 0.5698924731182796\n",
      "epoch: 1\t| Loss: 2.1353720599412918\t| precision: 0.5789817232375979\n",
      "mid epoch: 6215852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.149607231616974\t| precision: 0.5503355704697986\n",
      "epoch: 1\t| Loss: 2.1503338104486467\t| precision: 0.5304254173397954\n",
      "epoch: 1\t| Loss: 2.1491981500387194\t| precision: 0.5798421372191864\n",
      "epoch: 1\t| Loss: 2.1484489172697065\t| precision: 0.5646067415730337\n",
      "epoch: 1\t| Loss: 2.1405628550052644\t| precision: 0.5424764019988895\n",
      "epoch: 1\t| Loss: 2.147590863108635\t| precision: 0.5557644110275689\n",
      "epoch: 1\t| Loss: 2.1451051676273347\t| precision: 0.5361723961005644\n",
      "epoch: 1\t| Loss: 2.133987187743187\t| precision: 0.5670731707317073\n",
      "epoch: 1\t| Loss: 2.142734034061432\t| precision: 0.5763779527559055\n",
      "epoch: 1\t| Loss: 2.147803001999855\t| precision: 0.5391304347826087\n",
      "epoch: 1\t| Loss: 2.1402540212869643\t| precision: 0.5594965675057209\n",
      "epoch: 1\t| Loss: 2.1471923476457597\t| precision: 0.555622369212267\n",
      "epoch: 1\t| Loss: 2.135026096701622\t| precision: 0.5517068273092369\n",
      "epoch: 1\t| Loss: 2.143879310488701\t| precision: 0.5383333333333333\n",
      "epoch: 1\t| Loss: 2.151128234863281\t| precision: 0.5790094339622641\n",
      "mid epoch: 6659852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1422398293018343\t| precision: 0.5494565217391304\n",
      "epoch: 1\t| Loss: 2.144098568558693\t| precision: 0.5684507042253522\n",
      "epoch: 1\t| Loss: 2.14584202170372\t| precision: 0.5632311977715877\n",
      "epoch: 1\t| Loss: 2.1448316156864164\t| precision: 0.5655512890839276\n",
      "epoch: 1\t| Loss: 2.1417740494012834\t| precision: 0.5619442797866034\n",
      "epoch: 1\t| Loss: 2.147575019598007\t| precision: 0.5631220534311158\n",
      "epoch: 1\t| Loss: 2.1512376284599304\t| precision: 0.574085554866708\n",
      "epoch: 1\t| Loss: 2.1334069937467577\t| precision: 0.5658475110270952\n",
      "epoch: 1\t| Loss: 2.1435388588905333\t| precision: 0.5814713896457766\n",
      "epoch: 1\t| Loss: 2.1391548132896423\t| precision: 0.5669546436285097\n",
      "epoch: 1\t| Loss: 2.1386573177576067\t| precision: 0.5622727272727273\n",
      "epoch: 1\t| Loss: 2.1547974503040312\t| precision: 0.5456285896617741\n",
      "epoch: 1\t| Loss: 2.1480287611484528\t| precision: 0.5728368017524644\n",
      "epoch: 1\t| Loss: 2.15591776907444\t| precision: 0.5582245430809399\n",
      "epoch: 1\t| Loss: 2.1427641212940216\t| precision: 0.571849234393404\n",
      "mid epoch: 7103852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1374876320362093\t| precision: 0.5863013698630137\n",
      "epoch: 1\t| Loss: 2.145015239715576\t| precision: 0.5723231058234189\n",
      "epoch: 1\t| Loss: 2.1442225950956346\t| precision: 0.5634118967452301\n",
      "epoch: 1\t| Loss: 2.141651349067688\t| precision: 0.5467263570229435\n",
      "epoch: 1\t| Loss: 2.1290127259492873\t| precision: 0.5737905695039804\n",
      "epoch: 1\t| Loss: 2.146346701979637\t| precision: 0.5565662308129619\n",
      "epoch: 1\t| Loss: 2.1295944982767105\t| precision: 0.5577287066246057\n",
      "epoch: 1\t| Loss: 2.1491291975975035\t| precision: 0.5543822597676874\n",
      "epoch: 1\t| Loss: 2.1365976202487946\t| precision: 0.5571353343865192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.155067988038063\t| precision: 0.557504873294347\n",
      "epoch: 1\t| Loss: 2.1442330461740493\t| precision: 0.5478750640040962\n",
      "epoch: 1\t| Loss: 2.151380430459976\t| precision: 0.553415061295972\n",
      "epoch: 1\t| Loss: 2.147395026087761\t| precision: 0.5518763796909493\n",
      "epoch: 1\t| Loss: 2.1498368644714354\t| precision: 0.5626410835214447\n",
      "epoch: 1\t| Loss: 2.1570358872413635\t| precision: 0.5465774575398867\n",
      "mid epoch: 7547852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.149001089334488\t| precision: 0.5633017289459007\n",
      "epoch: 1\t| Loss: 2.147984331250191\t| precision: 0.5580722891566265\n",
      "epoch: 1\t| Loss: 2.146863262653351\t| precision: 0.5409309791332263\n",
      "epoch: 1\t| Loss: 2.1463212150335313\t| precision: 0.552190373174689\n",
      "epoch: 1\t| Loss: 2.1491418409347536\t| precision: 0.5583140877598153\n",
      "epoch: 1\t| Loss: 2.1462677282094957\t| precision: 0.5431309904153354\n",
      "epoch: 1\t| Loss: 2.1446660429239275\t| precision: 0.559462915601023\n",
      "epoch: 1\t| Loss: 2.137523409128189\t| precision: 0.5451729478575116\n",
      "epoch: 1\t| Loss: 2.150101264119148\t| precision: 0.5485714285714286\n",
      "epoch: 1\t| Loss: 2.141523515582085\t| precision: 0.5542682926829269\n",
      "epoch: 1\t| Loss: 2.1426581752300264\t| precision: 0.5457256461232605\n",
      "epoch: 1\t| Loss: 2.1368548953533173\t| precision: 0.5666497719209326\n",
      "epoch: 1\t| Loss: 2.1469148463010788\t| precision: 0.5560747663551402\n",
      "epoch: 1\t| Loss: 2.1480835598707197\t| precision: 0.554983922829582\n",
      "epoch: 1\t| Loss: 2.143224221467972\t| precision: 0.5712799167533819\n",
      "mid epoch: 7991852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1521641033887864\t| precision: 0.5515370705244123\n",
      "epoch: 1\t| Loss: 2.1473146629333497\t| precision: 0.5621242484969939\n",
      "epoch: 1\t| Loss: 2.1433790892362596\t| precision: 0.5667215815485996\n",
      "epoch: 1\t| Loss: 2.1434766495227815\t| precision: 0.5703009373458313\n",
      "epoch: 1\t| Loss: 2.1461205744743346\t| precision: 0.5591506572295247\n",
      "epoch: 1\t| Loss: 2.147224002480507\t| precision: 0.5558613098514034\n",
      "epoch: 1\t| Loss: 2.1383813685178756\t| precision: 0.5373054213633924\n",
      "epoch: 1\t| Loss: 2.1470059192180635\t| precision: 0.5646551724137931\n",
      "epoch: 1\t| Loss: 2.147055055499077\t| precision: 0.5492957746478874\n",
      "epoch: 1\t| Loss: 2.1354588252305984\t| precision: 0.5373808329152032\n",
      "epoch: 1\t| Loss: 2.147114684581757\t| precision: 0.5589419206440484\n",
      "epoch: 1\t| Loss: 2.1437627267837525\t| precision: 0.5725239616613419\n",
      "epoch: 1\t| Loss: 2.1439566510915755\t| precision: 0.5522478736330498\n",
      "epoch: 1\t| Loss: 2.1457465469837187\t| precision: 0.5592592592592592\n",
      "epoch: 1\t| Loss: 2.1463235265016554\t| precision: 0.5666456096020215\n",
      "mid epoch: 8435852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1431028670072556\t| precision: 0.5614669421487604\n",
      "epoch: 1\t| Loss: 2.1398987871408464\t| precision: 0.5540318156884256\n",
      "epoch: 1\t| Loss: 2.146724977493286\t| precision: 0.5503122831367107\n",
      "epoch: 1\t| Loss: 2.1402434146404268\t| precision: 0.5518394648829431\n",
      "epoch: 1\t| Loss: 2.147408652305603\t| precision: 0.5681024447031432\n",
      "epoch: 1\t| Loss: 2.1402999526262283\t| precision: 0.5775370581527937\n",
      "epoch: 1\t| Loss: 2.149824445843697\t| precision: 0.567479674796748\n",
      "epoch: 1\t| Loss: 2.141034277677536\t| precision: 0.5399898115129903\n",
      "epoch: 1\t| Loss: 2.1433037757873534\t| precision: 0.5636645962732919\n",
      "epoch: 1\t| Loss: 2.138763648271561\t| precision: 0.5698711595639246\n",
      "epoch: 1\t| Loss: 2.148337500691414\t| precision: 0.6085547634478289\n",
      "epoch: 1\t| Loss: 2.1479247188568116\t| precision: 0.5610753945061367\n",
      "epoch: 1\t| Loss: 2.13745261490345\t| precision: 0.5769010863350486\n",
      "epoch: 1\t| Loss: 2.1501119858026505\t| precision: 0.5762904140669314\n",
      "epoch: 1\t| Loss: 2.1397226083278658\t| precision: 0.5701275045537341\n",
      "mid epoch: 8879852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.15007838845253\t| precision: 0.5662232504337767\n",
      "epoch: 1\t| Loss: 2.1487789165973665\t| precision: 0.5661375661375662\n",
      "epoch: 1\t| Loss: 2.1412315768003465\t| precision: 0.5472936030617824\n",
      "epoch: 1\t| Loss: 2.1430917048454283\t| precision: 0.5506186726659168\n",
      "epoch: 1\t| Loss: 2.1435631424188615\t| precision: 0.5702398433675967\n",
      "epoch: 1\t| Loss: 2.1497146797180178\t| precision: 0.5776255707762558\n",
      "epoch: 1\t| Loss: 2.1394143998622894\t| precision: 0.5681024447031432\n",
      "epoch: 1\t| Loss: 2.146066988706589\t| precision: 0.5572640509013785\n",
      "epoch: 1\t| Loss: 2.141722285747528\t| precision: 0.5570726379027854\n",
      "epoch: 1\t| Loss: 2.1393204724788664\t| precision: 0.5563712637650761\n",
      "epoch: 1\t| Loss: 2.137185506820679\t| precision: 0.5618305744888024\n",
      "epoch: 1\t| Loss: 2.140899515748024\t| precision: 0.5586166471277842\n",
      "epoch: 1\t| Loss: 2.131984112262726\t| precision: 0.5777414075286416\n",
      "epoch: 1\t| Loss: 2.1425779753923417\t| precision: 0.5728155339805825\n",
      "epoch: 1\t| Loss: 2.145820654630661\t| precision: 0.5810553083280356\n",
      "mid epoch: 9323852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1470069551467894\t| precision: 0.5530565167243368\n",
      "epoch: 1\t| Loss: 2.1430704760551453\t| precision: 0.5706554419723392\n",
      "epoch: 1\t| Loss: 2.1492956095933913\t| precision: 0.5535603715170279\n",
      "epoch: 1\t| Loss: 2.1383127307891847\t| precision: 0.5686274509803921\n",
      "epoch: 1\t| Loss: 2.1480999225378037\t| precision: 0.542915185373286\n",
      "epoch: 1\t| Loss: 2.1423513823747635\t| precision: 0.5648204503956178\n",
      "epoch: 1\t| Loss: 2.1479133129119874\t| precision: 0.5564941921858501\n",
      "epoch: 1\t| Loss: 2.1419171506166457\t| precision: 0.5649141630901288\n",
      "epoch: 1\t| Loss: 2.142725818157196\t| precision: 0.572928821470245\n",
      "epoch: 1\t| Loss: 2.14258975148201\t| precision: 0.5498687664041995\n",
      "epoch: 1\t| Loss: 2.1449097204208374\t| precision: 0.5789813023855577\n",
      "epoch: 1\t| Loss: 2.139539162516594\t| precision: 0.5524673851389676\n",
      "epoch: 1\t| Loss: 2.15206816136837\t| precision: 0.5543418896777718\n",
      "epoch: 1\t| Loss: 2.1368345719575883\t| precision: 0.5637949836423118\n",
      "epoch: 1\t| Loss: 2.1302758687734604\t| precision: 0.581986143187067\n",
      "mid epoch: 9767852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1391571420431137\t| precision: 0.5753348214285714\n",
      "epoch: 1\t| Loss: 2.1340112328529357\t| precision: 0.5662055335968379\n",
      "epoch: 1\t| Loss: 2.1500310468673707\t| precision: 0.5792988313856428\n",
      "epoch: 1\t| Loss: 2.1387582194805144\t| precision: 0.5865191146881288\n",
      "epoch: 1\t| Loss: 2.1342780673503876\t| precision: 0.5603543743078627\n",
      "epoch: 1\t| Loss: 2.1434283274412156\t| precision: 0.6025641025641025\n",
      "epoch: 1\t| Loss: 2.1486841636896132\t| precision: 0.5691768826619965\n",
      "epoch: 1\t| Loss: 2.14716483771801\t| precision: 0.5745325922182921\n",
      "epoch: 1\t| Loss: 2.13860769867897\t| precision: 0.5455991516436903\n",
      "epoch: 1\t| Loss: 2.1403527241945266\t| precision: 0.5332936979785969\n",
      "epoch: 1\t| Loss: 2.140640232563019\t| precision: 0.5455104551045511\n",
      "epoch: 1\t| Loss: 2.135150496959686\t| precision: 0.5613818630475016\n",
      "epoch: 1\t| Loss: 2.1410519754886628\t| precision: 0.5609243697478992\n",
      "epoch: 1\t| Loss: 2.1385963463783266\t| precision: 0.548169014084507\n",
      "epoch: 1\t| Loss: 2.1453911346197128\t| precision: 0.5363636363636364\n",
      "mid epoch: 10211852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1367651802301406\t| precision: 0.5824800910125142\n",
      "epoch: 1\t| Loss: 2.141829181313515\t| precision: 0.5421538461538462\n",
      "epoch: 1\t| Loss: 2.1418890637159347\t| precision: 0.554942298043151\n",
      "epoch: 1\t| Loss: 2.141696013212204\t| precision: 0.5474701534963047\n",
      "epoch: 1\t| Loss: 2.1516856956481933\t| precision: 0.5554106910039114\n",
      "epoch: 1\t| Loss: 2.1444185298681258\t| precision: 0.5550488599348534\n",
      "epoch: 1\t| Loss: 2.1343706846237183\t| precision: 0.5889008620689655\n",
      "epoch: 1\t| Loss: 2.1465565478801727\t| precision: 0.5361653272101033\n",
      "epoch: 1\t| Loss: 2.147135893702507\t| precision: 0.5686380900705371\n",
      "epoch: 1\t| Loss: 2.145691088438034\t| precision: 0.5596473029045643\n",
      "epoch: 1\t| Loss: 2.1383600354194643\t| precision: 0.5650969529085873\n",
      "epoch: 1\t| Loss: 2.1428139281272887\t| precision: 0.5789779326364692\n",
      "epoch: 1\t| Loss: 2.1495187133550644\t| precision: 0.5569321533923304\n",
      "epoch: 1\t| Loss: 2.1462616223096846\t| precision: 0.5301399354144241\n",
      "epoch: 1\t| Loss: 2.1515639328956606\t| precision: 0.5617529880478087\n",
      "mid epoch: 10655852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1437100768089294\t| precision: 0.5421552205471803\n",
      "epoch: 1\t| Loss: 2.1457705980539323\t| precision: 0.5691839837810441\n",
      "epoch: 1\t| Loss: 2.1367806828022005\t| precision: 0.5816917728852838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.144517434835434\t| precision: 0.5896510228640193\n",
      "epoch: 1\t| Loss: 2.139708878993988\t| precision: 0.5538116591928252\n",
      "epoch: 1\t| Loss: 2.1356136023998262\t| precision: 0.5516912815626489\n",
      "epoch: 1\t| Loss: 2.138756583929062\t| precision: 0.5483870967741935\n",
      "epoch: 1\t| Loss: 2.1422621524333954\t| precision: 0.5476870359794404\n",
      "epoch: 1\t| Loss: 2.1349367463588713\t| precision: 0.5767366720516963\n",
      "epoch: 1\t| Loss: 2.1490291923284532\t| precision: 0.5774035850081477\n",
      "epoch: 1\t| Loss: 2.1492111998796464\t| precision: 0.5444512568976089\n",
      "epoch: 1\t| Loss: 2.158200501203537\t| precision: 0.5684029765311963\n",
      "epoch: 1\t| Loss: 2.1355358082056046\t| precision: 0.5747126436781609\n",
      "epoch: 1\t| Loss: 2.144391570687294\t| precision: 0.5624270711785297\n",
      "epoch: 1\t| Loss: 2.1394744449853897\t| precision: 0.5806794055201698\n",
      "mid epoch: 11099852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.139814168214798\t| precision: 0.5642370214997378\n",
      "epoch: 1\t| Loss: 2.148768364191055\t| precision: 0.5538284398388025\n",
      "epoch: 1\t| Loss: 2.1296799212694166\t| precision: 0.5959302325581395\n",
      "epoch: 1\t| Loss: 2.143622782230377\t| precision: 0.5592255125284739\n",
      "epoch: 1\t| Loss: 2.142550397515297\t| precision: 0.5629067245119306\n",
      "epoch: 1\t| Loss: 2.146741455793381\t| precision: 0.5597014925373134\n",
      "epoch: 1\t| Loss: 2.138470995426178\t| precision: 0.5560046189376443\n",
      "epoch: 1\t| Loss: 2.140420096516609\t| precision: 0.5666666666666667\n",
      "epoch: 1\t| Loss: 2.146376906633377\t| precision: 0.5588558855885588\n",
      "epoch: 1\t| Loss: 2.1352475833892823\t| precision: 0.556020942408377\n",
      "epoch: 1\t| Loss: 2.142125346660614\t| precision: 0.5563790365272631\n",
      "epoch: 1\t| Loss: 2.137161463499069\t| precision: 0.5435506901475488\n",
      "epoch: 1\t| Loss: 2.142779186964035\t| precision: 0.5618294748729531\n",
      "epoch: 1\t| Loss: 2.1308127349615096\t| precision: 0.5407110091743119\n",
      "epoch: 1\t| Loss: 2.13297794342041\t| precision: 0.5486725663716814\n",
      "mid epoch: 11543852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.15250219643116\t| precision: 0.5401858939311099\n",
      "epoch: 1\t| Loss: 2.142061148285866\t| precision: 0.5545706371191136\n",
      "epoch: 1\t| Loss: 2.14369848549366\t| precision: 0.5489159175039662\n",
      "epoch: 1\t| Loss: 2.138221098780632\t| precision: 0.5748281332628239\n",
      "epoch: 1\t| Loss: 2.138811920285225\t| precision: 0.563508064516129\n",
      "epoch: 1\t| Loss: 2.138067680001259\t| precision: 0.5624332977588047\n",
      "epoch: 1\t| Loss: 2.142196379899979\t| precision: 0.574122577265584\n",
      "epoch: 1\t| Loss: 2.1332054817676545\t| precision: 0.560126582278481\n",
      "epoch: 1\t| Loss: 2.136768006682396\t| precision: 0.5791464073473798\n",
      "epoch: 1\t| Loss: 2.1348426949977877\t| precision: 0.5708874458874459\n",
      "epoch: 1\t| Loss: 2.1307494753599165\t| precision: 0.563626723223754\n",
      "epoch: 1\t| Loss: 2.1392601025104523\t| precision: 0.5639606396063961\n",
      "epoch: 1\t| Loss: 2.142529333233833\t| precision: 0.569451309935739\n",
      "epoch: 1\t| Loss: 2.1476081997156142\t| precision: 0.5671247357293869\n",
      "epoch: 1\t| Loss: 2.141591650247574\t| precision: 0.5435729847494554\n",
      "mid epoch: 11987852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.141183827519417\t| precision: 0.5541845493562232\n",
      "epoch: 1\t| Loss: 2.1431616044044492\t| precision: 0.5332947307469601\n",
      "epoch: 1\t| Loss: 2.151881957054138\t| precision: 0.5631220534311158\n",
      "epoch: 1\t| Loss: 2.154855890870094\t| precision: 0.5605247465712582\n",
      "epoch: 1\t| Loss: 2.145822969675064\t| precision: 0.5714285714285714\n",
      "epoch: 1\t| Loss: 2.1327280247211458\t| precision: 0.5558739255014327\n",
      "epoch: 1\t| Loss: 2.1446629685163496\t| precision: 0.5649202733485194\n",
      "epoch: 1\t| Loss: 2.1489208644628524\t| precision: 0.5479723046488625\n",
      "epoch: 1\t| Loss: 2.147696046233177\t| precision: 0.5392265193370166\n",
      "epoch: 1\t| Loss: 2.1469860595464705\t| precision: 0.5916813122437024\n",
      "epoch: 1\t| Loss: 2.144077515602112\t| precision: 0.5401737242128122\n",
      "epoch: 1\t| Loss: 2.133729014992714\t| precision: 0.5631868131868132\n",
      "epoch: 1\t| Loss: 2.1432864654064177\t| precision: 0.5412186379928315\n",
      "epoch: 1\t| Loss: 2.1452606791257858\t| precision: 0.5385044642857143\n",
      "epoch: 1\t| Loss: 2.1440280520915986\t| precision: 0.5838440111420613\n",
      "mid epoch: 12431852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1317708122730257\t| precision: 0.5829528158295282\n",
      "epoch: 1\t| Loss: 2.1330056965351103\t| precision: 0.5359435173299102\n",
      "epoch: 1\t| Loss: 2.1527238857746123\t| precision: 0.5682326621923938\n",
      "epoch: 1\t| Loss: 2.152305104136467\t| precision: 0.5547404063205418\n",
      "epoch: 1\t| Loss: 2.1468494015932085\t| precision: 0.5512887953708574\n",
      "epoch: 1\t| Loss: 2.1510654920339585\t| precision: 0.5336512983571807\n",
      "epoch: 1\t| Loss: 2.1419959419965746\t| precision: 0.5359550561797752\n",
      "epoch: 1\t| Loss: 2.1486666810512545\t| precision: 0.5545774647887324\n",
      "epoch: 1\t| Loss: 2.1259877187013627\t| precision: 0.5520640788662969\n",
      "epoch: 1\t| Loss: 2.134716649055481\t| precision: 0.5515976951283394\n",
      "epoch: 1\t| Loss: 2.137830862402916\t| precision: 0.5549539794260964\n",
      "epoch: 1\t| Loss: 2.1354190665483475\t| precision: 0.5572305974316024\n",
      "epoch: 1\t| Loss: 2.1392788445949553\t| precision: 0.5526905829596412\n",
      "epoch: 1\t| Loss: 2.138832894563675\t| precision: 0.5756613756613757\n",
      "epoch: 1\t| Loss: 2.1395241767168045\t| precision: 0.5527099463966647\n",
      "mid epoch: 12875852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1437450999021532\t| precision: 0.5483119906868452\n",
      "epoch: 1\t| Loss: 2.1330107110738754\t| precision: 0.5824315722469765\n",
      "epoch: 1\t| Loss: 2.1469249588251116\t| precision: 0.5762614678899083\n",
      "epoch: 1\t| Loss: 2.143301270008087\t| precision: 0.5447263017356475\n",
      "epoch: 1\t| Loss: 2.139409090280533\t| precision: 0.5668276972624798\n",
      "epoch: 1\t| Loss: 2.142710837125778\t| precision: 0.5508948545861297\n",
      "epoch: 1\t| Loss: 2.142319849729538\t| precision: 0.5580608793686584\n",
      "epoch: 1\t| Loss: 2.14510125875473\t| precision: 0.5423469387755102\n",
      "epoch: 1\t| Loss: 2.1409329080581667\t| precision: 0.5642331635540464\n",
      "epoch: 1\t| Loss: 2.1370872139930723\t| precision: 0.5547169811320755\n",
      "epoch: 1\t| Loss: 2.1470175898075103\t| precision: 0.5467581047381546\n",
      "epoch: 1\t| Loss: 2.141948053240776\t| precision: 0.5693069306930693\n",
      "epoch: 1\t| Loss: 2.144688206911087\t| precision: 0.5536992840095465\n",
      "epoch: 1\t| Loss: 2.1372989875078203\t| precision: 0.5560141509433962\n",
      "epoch: 1\t| Loss: 2.148864305615425\t| precision: 0.569971671388102\n",
      "mid epoch: 13319852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.126460391879082\t| precision: 0.5581854043392505\n",
      "epoch: 1\t| Loss: 2.1460563147068026\t| precision: 0.5563790365272631\n",
      "epoch: 1\t| Loss: 2.138660889863968\t| precision: 0.5561993047508691\n",
      "epoch: 1\t| Loss: 2.1464389997720716\t| precision: 0.5464426877470355\n",
      "epoch: 1\t| Loss: 2.150459823012352\t| precision: 0.559768299104792\n",
      "epoch: 1\t| Loss: 2.134153568148613\t| precision: 0.563777994157741\n",
      "epoch: 1\t| Loss: 2.149897127747536\t| precision: 0.5613927916921198\n",
      "epoch: 1\t| Loss: 2.1337712007761\t| precision: 0.5657894736842105\n",
      "epoch: 1\t| Loss: 2.142969211935997\t| precision: 0.5577557755775577\n",
      "epoch: 1\t| Loss: 2.1401097565889358\t| precision: 0.5588785046728972\n",
      "epoch: 1\t| Loss: 2.139941540360451\t| precision: 0.5522819179664934\n",
      "epoch: 1\t| Loss: 2.133903053402901\t| precision: 0.5391868002357101\n",
      "epoch: 1\t| Loss: 2.151372982263565\t| precision: 0.5565610859728507\n",
      "epoch: 1\t| Loss: 2.144649752378464\t| precision: 0.5649819494584838\n",
      "epoch: 1\t| Loss: 2.147826189994812\t| precision: 0.5782800441014333\n",
      "mid epoch: 13763852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.149342403411865\t| precision: 0.5702230259192285\n",
      "epoch: 1\t| Loss: 2.1364981812238693\t| precision: 0.5569550930996714\n",
      "epoch: 1\t| Loss: 2.1361230677366256\t| precision: 0.5924300603400987\n",
      "epoch: 1\t| Loss: 2.1445949721336364\t| precision: 0.5617214043035108\n",
      "epoch: 1\t| Loss: 2.141545658707619\t| precision: 0.5477941176470589\n",
      "epoch: 1\t| Loss: 2.140197917222977\t| precision: 0.5746031746031746\n",
      "epoch: 1\t| Loss: 2.1433862334489824\t| precision: 0.5626756604834177\n",
      "epoch: 1\t| Loss: 2.1371976917982103\t| precision: 0.5628616517622304\n",
      "epoch: 1\t| Loss: 2.1331061685085295\t| precision: 0.5637769188293761\n",
      "epoch: 1\t| Loss: 2.1357507407665253\t| precision: 0.5737179487179487\n",
      "epoch: 1\t| Loss: 2.1431763511896134\t| precision: 0.5511440107671601\n",
      "epoch: 1\t| Loss: 2.14981147646904\t| precision: 0.5467980295566502\n",
      "epoch: 1\t| Loss: 2.1328821736574173\t| precision: 0.5462478184991274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t| Loss: 2.1357975995540617\t| precision: 0.5955196017423771\n",
      "epoch: 1\t| Loss: 2.150957292318344\t| precision: 0.575512405609493\n",
      "mid epoch: 14207852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.149132404923439\t| precision: 0.5431971029487843\n",
      "epoch: 1\t| Loss: 2.1457272297143937\t| precision: 0.5588235294117647\n",
      "epoch: 1\t| Loss: 2.131889607310295\t| precision: 0.5976203353163873\n",
      "epoch: 1\t| Loss: 2.1405892926454544\t| precision: 0.5629301868239921\n",
      "epoch: 1\t| Loss: 2.1446361327171326\t| precision: 0.5499103407053197\n",
      "epoch: 1\t| Loss: 2.131380224227905\t| precision: 0.5640449438202247\n",
      "epoch: 1\t| Loss: 2.138092311024666\t| precision: 0.5585241730279898\n",
      "epoch: 1\t| Loss: 2.1410814934968947\t| precision: 0.5378749364514489\n",
      "epoch: 1\t| Loss: 2.1417870497703553\t| precision: 0.5742039935240151\n",
      "epoch: 1\t| Loss: 2.137913075089455\t| precision: 0.5422023476802683\n",
      "epoch: 1\t| Loss: 2.145343442559242\t| precision: 0.564663951120163\n",
      "epoch: 1\t| Loss: 2.148653311729431\t| precision: 0.6061365059486538\n",
      "epoch: 1\t| Loss: 2.1354775428771973\t| precision: 0.556968463886063\n",
      "epoch: 1\t| Loss: 2.1462119007110596\t| precision: 0.5743455497382199\n",
      "epoch: 1\t| Loss: 2.1416668838262556\t| precision: 0.5478312537136066\n",
      "mid epoch: 14651852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.147001510858536\t| precision: 0.5591836734693878\n",
      "epoch: 1\t| Loss: 2.133764367699623\t| precision: 0.559959859508279\n",
      "epoch: 1\t| Loss: 2.1437071746587755\t| precision: 0.5557422969187675\n",
      "epoch: 1\t| Loss: 2.1439816319942473\t| precision: 0.562575210589651\n",
      "epoch: 1\t| Loss: 2.1359965068101885\t| precision: 0.5814663951120163\n",
      "epoch: 1\t| Loss: 2.142888050675392\t| precision: 0.5799595141700404\n",
      "epoch: 1\t| Loss: 2.1345046305656434\t| precision: 0.5590762620837809\n",
      "epoch: 1\t| Loss: 2.1397111833095552\t| precision: 0.5711030082041932\n",
      "epoch: 1\t| Loss: 2.1372842794656752\t| precision: 0.590122199592668\n",
      "epoch: 1\t| Loss: 2.1396691077947616\t| precision: 0.5646687697160884\n",
      "epoch: 1\t| Loss: 2.151185681819916\t| precision: 0.556437889960295\n",
      "epoch: 1\t| Loss: 2.1391013914346697\t| precision: 0.5576424870466321\n",
      "epoch: 1\t| Loss: 2.1359278959035874\t| precision: 0.5671388101983003\n",
      "epoch: 1\t| Loss: 2.135758727192879\t| precision: 0.5672913117546848\n",
      "epoch: 1\t| Loss: 2.131416955590248\t| precision: 0.5675984027381632\n",
      "mid epoch: 15095852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1338684046268463\t| precision: 0.5526849037487336\n",
      "epoch: 1\t| Loss: 2.139855399131775\t| precision: 0.5699263932702419\n",
      "epoch: 1\t| Loss: 2.1425988614559173\t| precision: 0.5631578947368421\n",
      "epoch: 1\t| Loss: 2.146150418519974\t| precision: 0.5781420765027322\n",
      "epoch: 1\t| Loss: 2.14173036634922\t| precision: 0.562358276643991\n",
      "epoch: 1\t| Loss: 2.13064789891243\t| precision: 0.5664811379097093\n",
      "epoch: 1\t| Loss: 2.142928360104561\t| precision: 0.5350929814037193\n",
      "epoch: 1\t| Loss: 2.1312525260448454\t| precision: 0.5870841487279843\n",
      "epoch: 1\t| Loss: 2.1460369193553923\t| precision: 0.5603174603174603\n",
      "epoch: 1\t| Loss: 2.1416633248329164\t| precision: 0.5362678239305642\n",
      "epoch: 1\t| Loss: 2.154535448551178\t| precision: 0.5579710144927537\n",
      "epoch: 1\t| Loss: 2.1388859516382217\t| precision: 0.5476062399139322\n",
      "epoch: 1\t| Loss: 2.1395579886436464\t| precision: 0.5596285434995112\n",
      "epoch: 1\t| Loss: 2.1405091977119444\t| precision: 0.5698022768124625\n",
      "epoch: 1\t| Loss: 2.140841402411461\t| precision: 0.5468384074941453\n",
      "mid epoch: 15539852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.1514657491445544\t| precision: 0.5912252541466024\n",
      "epoch: 1\t| Loss: 2.136981430053711\t| precision: 0.5761357963055417\n",
      "epoch: 1\t| Loss: 2.1304210084676742\t| precision: 0.5746031746031746\n",
      "epoch: 1\t| Loss: 2.137514504790306\t| precision: 0.5708502024291497\n",
      "epoch: 1\t| Loss: 2.129708324074745\t| precision: 0.5587529976019184\n",
      "epoch: 1\t| Loss: 2.138573812842369\t| precision: 0.5337389380530974\n",
      "epoch: 1\t| Loss: 2.1402508133649825\t| precision: 0.5402644230769231\n",
      "epoch: 1\t| Loss: 2.1295620155334474\t| precision: 0.5687022900763359\n",
      "epoch: 1\t| Loss: 2.1527813589572906\t| precision: 0.5666460012399256\n",
      "epoch: 1\t| Loss: 2.145002864599228\t| precision: 0.5711974110032363\n",
      "epoch: 1\t| Loss: 2.1460037618875503\t| precision: 0.5329138431752178\n",
      "epoch: 1\t| Loss: 2.1388776397705076\t| precision: 0.5408770979967515\n",
      "epoch: 1\t| Loss: 2.139919602870941\t| precision: 0.5598771750255885\n",
      "epoch: 1\t| Loss: 2.133320385813713\t| precision: 0.5458966565349544\n",
      "epoch: 1\t| Loss: 2.1256341671943666\t| precision: 0.5551926845199217\n",
      "mid epoch: 15983852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Loss: 2.139016888141632\t| precision: 0.5529817644159685\n",
      "epoch: 1\t| Loss: 2.143264107108116\t| precision: 0.5535148828372387\n",
      "epoch: 1\t| Loss: 2.1430317115783692\t| precision: 0.584375\n",
      "epoch: 1\t| Loss: 2.1402793169021606\t| precision: 0.5484593837535015\n",
      "epoch: 1\t| Loss: 2.1392600286006926\t| precision: 0.5710698141637368\n",
      "epoch: 1\t| Loss: 2.133067236542702\t| precision: 0.5841584158415841\n",
      "epoch: 1\t| Loss: 2.1376664638519287\t| precision: 0.5404232230059686\n",
      "epoch: 1\t| Loss: 2.1469077324867247\t| precision: 0.564214463840399\n",
      "epoch: 1\t| Loss: 2.1528716790676117\t| precision: 0.5655512890839276\n",
      "epoch: 1\t| Loss: 2.131595693230629\t| precision: 0.575602255253716\n",
      "epoch: 1\t| Loss: 2.143234605193138\t| precision: 0.5429417571569596\n",
      "epoch: 1\t| Loss: 2.1420752197504043\t| precision: 0.5788436268068331\n",
      "epoch: 1\t| Loss: 2.1291779148578645\t| precision: 0.552757793764988\n",
      "epoch: 1\t| Loss: 2.1383312064409257\t| precision: 0.5692488262910798\n",
      "epoch: 1\t| Loss: 2.1357746052742006\t| precision: 0.5587871287128713\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "done epoch 1\n",
      "epoch: 2\t| Loss: 0.010601745843887329\t| precision: 0.5754550792718731\n",
      "epoch: 2\t| Loss: 2.14656207382679\t| precision: 0.5715736040609137\n",
      "epoch: 2\t| Loss: 2.1435973626375198\t| precision: 0.5829297820823245\n",
      "epoch: 2\t| Loss: 2.141457914113998\t| precision: 0.539517014270033\n",
      "epoch: 2\t| Loss: 2.1436452025175097\t| precision: 0.5564957690393231\n",
      "epoch: 2\t| Loss: 2.144192649126053\t| precision: 0.5497150094996833\n",
      "epoch: 2\t| Loss: 2.136929250359535\t| precision: 0.5455966649296509\n",
      "epoch: 2\t| Loss: 2.1337208944559096\t| precision: 0.5736750145602796\n",
      "epoch: 2\t| Loss: 2.137331165075302\t| precision: 0.5784256559766764\n",
      "epoch: 2\t| Loss: 2.1451279175281526\t| precision: 0.5510989010989011\n",
      "epoch: 2\t| Loss: 2.1404514533281325\t| precision: 0.5609037328094303\n",
      "epoch: 2\t| Loss: 2.1425102227926254\t| precision: 0.5700431034482759\n",
      "epoch: 2\t| Loss: 2.141356870532036\t| precision: 0.565342544617156\n",
      "epoch: 2\t| Loss: 2.140794042944908\t| precision: 0.5515082527034718\n",
      "epoch: 2\t| Loss: 2.130496044754982\t| precision: 0.5538818076477404\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.145356265902519\t| precision: 0.5595800524934383\n",
      "epoch: 2\t| Loss: 2.1296278089284897\t| precision: 0.5346784363177806\n",
      "epoch: 2\t| Loss: 2.1375776666402815\t| precision: 0.5628815628815629\n",
      "epoch: 2\t| Loss: 2.1409034770727158\t| precision: 0.5600933488914819\n",
      "epoch: 2\t| Loss: 2.131940313577652\t| precision: 0.5472560975609756\n",
      "epoch: 2\t| Loss: 2.1362827157974245\t| precision: 0.570153693604363\n",
      "epoch: 2\t| Loss: 2.1386299484968188\t| precision: 0.5749162278602202\n",
      "epoch: 2\t| Loss: 2.144612443447113\t| precision: 0.584\n",
      "epoch: 2\t| Loss: 2.142028631567955\t| precision: 0.5542467451952883\n",
      "epoch: 2\t| Loss: 2.1315599983930587\t| precision: 0.5716463414634146\n",
      "epoch: 2\t| Loss: 2.1445337170362473\t| precision: 0.5407990996060776\n",
      "epoch: 2\t| Loss: 2.137899631857872\t| precision: 0.5604128718882817\n",
      "epoch: 2\t| Loss: 2.1393449693918227\t| precision: 0.5849615156897573\n",
      "epoch: 2\t| Loss: 2.1400801199674606\t| precision: 0.5372972972972972\n",
      "epoch: 2\t| Loss: 2.137847076058388\t| precision: 0.5698602794411177\n",
      "mid epoch: 887852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.132429805994034\t| precision: 0.5938586326767091\n",
      "epoch: 2\t| Loss: 2.1326920729875565\t| precision: 0.5425685425685426\n",
      "epoch: 2\t| Loss: 2.1427789270877837\t| precision: 0.580074986609534\n",
      "epoch: 2\t| Loss: 2.13518290579319\t| precision: 0.5392330383480826\n",
      "epoch: 2\t| Loss: 2.1401030123233795\t| precision: 0.5421760391198044\n",
      "epoch: 2\t| Loss: 2.1383157068490983\t| precision: 0.55800792303339\n",
      "epoch: 2\t| Loss: 2.1552614688873293\t| precision: 0.5509609784507863\n",
      "epoch: 2\t| Loss: 2.1449771094322205\t| precision: 0.5789186237028946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.1366376775503158\t| precision: 0.5763626551538047\n",
      "epoch: 2\t| Loss: 2.1338119107484816\t| precision: 0.5473465140478668\n",
      "epoch: 2\t| Loss: 2.1387346071004867\t| precision: 0.5503919372900336\n",
      "epoch: 2\t| Loss: 2.1395482349395754\t| precision: 0.5765086206896551\n",
      "epoch: 2\t| Loss: 2.1456825596094133\t| precision: 0.5773252614199229\n",
      "epoch: 2\t| Loss: 2.1330554366111754\t| precision: 0.5418200946870069\n",
      "epoch: 2\t| Loss: 2.1428807574510573\t| precision: 0.5804469273743017\n",
      "mid epoch: 1331852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.140018571019173\t| precision: 0.5759065765212047\n",
      "epoch: 2\t| Loss: 2.12864233314991\t| precision: 0.5505350772889417\n",
      "epoch: 2\t| Loss: 2.138518540859222\t| precision: 0.5639864099660249\n",
      "epoch: 2\t| Loss: 2.1489356702566145\t| precision: 0.5755847119224187\n",
      "epoch: 2\t| Loss: 2.1373402637243273\t| precision: 0.5468933177022274\n",
      "epoch: 2\t| Loss: 2.1397656202316284\t| precision: 0.5388919977616117\n",
      "epoch: 2\t| Loss: 2.1324868279695512\t| precision: 0.5597587719298246\n",
      "epoch: 2\t| Loss: 2.13778205037117\t| precision: 0.5535714285714286\n",
      "epoch: 2\t| Loss: 2.1462943667173384\t| precision: 0.5741046831955923\n",
      "epoch: 2\t| Loss: 2.1508594089746476\t| precision: 0.5579761068165847\n",
      "epoch: 2\t| Loss: 2.1411955893039702\t| precision: 0.5903866248693835\n",
      "epoch: 2\t| Loss: 2.1471362376213072\t| precision: 0.5678798382437897\n",
      "epoch: 2\t| Loss: 2.143863433599472\t| precision: 0.5579171094580234\n",
      "epoch: 2\t| Loss: 2.134351066946983\t| precision: 0.5633663366336633\n",
      "epoch: 2\t| Loss: 2.136887458562851\t| precision: 0.5565529622980251\n",
      "mid epoch: 1775852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1405465161800383\t| precision: 0.5568676196395277\n",
      "epoch: 2\t| Loss: 2.1396799129247666\t| precision: 0.5626788780767029\n",
      "epoch: 2\t| Loss: 2.1432293456792832\t| precision: 0.5756410256410256\n",
      "epoch: 2\t| Loss: 2.136992185115814\t| precision: 0.5514511873350924\n",
      "epoch: 2\t| Loss: 2.1379201412200928\t| precision: 0.559244126659857\n",
      "epoch: 2\t| Loss: 2.1358461982011794\t| precision: 0.5665051157781368\n",
      "epoch: 2\t| Loss: 2.1354061383008958\t| precision: 0.5720596355604638\n",
      "epoch: 2\t| Loss: 2.146647558808327\t| precision: 0.5517441860465117\n",
      "epoch: 2\t| Loss: 2.139711847305298\t| precision: 0.5298050139275766\n",
      "epoch: 2\t| Loss: 2.1438955771923065\t| precision: 0.5320197044334976\n",
      "epoch: 2\t| Loss: 2.1447993659973146\t| precision: 0.5586289837642814\n",
      "epoch: 2\t| Loss: 2.140537326335907\t| precision: 0.5617704280155642\n",
      "epoch: 2\t| Loss: 2.1442532253265383\t| precision: 0.5597964376590331\n",
      "epoch: 2\t| Loss: 2.140394307374954\t| precision: 0.5591661557326794\n",
      "epoch: 2\t| Loss: 2.1436447960138323\t| precision: 0.554806070826307\n",
      "mid epoch: 2219852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.146040145158768\t| precision: 0.5534629404617254\n",
      "epoch: 2\t| Loss: 2.1394398540258406\t| precision: 0.5466439135381115\n",
      "epoch: 2\t| Loss: 2.1381413352489473\t| precision: 0.5565882996172772\n",
      "epoch: 2\t| Loss: 2.1529438847303393\t| precision: 0.5645628257093225\n",
      "epoch: 2\t| Loss: 2.1444965422153475\t| precision: 0.5482885085574573\n",
      "epoch: 2\t| Loss: 2.1377170318365097\t| precision: 0.5680445889456572\n",
      "epoch: 2\t| Loss: 2.135569960474968\t| precision: 0.5660998354360943\n",
      "epoch: 2\t| Loss: 2.1322284537553786\t| precision: 0.5681943677526229\n",
      "epoch: 2\t| Loss: 2.142871745824814\t| precision: 0.5589330024813896\n",
      "epoch: 2\t| Loss: 2.1370133674144745\t| precision: 0.574361820199778\n",
      "epoch: 2\t| Loss: 2.1451412361860274\t| precision: 0.5749704840613932\n",
      "epoch: 2\t| Loss: 2.1285803949832918\t| precision: 0.5652615540883698\n",
      "epoch: 2\t| Loss: 2.146468559503555\t| precision: 0.5626491646778043\n",
      "epoch: 2\t| Loss: 2.139379951953888\t| precision: 0.5461095100864554\n",
      "epoch: 2\t| Loss: 2.1363627415895463\t| precision: 0.5799115603284902\n",
      "mid epoch: 2663852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.136038935780525\t| precision: 0.5747986191024166\n",
      "epoch: 2\t| Loss: 2.142659847140312\t| precision: 0.5578465063001146\n",
      "epoch: 2\t| Loss: 2.1329949998855593\t| precision: 0.559906487434249\n",
      "epoch: 2\t| Loss: 2.140251495242119\t| precision: 0.5433070866141733\n",
      "epoch: 2\t| Loss: 2.142295303940773\t| precision: 0.5446735395189003\n",
      "epoch: 2\t| Loss: 2.132906676530838\t| precision: 0.5811921891058581\n",
      "epoch: 2\t| Loss: 2.146243534684181\t| precision: 0.5830969937606353\n",
      "epoch: 2\t| Loss: 2.13424887239933\t| precision: 0.5326850351161534\n",
      "epoch: 2\t| Loss: 2.1387687689065933\t| precision: 0.5720496894409938\n",
      "epoch: 2\t| Loss: 2.1434008955955504\t| precision: 0.5585434173669468\n",
      "epoch: 2\t| Loss: 2.136155071854591\t| precision: 0.5695290858725762\n",
      "epoch: 2\t| Loss: 2.12368203997612\t| precision: 0.537712895377129\n",
      "epoch: 2\t| Loss: 2.140976345539093\t| precision: 0.5480505216913784\n",
      "epoch: 2\t| Loss: 2.1408370423316954\t| precision: 0.5407956844234659\n",
      "epoch: 2\t| Loss: 2.1439009976387022\t| precision: 0.5410733844468785\n",
      "mid epoch: 3107852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1434655398130418\t| precision: 0.5693265421618563\n",
      "epoch: 2\t| Loss: 2.148358670473099\t| precision: 0.5604099244875944\n",
      "epoch: 2\t| Loss: 2.1354800248146057\t| precision: 0.5585193249863909\n",
      "epoch: 2\t| Loss: 2.13542921602726\t| precision: 0.5928338762214984\n",
      "epoch: 2\t| Loss: 2.1452472883462907\t| precision: 0.5880149812734082\n",
      "epoch: 2\t| Loss: 2.142441246509552\t| precision: 0.5513180033651149\n",
      "epoch: 2\t| Loss: 2.144462460875511\t| precision: 0.5386363636363637\n",
      "epoch: 2\t| Loss: 2.126728017926216\t| precision: 0.5652173913043478\n",
      "epoch: 2\t| Loss: 2.131622996330261\t| precision: 0.5841199333703498\n",
      "epoch: 2\t| Loss: 2.154422661662102\t| precision: 0.551667577911427\n",
      "epoch: 2\t| Loss: 2.134477259516716\t| precision: 0.5364583333333334\n",
      "epoch: 2\t| Loss: 2.14373766541481\t| precision: 0.5701042873696408\n",
      "epoch: 2\t| Loss: 2.1337843769788742\t| precision: 0.5674418604651162\n",
      "epoch: 2\t| Loss: 2.141289485692978\t| precision: 0.5814266487213997\n",
      "epoch: 2\t| Loss: 2.1374765741825104\t| precision: 0.5797101449275363\n",
      "mid epoch: 3551852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1370326501131056\t| precision: 0.5703703703703704\n",
      "epoch: 2\t| Loss: 2.136059287786484\t| precision: 0.539873417721519\n",
      "epoch: 2\t| Loss: 2.139009796977043\t| precision: 0.5755627009646302\n",
      "epoch: 2\t| Loss: 2.143876900672913\t| precision: 0.5581395348837209\n",
      "epoch: 2\t| Loss: 2.1295667767524717\t| precision: 0.5680136596471258\n",
      "epoch: 2\t| Loss: 2.1386301904916762\t| precision: 0.5458851391355832\n",
      "epoch: 2\t| Loss: 2.135834258198738\t| precision: 0.567458019687319\n",
      "epoch: 2\t| Loss: 2.1401940166950224\t| precision: 0.5440806045340051\n",
      "epoch: 2\t| Loss: 2.138575046658516\t| precision: 0.558708959376739\n",
      "epoch: 2\t| Loss: 2.138850985765457\t| precision: 0.5423293996921498\n",
      "epoch: 2\t| Loss: 2.1361229783296585\t| precision: 0.5724439584472389\n",
      "epoch: 2\t| Loss: 2.1440072411298754\t| precision: 0.5957446808510638\n",
      "epoch: 2\t| Loss: 2.1457280045747757\t| precision: 0.5523911875335841\n",
      "epoch: 2\t| Loss: 2.143159139752388\t| precision: 0.5479308538501834\n",
      "epoch: 2\t| Loss: 2.129630252122879\t| precision: 0.5640311804008908\n",
      "mid epoch: 3995852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1387129175662993\t| precision: 0.5468424705065926\n",
      "epoch: 2\t| Loss: 2.1377307629585265\t| precision: 0.5537017726798749\n",
      "epoch: 2\t| Loss: 2.1454409736394884\t| precision: 0.5428259683578832\n",
      "epoch: 2\t| Loss: 2.1397204965353014\t| precision: 0.5890722822993739\n",
      "epoch: 2\t| Loss: 2.1272889375686646\t| precision: 0.5581241743725232\n",
      "epoch: 2\t| Loss: 2.1424201971292494\t| precision: 0.5663345568956476\n",
      "epoch: 2\t| Loss: 2.1390988105535507\t| precision: 0.5554926387315968\n",
      "epoch: 2\t| Loss: 2.1476627933979033\t| precision: 0.5498094719651606\n",
      "epoch: 2\t| Loss: 2.139103010892868\t| precision: 0.5544852498494882\n",
      "epoch: 2\t| Loss: 2.13587591946125\t| precision: 0.551363900174115\n",
      "epoch: 2\t| Loss: 2.131355874538422\t| precision: 0.5633537447008949\n",
      "epoch: 2\t| Loss: 2.14423360645771\t| precision: 0.5538904899135446\n",
      "epoch: 2\t| Loss: 2.132881407737732\t| precision: 0.5502614758861127\n",
      "epoch: 2\t| Loss: 2.1452542662620546\t| precision: 0.5504848830576156\n",
      "epoch: 2\t| Loss: 2.149364259839058\t| precision: 0.5452337583485124\n",
      "mid epoch: 4439852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1291774380207062\t| precision: 0.5771028037383178\n",
      "epoch: 2\t| Loss: 2.1378842478990556\t| precision: 0.550414364640884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.1445187747478487\t| precision: 0.5505967825635704\n",
      "epoch: 2\t| Loss: 2.1435497826337815\t| precision: 0.5601300108342362\n",
      "epoch: 2\t| Loss: 2.1370330810546876\t| precision: 0.5875207067918278\n",
      "epoch: 2\t| Loss: 2.1356989258527754\t| precision: 0.591044776119403\n",
      "epoch: 2\t| Loss: 2.141528998017311\t| precision: 0.573224043715847\n",
      "epoch: 2\t| Loss: 2.1439471787214277\t| precision: 0.5326913329954385\n",
      "epoch: 2\t| Loss: 2.1423410218954086\t| precision: 0.5842105263157895\n",
      "epoch: 2\t| Loss: 2.137131509780884\t| precision: 0.5377407600208225\n",
      "epoch: 2\t| Loss: 2.147488324642181\t| precision: 0.5712216320618059\n",
      "epoch: 2\t| Loss: 2.1373765140771868\t| precision: 0.5361216730038023\n",
      "epoch: 2\t| Loss: 2.148872946500778\t| precision: 0.546875\n",
      "epoch: 2\t| Loss: 2.1361354422569274\t| precision: 0.5675944333996024\n",
      "epoch: 2\t| Loss: 2.136680036187172\t| precision: 0.5447042640990372\n",
      "mid epoch: 4883852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1374755746126173\t| precision: 0.5600468658465143\n",
      "epoch: 2\t| Loss: 2.1485018426179887\t| precision: 0.5778756191524491\n",
      "epoch: 2\t| Loss: 2.1445739674568176\t| precision: 0.5886075949367089\n",
      "epoch: 2\t| Loss: 2.129096881151199\t| precision: 0.5568246716162193\n",
      "epoch: 2\t| Loss: 2.1373949211835863\t| precision: 0.5643617021276596\n",
      "epoch: 2\t| Loss: 2.1374469774961473\t| precision: 0.5634028892455859\n",
      "epoch: 2\t| Loss: 2.152172537446022\t| precision: 0.5602910602910602\n",
      "epoch: 2\t| Loss: 2.1386593592166903\t| precision: 0.5765509391007398\n",
      "epoch: 2\t| Loss: 2.133006956577301\t| precision: 0.5803413772807534\n",
      "epoch: 2\t| Loss: 2.1381633293628695\t| precision: 0.5625744934445769\n",
      "epoch: 2\t| Loss: 2.1393649595975877\t| precision: 0.5570321151716501\n",
      "epoch: 2\t| Loss: 2.133958040475845\t| precision: 0.5622270742358079\n",
      "epoch: 2\t| Loss: 2.139357239603996\t| precision: 0.5443243243243243\n",
      "epoch: 2\t| Loss: 2.140008161664009\t| precision: 0.5694369973190349\n",
      "epoch: 2\t| Loss: 2.1376304596662523\t| precision: 0.5570175438596491\n",
      "mid epoch: 5327852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1485074031352998\t| precision: 0.5582857142857143\n",
      "epoch: 2\t| Loss: 2.141076601743698\t| precision: 0.5333749220212103\n",
      "epoch: 2\t| Loss: 2.1432124835252764\t| precision: 0.5558060879368658\n",
      "epoch: 2\t| Loss: 2.133076808452606\t| precision: 0.5807990459153249\n",
      "epoch: 2\t| Loss: 2.1268637591600417\t| precision: 0.5574971815107103\n",
      "epoch: 2\t| Loss: 2.1355888825654983\t| precision: 0.5573440643863179\n",
      "epoch: 2\t| Loss: 2.13266034245491\t| precision: 0.5551447935921133\n",
      "epoch: 2\t| Loss: 2.1388726860284804\t| precision: 0.5479693937610359\n",
      "epoch: 2\t| Loss: 2.138547798395157\t| precision: 0.5550611790878754\n",
      "epoch: 2\t| Loss: 2.1372076535224913\t| precision: 0.5752259436469963\n",
      "epoch: 2\t| Loss: 2.13235981464386\t| precision: 0.5472207231516459\n",
      "epoch: 2\t| Loss: 2.1386170357465746\t| precision: 0.5579374275782155\n",
      "epoch: 2\t| Loss: 2.137167296409607\t| precision: 0.5474530831099196\n",
      "epoch: 2\t| Loss: 2.124262313246727\t| precision: 0.5416427340608846\n",
      "epoch: 2\t| Loss: 2.136705790758133\t| precision: 0.5776464266523375\n",
      "epoch: 2\t| Loss: 2.1331641310453415\t| precision: 0.5329305135951662\n",
      "epoch: 2\t| Loss: 2.1391594594717027\t| precision: 0.5762910798122066\n",
      "epoch: 2\t| Loss: 2.14119190454483\t| precision: 0.5442842430484037\n",
      "epoch: 2\t| Loss: 2.13705939412117\t| precision: 0.5528815706143129\n",
      "epoch: 2\t| Loss: 2.1348555421829225\t| precision: 0.5759817871371656\n",
      "epoch: 2\t| Loss: 2.1409154099226\t| precision: 0.5945190156599552\n",
      "epoch: 2\t| Loss: 2.142578062415123\t| precision: 0.5524708495280399\n",
      "epoch: 2\t| Loss: 2.136914606690407\t| precision: 0.5864232743867656\n",
      "epoch: 2\t| Loss: 2.1490835708379747\t| precision: 0.5896201177100053\n",
      "epoch: 2\t| Loss: 2.145468128323555\t| precision: 0.5478387566779991\n",
      "epoch: 2\t| Loss: 2.141988165974617\t| precision: 0.5493537015276145\n",
      "epoch: 2\t| Loss: 2.1402516055107115\t| precision: 0.5632247377139702\n",
      "epoch: 2\t| Loss: 2.1369241374731063\t| precision: 0.5800889877641824\n",
      "epoch: 2\t| Loss: 2.1328310698270796\t| precision: 0.5495296070835639\n",
      "mid epoch: 6215852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1339217263460157\t| precision: 0.5607675906183369\n",
      "epoch: 2\t| Loss: 2.1433405965566634\t| precision: 0.5640750670241287\n",
      "epoch: 2\t| Loss: 2.1434908753633497\t| precision: 0.5478359908883826\n",
      "epoch: 2\t| Loss: 2.1432515239715575\t| precision: 0.5701136978884678\n",
      "epoch: 2\t| Loss: 2.141677289009094\t| precision: 0.546205472379969\n",
      "epoch: 2\t| Loss: 2.1384555262327196\t| precision: 0.5453083109919571\n",
      "epoch: 2\t| Loss: 2.139361806511879\t| precision: 0.5384615384615384\n",
      "epoch: 2\t| Loss: 2.124757533669472\t| precision: 0.5422171165996553\n",
      "epoch: 2\t| Loss: 2.1254602551460264\t| precision: 0.5594202898550724\n",
      "epoch: 2\t| Loss: 2.1387685644626617\t| precision: 0.5911083281152161\n",
      "epoch: 2\t| Loss: 2.1323915517330168\t| precision: 0.5611814345991561\n",
      "epoch: 2\t| Loss: 2.141994703412056\t| precision: 0.5462274176408076\n",
      "epoch: 2\t| Loss: 2.1345510518550874\t| precision: 0.59227921734532\n",
      "epoch: 2\t| Loss: 2.124546378850937\t| precision: 0.5611200861604739\n",
      "epoch: 2\t| Loss: 2.1479324996471405\t| precision: 0.5534557235421166\n",
      "mid epoch: 6659852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.129058955311775\t| precision: 0.5745664739884393\n",
      "epoch: 2\t| Loss: 2.139530498981476\t| precision: 0.5605063291139241\n",
      "epoch: 2\t| Loss: 2.1248591250181197\t| precision: 0.571117166212534\n",
      "epoch: 2\t| Loss: 2.137899854183197\t| precision: 0.5809617271835132\n",
      "epoch: 2\t| Loss: 2.133826726078987\t| precision: 0.5604982206405694\n",
      "epoch: 2\t| Loss: 2.138002491593361\t| precision: 0.5661265028750654\n",
      "epoch: 2\t| Loss: 2.147157187461853\t| precision: 0.5682870370370371\n",
      "epoch: 2\t| Loss: 2.1374840062856673\t| precision: 0.5472295514511873\n",
      "epoch: 2\t| Loss: 2.1349574702978136\t| precision: 0.551948051948052\n",
      "epoch: 2\t| Loss: 2.1401329797506334\t| precision: 0.5885643256681169\n",
      "epoch: 2\t| Loss: 2.1263442289829255\t| precision: 0.5627777777777778\n",
      "epoch: 2\t| Loss: 2.1409453028440475\t| precision: 0.5461496450027308\n",
      "epoch: 2\t| Loss: 2.1389874720573427\t| precision: 0.564327485380117\n",
      "epoch: 2\t| Loss: 2.133041164278984\t| precision: 0.5526011560693641\n",
      "epoch: 2\t| Loss: 2.1420791816711424\t| precision: 0.5688910225636101\n",
      "mid epoch: 7103852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1330744510889055\t| precision: 0.5653220951234196\n",
      "epoch: 2\t| Loss: 2.150782893896103\t| precision: 0.5435267857142857\n",
      "epoch: 2\t| Loss: 2.1378610062599184\t| precision: 0.5515898767034393\n",
      "epoch: 2\t| Loss: 2.1384001475572587\t| precision: 0.5399339933993399\n",
      "epoch: 2\t| Loss: 2.1394050830602644\t| precision: 0.5549065420560748\n",
      "epoch: 2\t| Loss: 2.1421587401628495\t| precision: 0.5626450116009281\n",
      "epoch: 2\t| Loss: 2.131176088452339\t| precision: 0.5469287469287469\n",
      "epoch: 2\t| Loss: 2.1446416652202607\t| precision: 0.5880149812734082\n",
      "epoch: 2\t| Loss: 2.1412388348579405\t| precision: 0.5596120935539076\n",
      "epoch: 2\t| Loss: 2.145590994358063\t| precision: 0.5714285714285714\n",
      "epoch: 2\t| Loss: 2.129122685790062\t| precision: 0.535607420706164\n",
      "epoch: 2\t| Loss: 2.146606065630913\t| precision: 0.5340010542962572\n",
      "epoch: 2\t| Loss: 2.138569094538689\t| precision: 0.5553719008264463\n",
      "epoch: 2\t| Loss: 2.1333832705020903\t| precision: 0.5438856015779092\n",
      "epoch: 2\t| Loss: 2.1425137591362\t| precision: 0.5629067245119306\n",
      "mid epoch: 7547852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1320899432897567\t| precision: 0.5552016985138004\n",
      "epoch: 2\t| Loss: 2.1344299989938738\t| precision: 0.5726962457337884\n",
      "epoch: 2\t| Loss: 2.1367612105607985\t| precision: 0.5424\n",
      "epoch: 2\t| Loss: 2.143035920858383\t| precision: 0.5175438596491229\n",
      "epoch: 2\t| Loss: 2.13310505092144\t| precision: 0.5562067128396377\n",
      "epoch: 2\t| Loss: 2.135703529715538\t| precision: 0.5434909515469936\n",
      "epoch: 2\t| Loss: 2.1286664819717407\t| precision: 0.5750853242320819\n",
      "epoch: 2\t| Loss: 2.1258288896083832\t| precision: 0.5489773950484392\n",
      "epoch: 2\t| Loss: 2.140138530731201\t| precision: 0.5579110651499483\n",
      "epoch: 2\t| Loss: 2.137307261824608\t| precision: 0.5503489531405783\n",
      "epoch: 2\t| Loss: 2.140140361189842\t| precision: 0.5502342529932327\n",
      "epoch: 2\t| Loss: 2.1351701933145524\t| precision: 0.5483476132190942\n",
      "epoch: 2\t| Loss: 2.1305601316690446\t| precision: 0.5732273028495692\n",
      "epoch: 2\t| Loss: 2.1323536735773088\t| precision: 0.5450261780104712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.144137145280838\t| precision: 0.5491452991452992\n",
      "mid epoch: 7991852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1409463900327683\t| precision: 0.5706006322444679\n",
      "epoch: 2\t| Loss: 2.135416386127472\t| precision: 0.5781426953567383\n",
      "epoch: 2\t| Loss: 2.1412679731845854\t| precision: 0.5455062571103527\n",
      "epoch: 2\t| Loss: 2.1323369538784025\t| precision: 0.5754716981132075\n",
      "epoch: 2\t| Loss: 2.145702986717224\t| precision: 0.5647249190938511\n",
      "epoch: 2\t| Loss: 2.1325427103042602\t| precision: 0.5646992054483542\n",
      "epoch: 2\t| Loss: 2.135221792459488\t| precision: 0.5886109632783395\n",
      "epoch: 2\t| Loss: 2.1477802765369414\t| precision: 0.5643879173290938\n",
      "epoch: 2\t| Loss: 2.1356387835741044\t| precision: 0.5747406955460647\n",
      "epoch: 2\t| Loss: 2.146267749071121\t| precision: 0.56991643454039\n",
      "epoch: 2\t| Loss: 2.1361894476413728\t| precision: 0.5633507853403141\n",
      "epoch: 2\t| Loss: 2.14195404112339\t| precision: 0.5619546247818499\n",
      "epoch: 2\t| Loss: 2.1378991520404815\t| precision: 0.5768063145112325\n",
      "epoch: 2\t| Loss: 2.147286242246628\t| precision: 0.5528205128205128\n",
      "epoch: 2\t| Loss: 2.1440769535303117\t| precision: 0.5389869095048377\n",
      "mid epoch: 8435852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1397701531648634\t| precision: 0.5714285714285714\n",
      "epoch: 2\t| Loss: 2.1308501023054123\t| precision: 0.5430283224400871\n",
      "epoch: 2\t| Loss: 2.1377170675992967\t| precision: 0.5794497473329591\n",
      "epoch: 2\t| Loss: 2.133013864159584\t| precision: 0.5528789659224442\n",
      "epoch: 2\t| Loss: 2.131921578645706\t| precision: 0.5675675675675675\n",
      "epoch: 2\t| Loss: 2.138956428766251\t| precision: 0.5772655840754322\n",
      "epoch: 2\t| Loss: 2.151016574501991\t| precision: 0.5285462036492055\n",
      "epoch: 2\t| Loss: 2.1402572804689406\t| precision: 0.5592469545957918\n",
      "epoch: 2\t| Loss: 2.1458314055204393\t| precision: 0.5692771084337349\n",
      "epoch: 2\t| Loss: 2.1377103435993194\t| precision: 0.5495890410958905\n",
      "epoch: 2\t| Loss: 2.131702958345413\t| precision: 0.5334467120181405\n",
      "epoch: 2\t| Loss: 2.132751030921936\t| precision: 0.5579010856453559\n",
      "epoch: 2\t| Loss: 2.1437278026342392\t| precision: 0.570489844683393\n",
      "epoch: 2\t| Loss: 2.1341997545957567\t| precision: 0.5454545454545454\n",
      "epoch: 2\t| Loss: 2.146061421632767\t| precision: 0.5568531038721574\n",
      "mid epoch: 8879852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1393084037303924\t| precision: 0.5916622127204704\n",
      "epoch: 2\t| Loss: 2.1376896679401396\t| precision: 0.5429394812680115\n",
      "epoch: 2\t| Loss: 2.13283353805542\t| precision: 0.5522101217168481\n",
      "epoch: 2\t| Loss: 2.1433542531728746\t| precision: 0.5582599689280165\n",
      "epoch: 2\t| Loss: 2.132615651488304\t| precision: 0.544013121924549\n",
      "epoch: 2\t| Loss: 2.137793257832527\t| precision: 0.5782356728911784\n",
      "epoch: 2\t| Loss: 2.134589881896973\t| precision: 0.5536368393511251\n",
      "epoch: 2\t| Loss: 2.13875256896019\t| precision: 0.5784708249496981\n",
      "epoch: 2\t| Loss: 2.1468268448114394\t| precision: 0.5661375661375662\n",
      "epoch: 2\t| Loss: 2.1319733029603958\t| precision: 0.5642651296829971\n",
      "epoch: 2\t| Loss: 2.137672029733658\t| precision: 0.5712025316455697\n",
      "epoch: 2\t| Loss: 2.1388678658008575\t| precision: 0.5878378378378378\n",
      "epoch: 2\t| Loss: 2.1341610991954805\t| precision: 0.5729600952948184\n",
      "epoch: 2\t| Loss: 2.1341691428422926\t| precision: 0.5764488286066585\n",
      "epoch: 2\t| Loss: 2.13631092607975\t| precision: 0.5491400491400491\n",
      "mid epoch: 9323852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.140959631204605\t| precision: 0.5557830092118731\n",
      "epoch: 2\t| Loss: 2.1384206652641295\t| precision: 0.5341905318527177\n",
      "epoch: 2\t| Loss: 2.144929373860359\t| precision: 0.5592391304347826\n",
      "epoch: 2\t| Loss: 2.131404787302017\t| precision: 0.5611210494931426\n",
      "epoch: 2\t| Loss: 2.1300381702184676\t| precision: 0.5512091038406828\n",
      "epoch: 2\t| Loss: 2.1468040955066683\t| precision: 0.5685058896466212\n",
      "epoch: 2\t| Loss: 2.132879791855812\t| precision: 0.5379388448471121\n",
      "epoch: 2\t| Loss: 2.1230069118738175\t| precision: 0.5725534308211474\n",
      "epoch: 2\t| Loss: 2.1353892242908477\t| precision: 0.5687303252885625\n",
      "epoch: 2\t| Loss: 2.141731532216072\t| precision: 0.5581778265642151\n",
      "epoch: 2\t| Loss: 2.1441130024194717\t| precision: 0.5529411764705883\n",
      "epoch: 2\t| Loss: 2.1382892364263535\t| precision: 0.5672097759674134\n",
      "epoch: 2\t| Loss: 2.1383361870050432\t| precision: 0.5633047210300429\n",
      "epoch: 2\t| Loss: 2.136929477453232\t| precision: 0.5515075376884422\n",
      "epoch: 2\t| Loss: 2.1317317163944245\t| precision: 0.5757912270960578\n",
      "mid epoch: 9767852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1485196381807325\t| precision: 0.5594688221709007\n",
      "epoch: 2\t| Loss: 2.1452019810676575\t| precision: 0.5758196721311475\n",
      "epoch: 2\t| Loss: 2.1299115991592408\t| precision: 0.5676126878130217\n",
      "epoch: 2\t| Loss: 2.134246161580086\t| precision: 0.5595505617977528\n",
      "epoch: 2\t| Loss: 2.1316585433483124\t| precision: 0.5624652970571904\n",
      "epoch: 2\t| Loss: 2.1367669171094894\t| precision: 0.5529820497973365\n",
      "epoch: 2\t| Loss: 2.1422207283973695\t| precision: 0.5202374527792768\n",
      "epoch: 2\t| Loss: 2.1346733927726746\t| precision: 0.5702387562465298\n",
      "epoch: 2\t| Loss: 2.130578337907791\t| precision: 0.5658174097664543\n",
      "epoch: 2\t| Loss: 2.1351420509815218\t| precision: 0.5644578313253013\n",
      "epoch: 2\t| Loss: 2.137738837003708\t| precision: 0.5452369554810914\n",
      "epoch: 2\t| Loss: 2.1377202594280242\t| precision: 0.5528846153846154\n",
      "epoch: 2\t| Loss: 2.138547332286835\t| precision: 0.5520833333333334\n",
      "epoch: 2\t| Loss: 2.140971401929855\t| precision: 0.5474701534963047\n",
      "epoch: 2\t| Loss: 2.13191810965538\t| precision: 0.5709046454767727\n",
      "mid epoch: 10211852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1474820166826247\t| precision: 0.5491480996068152\n",
      "epoch: 2\t| Loss: 2.1330311280488967\t| precision: 0.5520249221183801\n",
      "epoch: 2\t| Loss: 2.134489418268204\t| precision: 0.5763358778625954\n",
      "epoch: 2\t| Loss: 2.1401384794712066\t| precision: 0.5560975609756098\n",
      "epoch: 2\t| Loss: 2.1429196441173555\t| precision: 0.5570200573065902\n",
      "epoch: 2\t| Loss: 2.141619328260422\t| precision: 0.5820580474934037\n",
      "epoch: 2\t| Loss: 2.1365316063165665\t| precision: 0.5535614133482895\n",
      "epoch: 2\t| Loss: 2.1361257272958754\t| precision: 0.5644012944983818\n",
      "epoch: 2\t| Loss: 2.1400770890712737\t| precision: 0.5687919463087249\n",
      "epoch: 2\t| Loss: 2.142979615330696\t| precision: 0.562844880441447\n",
      "epoch: 2\t| Loss: 2.1387921553850173\t| precision: 0.5471698113207547\n",
      "epoch: 2\t| Loss: 2.1386217606067657\t| precision: 0.5589248793935218\n",
      "epoch: 2\t| Loss: 2.1292797428369523\t| precision: 0.5611045828437132\n",
      "epoch: 2\t| Loss: 2.1310117572546003\t| precision: 0.5574743658931463\n",
      "epoch: 2\t| Loss: 2.1407342183589937\t| precision: 0.5639943741209564\n",
      "mid epoch: 10655852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.137598477602005\t| precision: 0.5727429557216791\n",
      "epoch: 2\t| Loss: 2.139091569185257\t| precision: 0.550253807106599\n",
      "epoch: 2\t| Loss: 2.132102701663971\t| precision: 0.5658129839189994\n",
      "epoch: 2\t| Loss: 2.1293874162435533\t| precision: 0.5641316685584563\n",
      "epoch: 2\t| Loss: 2.1371325731277464\t| precision: 0.5576817933296884\n",
      "epoch: 2\t| Loss: 2.1452191323041916\t| precision: 0.5777173913043478\n",
      "epoch: 2\t| Loss: 2.136704185605049\t| precision: 0.5736310473152578\n",
      "epoch: 2\t| Loss: 2.1284659516811373\t| precision: 0.562302340290955\n",
      "epoch: 2\t| Loss: 2.1367751467227936\t| precision: 0.5532157085941947\n",
      "epoch: 2\t| Loss: 2.1379978853464126\t| precision: 0.5742280285035629\n",
      "epoch: 2\t| Loss: 2.1356383144855497\t| precision: 0.5558894230769231\n",
      "epoch: 2\t| Loss: 2.1356733548641205\t| precision: 0.5608815426997246\n",
      "epoch: 2\t| Loss: 2.142140833735466\t| precision: 0.5678330263965623\n",
      "epoch: 2\t| Loss: 2.134096963405609\t| precision: 0.5852618757612668\n",
      "epoch: 2\t| Loss: 2.130311440825462\t| precision: 0.558589306029579\n",
      "mid epoch: 11099852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1407883477211\t| precision: 0.5724925521350546\n",
      "epoch: 2\t| Loss: 2.1269660818576814\t| precision: 0.5715080690038954\n",
      "epoch: 2\t| Loss: 2.1303959465026856\t| precision: 0.5587570621468927\n",
      "epoch: 2\t| Loss: 2.1340399688482283\t| precision: 0.5374692874692875\n",
      "epoch: 2\t| Loss: 2.1394139415025712\t| precision: 0.53895754970446\n",
      "epoch: 2\t| Loss: 2.137991864681244\t| precision: 0.5718816067653277\n",
      "epoch: 2\t| Loss: 2.15383482336998\t| precision: 0.5491803278688525\n",
      "epoch: 2\t| Loss: 2.1312003433704376\t| precision: 0.5885245901639344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.132319138646126\t| precision: 0.5438871473354232\n",
      "epoch: 2\t| Loss: 2.1451578134298326\t| precision: 0.5956227201667535\n",
      "epoch: 2\t| Loss: 2.1437111085653306\t| precision: 0.5623456790123457\n",
      "epoch: 2\t| Loss: 2.129543174505234\t| precision: 0.5378006872852233\n",
      "epoch: 2\t| Loss: 2.1309649467468263\t| precision: 0.5514069264069265\n",
      "epoch: 2\t| Loss: 2.1322144573926924\t| precision: 0.5700431034482759\n",
      "epoch: 2\t| Loss: 2.1393725800514223\t| precision: 0.5563230605738576\n",
      "mid epoch: 11543852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1402739322185518\t| precision: 0.5782016348773842\n",
      "epoch: 2\t| Loss: 2.1450747662782668\t| precision: 0.5793269230769231\n",
      "epoch: 2\t| Loss: 2.1321280705928802\t| precision: 0.572599531615925\n",
      "epoch: 2\t| Loss: 2.129420217871666\t| precision: 0.5664804469273743\n",
      "epoch: 2\t| Loss: 2.142715746164322\t| precision: 0.5736228813559322\n",
      "epoch: 2\t| Loss: 2.123366693854332\t| precision: 0.5574412532637075\n",
      "epoch: 2\t| Loss: 2.145994359254837\t| precision: 0.5481563015960375\n",
      "epoch: 2\t| Loss: 2.146045762300491\t| precision: 0.551219512195122\n",
      "epoch: 2\t| Loss: 2.1336399900913237\t| precision: 0.5849150849150849\n",
      "epoch: 2\t| Loss: 2.1416942542791366\t| precision: 0.5561993047508691\n",
      "epoch: 2\t| Loss: 2.1354776775836943\t| precision: 0.5414118954974986\n",
      "epoch: 2\t| Loss: 2.1384202629327773\t| precision: 0.5582733812949641\n",
      "epoch: 2\t| Loss: 2.1347362571954727\t| precision: 0.5704148471615721\n",
      "epoch: 2\t| Loss: 2.1353144323825837\t| precision: 0.5579352004393191\n",
      "epoch: 2\t| Loss: 2.1388302838802336\t| precision: 0.572972972972973\n",
      "mid epoch: 11987852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1249023777246476\t| precision: 0.5658042744656918\n",
      "epoch: 2\t| Loss: 2.1417821180820464\t| precision: 0.5693967631191761\n",
      "epoch: 2\t| Loss: 2.1381342208385465\t| precision: 0.5784447476125512\n",
      "epoch: 2\t| Loss: 2.143076519370079\t| precision: 0.5561097256857855\n",
      "epoch: 2\t| Loss: 2.143507588505745\t| precision: 0.5520082389289392\n",
      "epoch: 2\t| Loss: 2.1389739525318148\t| precision: 0.5403174603174603\n",
      "epoch: 2\t| Loss: 2.1350913965702056\t| precision: 0.532602423542989\n",
      "epoch: 2\t| Loss: 2.1256228506565096\t| precision: 0.5691150954308849\n",
      "epoch: 2\t| Loss: 2.14110229074955\t| precision: 0.5908584169453734\n",
      "epoch: 2\t| Loss: 2.1427145981788636\t| precision: 0.5565123789020452\n",
      "epoch: 2\t| Loss: 2.139934514760971\t| precision: 0.5682556879739978\n",
      "epoch: 2\t| Loss: 2.135967943072319\t| precision: 0.5394736842105263\n",
      "epoch: 2\t| Loss: 2.134559206366539\t| precision: 0.5557221389305347\n",
      "epoch: 2\t| Loss: 2.139651519060135\t| precision: 0.5580503833515882\n",
      "epoch: 2\t| Loss: 2.1381239527463913\t| precision: 0.5514663599769982\n",
      "mid epoch: 12431852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.135816767811775\t| precision: 0.5428571428571428\n",
      "epoch: 2\t| Loss: 2.1371459299325943\t| precision: 0.5514705882352942\n",
      "epoch: 2\t| Loss: 2.1391065210103988\t| precision: 0.5434391851408029\n",
      "epoch: 2\t| Loss: 2.138031197786331\t| precision: 0.5280429594272077\n",
      "epoch: 2\t| Loss: 2.133580721616745\t| precision: 0.5417149478563151\n",
      "epoch: 2\t| Loss: 2.144570224285126\t| precision: 0.5536585365853659\n",
      "epoch: 2\t| Loss: 2.1288464695215223\t| precision: 0.5603864734299517\n",
      "epoch: 2\t| Loss: 2.133985409736633\t| precision: 0.5571245186136072\n",
      "epoch: 2\t| Loss: 2.131918027997017\t| precision: 0.5810113519091847\n",
      "epoch: 2\t| Loss: 2.1264757603406905\t| precision: 0.572\n",
      "epoch: 2\t| Loss: 2.1369984340667725\t| precision: 0.5764192139737991\n",
      "epoch: 2\t| Loss: 2.136961278319359\t| precision: 0.553763440860215\n",
      "epoch: 2\t| Loss: 2.1398433208465577\t| precision: 0.5423452768729642\n",
      "epoch: 2\t| Loss: 2.121882417201996\t| precision: 0.5247735748534896\n",
      "epoch: 2\t| Loss: 2.133434860706329\t| precision: 0.6002351557907113\n",
      "mid epoch: 12875852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1368403124809263\t| precision: 0.5674390968508616\n",
      "epoch: 2\t| Loss: 2.142800203561783\t| precision: 0.5715116279069767\n",
      "epoch: 2\t| Loss: 2.128981170654297\t| precision: 0.5884520884520884\n",
      "epoch: 2\t| Loss: 2.135937283039093\t| precision: 0.5776942355889725\n",
      "epoch: 2\t| Loss: 2.137539206147194\t| precision: 0.5716900549115315\n",
      "epoch: 2\t| Loss: 2.124036073088646\t| precision: 0.5971443141254462\n",
      "epoch: 2\t| Loss: 2.1250113892555236\t| precision: 0.5655783065855008\n",
      "epoch: 2\t| Loss: 2.1381512260437012\t| precision: 0.5383783783783784\n",
      "epoch: 2\t| Loss: 2.1367513012886046\t| precision: 0.555609995100441\n",
      "epoch: 2\t| Loss: 2.1438082951307296\t| precision: 0.5381637168141593\n",
      "epoch: 2\t| Loss: 2.131076592206955\t| precision: 0.5567061143984221\n",
      "epoch: 2\t| Loss: 2.1426481235027315\t| precision: 0.5480386583285958\n",
      "epoch: 2\t| Loss: 2.1312550282478333\t| precision: 0.5641456582633053\n",
      "epoch: 2\t| Loss: 2.1348487550020216\t| precision: 0.5520774046670461\n",
      "epoch: 2\t| Loss: 2.1418209499120713\t| precision: 0.5727181544633901\n",
      "mid epoch: 13319852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1401598143577574\t| precision: 0.5710023866348448\n",
      "epoch: 2\t| Loss: 2.131840322613716\t| precision: 0.5815347721822542\n",
      "epoch: 2\t| Loss: 2.1320534640550615\t| precision: 0.5704633204633205\n",
      "epoch: 2\t| Loss: 2.121779075860977\t| precision: 0.5627853881278538\n",
      "epoch: 2\t| Loss: 2.144665403366089\t| precision: 0.5631852279284478\n",
      "epoch: 2\t| Loss: 2.140652130842209\t| precision: 0.5786548082267927\n",
      "epoch: 2\t| Loss: 2.128695085644722\t| precision: 0.5801699716713881\n",
      "epoch: 2\t| Loss: 2.135699472427368\t| precision: 0.5686947988223748\n",
      "epoch: 2\t| Loss: 2.13293082177639\t| precision: 0.5628235294117647\n",
      "epoch: 2\t| Loss: 2.126228702068329\t| precision: 0.573170731707317\n",
      "epoch: 2\t| Loss: 2.128643419742584\t| precision: 0.5505154639175258\n",
      "epoch: 2\t| Loss: 2.141174585223198\t| precision: 0.5540051679586563\n",
      "epoch: 2\t| Loss: 2.1244830638170242\t| precision: 0.5603337612323491\n",
      "epoch: 2\t| Loss: 2.138117409944534\t| precision: 0.589824120603015\n",
      "epoch: 2\t| Loss: 2.1340589129924776\t| precision: 0.5971796443899449\n",
      "mid epoch: 13763852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.141746881008148\t| precision: 0.566412213740458\n",
      "epoch: 2\t| Loss: 2.135596641898155\t| precision: 0.570828331332533\n",
      "epoch: 2\t| Loss: 2.142799912691116\t| precision: 0.5467712733856367\n",
      "epoch: 2\t| Loss: 2.1290193635225294\t| precision: 0.5531095211887727\n",
      "epoch: 2\t| Loss: 2.1330957168340685\t| precision: 0.5623356128353498\n",
      "epoch: 2\t| Loss: 2.136641783118248\t| precision: 0.5493107104984093\n",
      "epoch: 2\t| Loss: 2.1308586436510084\t| precision: 0.5792151162790697\n",
      "epoch: 2\t| Loss: 2.135310350060463\t| precision: 0.5490745933819405\n",
      "epoch: 2\t| Loss: 2.1317526519298555\t| precision: 0.5811777655476059\n",
      "epoch: 2\t| Loss: 2.1330988454818725\t| precision: 0.5540540540540541\n",
      "epoch: 2\t| Loss: 2.1424279975891114\t| precision: 0.5716552088841883\n",
      "epoch: 2\t| Loss: 2.1396028751134875\t| precision: 0.5793609219486642\n",
      "epoch: 2\t| Loss: 2.1352917557954787\t| precision: 0.580316742081448\n",
      "epoch: 2\t| Loss: 2.138883128762245\t| precision: 0.5612732095490717\n",
      "epoch: 2\t| Loss: 2.139612565636635\t| precision: 0.5861358956760466\n",
      "mid epoch: 14207852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.139226584434509\t| precision: 0.5351351351351351\n",
      "epoch: 2\t| Loss: 2.140158023238182\t| precision: 0.5866604039455143\n",
      "epoch: 2\t| Loss: 2.145423160791397\t| precision: 0.5442622950819672\n",
      "epoch: 2\t| Loss: 2.1322518646717072\t| precision: 0.5571266968325792\n",
      "epoch: 2\t| Loss: 2.1381625217199325\t| precision: 0.5717488789237668\n",
      "epoch: 2\t| Loss: 2.1352025204896927\t| precision: 0.5429835651074589\n",
      "epoch: 2\t| Loss: 2.14137049138546\t| precision: 0.5562659846547314\n",
      "epoch: 2\t| Loss: 2.1356316673755646\t| precision: 0.558759521218716\n",
      "epoch: 2\t| Loss: 2.131894773840904\t| precision: 0.5530624620982414\n",
      "epoch: 2\t| Loss: 2.139061329960823\t| precision: 0.5418388429752066\n",
      "epoch: 2\t| Loss: 2.1405865174531935\t| precision: 0.5693473193473193\n",
      "epoch: 2\t| Loss: 2.133924087882042\t| precision: 0.549365692222835\n",
      "epoch: 2\t| Loss: 2.1364463794231416\t| precision: 0.5673998871968415\n",
      "epoch: 2\t| Loss: 2.1393329298496244\t| precision: 0.568986568986569\n",
      "epoch: 2\t| Loss: 2.1284455502033235\t| precision: 0.5735785953177257\n",
      "mid epoch: 14651852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1371366775035856\t| precision: 0.5565782349756362\n",
      "epoch: 2\t| Loss: 2.130184059739113\t| precision: 0.568545994065282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t| Loss: 2.1333222711086273\t| precision: 0.53125\n",
      "epoch: 2\t| Loss: 2.129598368406296\t| precision: 0.565695792880259\n",
      "epoch: 2\t| Loss: 2.1449147194623945\t| precision: 0.5717761557177615\n",
      "epoch: 2\t| Loss: 2.1305498868227004\t| precision: 0.561915244909191\n",
      "epoch: 2\t| Loss: 2.1377194958925245\t| precision: 0.5405257393209201\n",
      "epoch: 2\t| Loss: 2.139346051812172\t| precision: 0.5824451410658307\n",
      "epoch: 2\t| Loss: 2.1294828456640245\t| precision: 0.5720419847328244\n",
      "epoch: 2\t| Loss: 2.1460251456499098\t| precision: 0.5356744704570792\n",
      "epoch: 2\t| Loss: 2.1306009703874587\t| precision: 0.5741789354473387\n",
      "epoch: 2\t| Loss: 2.14677562892437\t| precision: 0.540973505853358\n",
      "epoch: 2\t| Loss: 2.1451140981912613\t| precision: 0.5686370297997069\n",
      "epoch: 2\t| Loss: 2.1273239642381667\t| precision: 0.5566502463054187\n",
      "epoch: 2\t| Loss: 2.1389800649881363\t| precision: 0.5437239738251041\n",
      "mid epoch: 15095852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.136727100014687\t| precision: 0.5922388059701492\n",
      "epoch: 2\t| Loss: 2.1503557646274567\t| precision: 0.5413363533408834\n",
      "epoch: 2\t| Loss: 2.120584074854851\t| precision: 0.576063829787234\n",
      "epoch: 2\t| Loss: 2.130047579407692\t| precision: 0.5778461538461539\n",
      "epoch: 2\t| Loss: 2.1306433379650116\t| precision: 0.5634792004321988\n",
      "epoch: 2\t| Loss: 2.1404201805591585\t| precision: 0.5655960805661404\n",
      "epoch: 2\t| Loss: 2.141900765299797\t| precision: 0.5544367526746381\n",
      "epoch: 2\t| Loss: 2.1334760373830797\t| precision: 0.5721417873190214\n",
      "epoch: 2\t| Loss: 2.137747432589531\t| precision: 0.5477154424522845\n",
      "epoch: 2\t| Loss: 2.135143234729767\t| precision: 0.5605499735589635\n",
      "epoch: 2\t| Loss: 2.1362336754798887\t| precision: 0.5602209944751381\n",
      "epoch: 2\t| Loss: 2.137761631011963\t| precision: 0.555992141453831\n",
      "epoch: 2\t| Loss: 2.13908234000206\t| precision: 0.5893670886075949\n",
      "epoch: 2\t| Loss: 2.131563990712166\t| precision: 0.6008943543879263\n",
      "epoch: 2\t| Loss: 2.1295792257785795\t| precision: 0.5450236966824644\n",
      "mid epoch: 15539852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1326743733882902\t| precision: 0.5572755417956656\n",
      "epoch: 2\t| Loss: 2.129984889626503\t| precision: 0.5581996896016554\n",
      "epoch: 2\t| Loss: 2.1338060230016707\t| precision: 0.5510852302805718\n",
      "epoch: 2\t| Loss: 2.1373151749372483\t| precision: 0.5652687536916716\n",
      "epoch: 2\t| Loss: 2.1339096397161486\t| precision: 0.5496277915632755\n",
      "epoch: 2\t| Loss: 2.1398659855127335\t| precision: 0.5493119266055045\n",
      "epoch: 2\t| Loss: 2.1383750224113465\t| precision: 0.5555555555555556\n",
      "epoch: 2\t| Loss: 2.13852920293808\t| precision: 0.5592799503414029\n",
      "epoch: 2\t| Loss: 2.135063437819481\t| precision: 0.5455049944506104\n",
      "epoch: 2\t| Loss: 2.132554762363434\t| precision: 0.5514322916666666\n",
      "epoch: 2\t| Loss: 2.1442129695415497\t| precision: 0.5809922295277944\n",
      "epoch: 2\t| Loss: 2.138115101456642\t| precision: 0.5665768194070081\n",
      "epoch: 2\t| Loss: 2.1392519795894622\t| precision: 0.5634028892455859\n",
      "epoch: 2\t| Loss: 2.136612156033516\t| precision: 0.5461493239271017\n",
      "epoch: 2\t| Loss: 2.142187271118164\t| precision: 0.5356762513312034\n",
      "mid epoch: 15983852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Loss: 2.1412516647577284\t| precision: 0.5584725536992841\n",
      "epoch: 2\t| Loss: 2.1377199959754942\t| precision: 0.5503122831367107\n",
      "epoch: 2\t| Loss: 2.1378759211301803\t| precision: 0.5673239436619718\n",
      "epoch: 2\t| Loss: 2.1392795222997667\t| precision: 0.5630853994490358\n",
      "epoch: 2\t| Loss: 2.1345259058475494\t| precision: 0.5729230769230769\n",
      "epoch: 2\t| Loss: 2.1366599416732788\t| precision: 0.5765717356260075\n",
      "epoch: 2\t| Loss: 2.1335694509744645\t| precision: 0.5996228786926462\n",
      "epoch: 2\t| Loss: 2.1295736026763916\t| precision: 0.5721442885771543\n",
      "epoch: 2\t| Loss: 2.1305097126960755\t| precision: 0.5577836411609499\n",
      "epoch: 2\t| Loss: 2.138807435631752\t| precision: 0.5658424381828637\n",
      "epoch: 2\t| Loss: 2.128191509246826\t| precision: 0.5574559059326564\n",
      "epoch: 2\t| Loss: 2.123538762331009\t| precision: 0.5926950747094631\n",
      "epoch: 2\t| Loss: 2.1402023100852965\t| precision: 0.5666293393057111\n",
      "epoch: 2\t| Loss: 2.1393368846178054\t| precision: 0.5587918015102481\n",
      "epoch: 2\t| Loss: 2.1368516051769255\t| precision: 0.5558375634517766\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "done epoch 2\n",
      "epoch: 3\t| Loss: 0.010492241382598877\t| precision: 0.572459744586341\n",
      "epoch: 3\t| Loss: 2.134633882045746\t| precision: 0.5754285714285714\n",
      "epoch: 3\t| Loss: 2.128602669239044\t| precision: 0.5289256198347108\n",
      "epoch: 3\t| Loss: 2.1363315135240555\t| precision: 0.5393900889453621\n",
      "epoch: 3\t| Loss: 2.129365736246109\t| precision: 0.5442896935933148\n",
      "epoch: 3\t| Loss: 2.132451230287552\t| precision: 0.5815856777493607\n",
      "epoch: 3\t| Loss: 2.127905558347702\t| precision: 0.5738213399503722\n",
      "epoch: 3\t| Loss: 2.136306692957878\t| precision: 0.5585\n",
      "epoch: 3\t| Loss: 2.1243666911125185\t| precision: 0.5673590504451038\n",
      "epoch: 3\t| Loss: 2.142227042913437\t| precision: 0.5857409133271202\n",
      "epoch: 3\t| Loss: 2.137082473039627\t| precision: 0.5464340687532068\n",
      "epoch: 3\t| Loss: 2.1251949697732924\t| precision: 0.545335085413929\n",
      "epoch: 3\t| Loss: 2.13188081741333\t| precision: 0.5674876847290641\n",
      "epoch: 3\t| Loss: 2.141093047261238\t| precision: 0.5132566283141571\n",
      "epoch: 3\t| Loss: 2.1348408675193786\t| precision: 0.5194274028629857\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t| Loss: 2.126900753378868\t| precision: 0.573512252042007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f0e93697b0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'to__save: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"** ** * Saving fine - tuned model ** ** * \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f0e93697b0fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, e, validload, optim)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_accumulation_steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_accumulation_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtemp_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/well/rahimi/users/gra027/conda/envs/MLGPvision/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/well/rahimi/users/gra027/conda/envs/MLGPvision/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(model, e, validload, optim):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(validload):\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "        age_ids, year_ids,input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "#         print(batch)\n",
    "        loss, pred, label = model(input_ids, age_ids, segment_ids, posi_ids,year_ids,  attention_mask=attMask,\n",
    "                                  masked_lm_labels=masked_label)\n",
    "\n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if step % 200 == 0:\n",
    "            print(\"epoch: {}\\t| Loss: {}\\t| precision: {}\".format(e, temp_loss / 200, cal_acc(label, pred)))\n",
    "            temp_loss = 0\n",
    "\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        if (step+1 )%3000 ==0:\n",
    "            print(\"mid epoch: \" +str(step *148) + \"..... ** ** * Saving fine - tuned model ** ** * \")\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "print('starting epoch0')\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_CEHR_newcut1985_2020_DM__6msummary.bin\")\n",
    "\n",
    "print('to__save: ',output_model_file)\n",
    "for e in range(50):\n",
    "    train(model, e, trainload, optim)\n",
    "\n",
    "    print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "    #         create_folder(global_params['output_dir'])\n",
    "    print('done epoch', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, e, validload, optim):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(validload):\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "        age_ids, year_ids,input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "#         print(batch)\n",
    "        print(age_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python MLGPvision",
   "language": "python",
   "name": "mlgpvision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
