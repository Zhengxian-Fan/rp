{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting run....\n",
      "starting run....\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "print('starting run....')\n",
    "\n",
    "import sys\n",
    "\n",
    "print('starting run....')\n",
    "\n",
    "sys.path.insert(0,'/gpfs3/well/rahimi/users/gra027/JNb/')\n",
    "\n",
    "from general_model_newCutCPRD.ModelPkg.BEHRTraw import *\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from general_model_newCutCPRD.ModelPkg import utils\n",
    "from general_model_newCutCPRD.ModelPkg.MLMRaw import *\n",
    "\n",
    "from general_model_newCutCPRD.ModelPkg.DataProc import *\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from general_model_newCutCPRD.pytorch_pretrained_bert  import optimizer\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3662\n",
      "read data....\n",
      "dict_keys(['bert.embeddings.word_embeddings.weight', 'bert.embeddings.segment_embeddings.weight', 'bert.embeddings.age_embeddings.weight', 'bert.embeddings.year_embeddings.weight', 'bert.embeddings.posi_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight'])\n"
     ]
    }
   ],
   "source": [
    "file_config = {\n",
    "        'vocab': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/GeneralVocDM_25k',\n",
    "    'oldvocab': '/gpfs3/well/rahimi/users/gra027/JNb/ExpHypCancer/Data/AllSubclass/DMBp',\n",
    "    'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_for_pretraining_28M_1985_2020__unique_per6m_70pc_sample.parquet/',\n",
    "\n",
    "#     'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_for_pretraining_28M_1985_2020__unique_per6m_50pc_sample___10kdebug.parquet/',\n",
    "    #\n",
    "    'yearVocab':  '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/yearVoc_1985_2021',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 124,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:2',\n",
    "    'output_dir':'/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/SavedModels/',\n",
    "    'output_name': 'MLM_newcut1985_2020_DM.bin',\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 250,\n",
    "    'max_age': 110,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    'yearOn':True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "YearVocab = utils.load_obj(file_config['yearVocab'])\n",
    "create_folder(global_params['output_dir'])\n",
    "BertVocab = utils.load_obj(file_config['vocab'])\n",
    "print(len(BertVocab['token2idx']))\n",
    "\n",
    "ageVocab, _ = utils.age_vocab(max_age=global_params['max_age'], year=global_params['age_year'], symbol=global_params['age_symbol'])\n",
    "fulldata = pd.read_parquet(file_config['fulld'])\n",
    "print('read data....')\n",
    "\n",
    "trainSet = MLMLoader(token2idx=BertVocab['token2idx'], dataframe=fulldata, max_len=global_params['max_len_seq'], max_age=global_params['max_age'], year=global_params['age_year'], age_symbol=global_params['age_symbol'],year2idx = YearVocab['token2idx'] )\n",
    "trainload = DataLoader(dataset=trainSet, batch_size=global_params['batch_size'], shuffle=True)\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 150, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()), # number of vocab for age embedding\n",
    "\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.15, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 6, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.15, # multi-head attention dropout rate\n",
    "    'intermediate_size': 108, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range,\n",
    "    'yearOn':True,\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()),\n",
    "\n",
    "}\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForMaskedLM(conf)\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_newcut1985_2020_DM.bin\")\n",
    "model = toLoad(model, output_model_file)\n",
    "model = model.to(global_params['device'])\n",
    "optim = optimizer.adam(params=list(model.named_parameters()), config=optim_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# YearVocab = {'token2idx': {'PAD': 0,\n",
    "#   '1987': 1,\n",
    "#   '1988': 2,\n",
    "#   '1989': 3,\n",
    "#   '1990': 4,\n",
    "#   '1991': 5,\n",
    "#   '1992': 6,\n",
    "#   '1993': 7,\n",
    "#   '1994': 8,\n",
    "#   '1995': 9,\n",
    "#   '1996': 10,\n",
    "#   '1997': 11,\n",
    "#   '1998': 12,\n",
    "#   '1999': 13,\n",
    "#   '2000': 14,\n",
    "#   '2001': 15,\n",
    "#   '2002': 16,\n",
    "#   '2003': 17,\n",
    "#   '2004': 18,\n",
    "#   '2005': 19,\n",
    "#   '2006': 20,\n",
    "#   '2007': 21,\n",
    "#   '2008': 22,\n",
    "#   '2009': 23,\n",
    "#   '2010': 24,\n",
    "#   '2011': 25,\n",
    "#   '2012': 26,\n",
    "#   '2013': 27,\n",
    "#   '2014': 28,\n",
    "#   '2015': 29,\n",
    "#  '2016': 30,\n",
    "#   '2017': 31,\n",
    "#   '2018': 32,\n",
    "#   '2019': 33,\n",
    "#   '2020': 34,\n",
    "#  '2021': 35,\n",
    "\n",
    "#  'UNK': 36\n",
    "\n",
    "# },\n",
    "#  'idx2token': {0: 'PAD',\n",
    "#   1: '1987',\n",
    "#   2: '1988',\n",
    "#   3: '1989',\n",
    "#   4: '1990',\n",
    "#   5: '1991',\n",
    "#   6: '1992',\n",
    "#   7: '1993',\n",
    "#   8: '1994',\n",
    "#   9: '1995',\n",
    "#   10: '1996',\n",
    "#   11: '1997',\n",
    "#   12: '1998',\n",
    "#   13: '1999',\n",
    "#   14: '2000',\n",
    "#   15: '2001',\n",
    "#   16: '2002',\n",
    "#   17: '2003',\n",
    "#   18: '2004',\n",
    "#   19: '2005',\n",
    "#   20: '2006',\n",
    "#   21: '2007',\n",
    "#   22: '2008',\n",
    "#   23: '2009',\n",
    "#   24: '2010',\n",
    "#   25: '2011',\n",
    "#   26: '2012',\n",
    "#   27: '2013',\n",
    "#   28: '2014',\n",
    "#   29: '2015',\n",
    "#                30: '2016',\n",
    "#   31: '2017',\n",
    "#   32: '2018',\n",
    "#   33: '2019',\n",
    "#   34: '2020',\n",
    "#   35: '2021',\n",
    "\n",
    "#   36: 'UNK'}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = utils.load_obj(file_config['vocab'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# olddic = os.path.join('/gpfs3/well/rahimi/users/gra027/JNb/ExpHypCancer/ModelBins/',\n",
    "#                                              \"BEHRT_mlm_DMBp.bin\")\n",
    "# dd = torch.load(olddic,  map_location='cpu')\n",
    "# modeld = model.state_dict()\n",
    "# modeld['bert.embeddings.age_embeddings.weight'] = dd['bert.embeddings.age_embeddings.weight']\n",
    "# modeld['bert.embeddings.year_embeddings.weight'][:31] = dd['bert.embeddings.year_embeddings.weight']\n",
    "# modeld['bert.embeddings.posi_embeddings.weight'] = dd['bert.embeddings.posi_embeddings.weight']\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for x in BertVocab['idx2token']:\n",
    "#     if 'bnf' not in BertVocab['idx2token'][x] and 'vtm' not in BertVocab['idx2token'][x] and 'BANDAGE' not in BertVocab['idx2token'][x]:\n",
    "#         if BertVocab['idx2token'][x] in BertVocabold['token2idx']:\n",
    "#             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x]]\n",
    "\n",
    "#             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#             count = count+1\n",
    "#         elif     BertVocab['idx2token'][x][:-1] in BertVocabold['token2idx']:\n",
    "#             oldindx = BertVocabold['token2idx'][BertVocab['idx2token'][x][:-1]]\n",
    "#             modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#             count = count+1\n",
    "\n",
    "#         else:\n",
    "#             for y in BertVocabold['token2idx']:\n",
    "#                 if BertVocab['idx2token'][x][:-1] in y :\n",
    "#                     oldindx = BertVocabold['token2idx'][y]\n",
    "#     #                 print(BertVocab['idx2token'][x], y )\n",
    "#                     modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#                     count = count+1\n",
    "#                     break\n",
    "#     else:\n",
    "#         if 'bnf' in BertVocab['idx2token'][x]:\n",
    "            \n",
    "#             text2check = BertVocab['idx2token'][x][4:8]\n",
    "#             if text2check in BertVocabold['token2idx']:\n",
    "#                 oldindx = BertVocabold['token2idx'][text2check]\n",
    "#                 modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "#                 count = count+1\n",
    "\n",
    "#             else:\n",
    "#                 for y in BertVocabold['token2idx']:\n",
    "#                     if text2check in y :\n",
    "# #                         print(y, text2check)\n",
    "#                         count = count+1\n",
    "#                         oldindx = BertVocabold['token2idx'][y]\n",
    "#                         modeld['bert.embeddings.word_embeddings.weight'][x] = dd['bert.embeddings.word_embeddings.weight'][oldindx]\n",
    "\n",
    "#                         break\n",
    "                \n",
    "                \n",
    "# for x in modeld:\n",
    "#     if x not in ['cls.predictions.bias','cls.predictions.decoder.weight','bert.embeddings.segment_embeddings.weight' , 'bert.embeddings.word_embeddings.weight','bert.embeddings.age_embeddings.weight', 'bert.embeddings.year_embeddings.weight', 'bert.embeddings.posi_embeddings.weight']:\n",
    "#         if x in dd:\n",
    "#             print(x)\n",
    "#             modeld[x] = dd[x]\n",
    "# model.load_state_dict(modeld)\n",
    "# print(count, len(BertVocab['idx2token']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch0\n",
      "to__save: output_model_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/rahimi/users/gra027/JNb/Graph/ModelPkg/utils.py:606: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  truepred = logs(torch.tensor(truepred))\n",
      "/gpfs3/well/rahimi/users/gra027/JNb/Graph/ModelPkg/pytorch_pretrained_bert/optimization.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378065146/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 0.012953751087188721\t| precision: 0.48677581863979846\n",
      "epoch: 0\t| Loss: 2.4878567481040954\t| precision: 0.5279085406859448\n",
      "epoch: 0\t| Loss: 2.4540855956077574\t| precision: 0.5136363636363637\n",
      "epoch: 0\t| Loss: 2.468154457807541\t| precision: 0.5137931034482759\n",
      "epoch: 0\t| Loss: 2.4593356370925905\t| precision: 0.49615877080665816\n",
      "epoch: 0\t| Loss: 2.460105185508728\t| precision: 0.5371428571428571\n",
      "epoch: 0\t| Loss: 2.4483804368972777\t| precision: 0.5322327044025157\n",
      "epoch: 0\t| Loss: 2.4518670427799223\t| precision: 0.5141196013289037\n",
      "epoch: 0\t| Loss: 2.449880166053772\t| precision: 0.5225035161744023\n",
      "epoch: 0\t| Loss: 2.4328471279144286\t| precision: 0.527106466361855\n",
      "epoch: 0\t| Loss: 2.4446197783946992\t| precision: 0.5443873807776962\n",
      "epoch: 0\t| Loss: 2.4439581596851347\t| precision: 0.5165936130244208\n",
      "epoch: 0\t| Loss: 2.450011568069458\t| precision: 0.5299026425591099\n",
      "epoch: 0\t| Loss: 2.451076979637146\t| precision: 0.5404120443740095\n",
      "epoch: 0\t| Loss: 2.437532482147217\t| precision: 0.5332950631458094\n",
      "mid epoch: 443852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.444683252573013\t| precision: 0.5301970756516211\n",
      "epoch: 0\t| Loss: 2.4420347321033478\t| precision: 0.5510499637943519\n",
      "epoch: 0\t| Loss: 2.449271160364151\t| precision: 0.5083333333333333\n",
      "epoch: 0\t| Loss: 2.4261647176742556\t| precision: 0.5414798206278026\n",
      "epoch: 0\t| Loss: 2.449007625579834\t| precision: 0.4915572232645403\n",
      "epoch: 0\t| Loss: 2.4426936960220336\t| precision: 0.5274447949526814\n",
      "epoch: 0\t| Loss: 2.4499268078804017\t| precision: 0.5487528344671202\n",
      "epoch: 0\t| Loss: 2.43973046541214\t| precision: 0.5089349535382416\n",
      "epoch: 0\t| Loss: 2.4413638603687287\t| precision: 0.5390211640211641\n",
      "epoch: 0\t| Loss: 2.446713958978653\t| precision: 0.5015654351909831\n",
      "epoch: 0\t| Loss: 2.446685107946396\t| precision: 0.5169117647058824\n",
      "epoch: 0\t| Loss: 2.438692547082901\t| precision: 0.5169971671388102\n",
      "epoch: 0\t| Loss: 2.4338243985176087\t| precision: 0.4923179692718771\n",
      "epoch: 0\t| Loss: 2.4358081436157226\t| precision: 0.5118549511854951\n",
      "epoch: 0\t| Loss: 2.447886972427368\t| precision: 0.522207267833109\n",
      "mid epoch: 887852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.4415105676651\t| precision: 0.4947683109118087\n",
      "epoch: 0\t| Loss: 2.4408195662498473\t| precision: 0.5136425648021828\n",
      "epoch: 0\t| Loss: 2.429922022819519\t| precision: 0.5249643366619116\n",
      "epoch: 0\t| Loss: 2.4329240214824677\t| precision: 0.5228884590586719\n",
      "epoch: 0\t| Loss: 2.431165724992752\t| precision: 0.5386740331491713\n",
      "epoch: 0\t| Loss: 2.421400817632675\t| precision: 0.5282051282051282\n",
      "epoch: 0\t| Loss: 2.4454362547397612\t| precision: 0.5147891755821271\n",
      "epoch: 0\t| Loss: 2.4381529557704926\t| precision: 0.5368360936607652\n",
      "epoch: 0\t| Loss: 2.4369933223724365\t| precision: 0.50236008091706\n",
      "epoch: 0\t| Loss: 2.438367450237274\t| precision: 0.5047297297297297\n",
      "epoch: 0\t| Loss: 2.4411159336566923\t| precision: 0.5281111813013266\n",
      "epoch: 0\t| Loss: 2.430768463611603\t| precision: 0.5190140845070422\n",
      "epoch: 0\t| Loss: 2.429804564714432\t| precision: 0.5282767797737857\n",
      "epoch: 0\t| Loss: 2.4404177236557008\t| precision: 0.5170166545981173\n",
      "epoch: 0\t| Loss: 2.433532485961914\t| precision: 0.5098870056497176\n",
      "mid epoch: 1331852..... ** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 0\t| Loss: 2.428547774553299\t| precision: 0.5180887372013652\n",
      "epoch: 0\t| Loss: 2.4327532839775086\t| precision: 0.47436773752563227\n",
      "epoch: 0\t| Loss: 2.428333899974823\t| precision: 0.5082726671078756\n",
      "epoch: 0\t| Loss: 2.4297738540172578\t| precision: 0.4919614147909968\n",
      "epoch: 0\t| Loss: 2.429713096618652\t| precision: 0.5006613756613757\n",
      "epoch: 0\t| Loss: 2.436156085729599\t| precision: 0.5096731154102735\n",
      "epoch: 0\t| Loss: 2.440213499069214\t| precision: 0.5177304964539007\n",
      "epoch: 0\t| Loss: 2.422397874593735\t| precision: 0.5081168831168831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(model, e, validload, optim):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(validload):\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "        age_ids, year_ids,input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "#         print(batch)\n",
    "        loss, pred, label = model(input_ids, age_ids, segment_ids, posi_ids,year_ids,  attention_mask=attMask,\n",
    "                                  masked_lm_labels=masked_label)\n",
    "\n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if step % 200 == 0:\n",
    "            print(\"epoch: {}\\t| Loss: {}\\t| precision: {}\".format(e, temp_loss / 200, cal_acc(label, pred)))\n",
    "            temp_loss = 0\n",
    "\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        if (step+1 )%3000 ==0:\n",
    "            print(\"mid epoch: \" +str(step *148) + \"..... ** ** * Saving fine - tuned model ** ** * \")\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "print('starting epoch0')\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_newcut1985_2020_DM__6msummary.bin\")\n",
    "\n",
    "print('to__save: output_model_file')\n",
    "for e in range(50):\n",
    "    train(model, e, trainload, optim)\n",
    "\n",
    "    print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "    #         create_folder(global_params['output_dir'])\n",
    "    print('done epoch', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python MLGPvision",
   "language": "python",
   "name": "mlgpvision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
