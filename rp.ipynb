{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting run....\n",
      "starting run....\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengxian/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "print('starting run....')\n",
    "\n",
    "import sys\n",
    "\n",
    "print('starting run....')\n",
    "\n",
    "sys.path.insert(0,'/home/zhengxian/BEHRT/ModelPkg/')\n",
    "\n",
    "from ModelPkg.BEHRTraw import *\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from ModelPkg import utils\n",
    "from ModelPkg.MLMRaw import *\n",
    "\n",
    "from ModelPkg.DataProc import *\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from pytorch_pretrained_bert import optimizer\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zhengxian/BEHRT'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     10\u001b[0m optim_config \u001b[39m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3e-5\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwarmup_proportion\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.1\u001b[39m,\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     15\u001b[0m global_params \u001b[39m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m128\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgradient_accumulation_steps\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[39m'\u001b[39m\u001b[39myearOn\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m }\n\u001b[0;32m---> 31\u001b[0m YearVocab \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mload_obj(file_config[\u001b[39m'\u001b[39m\u001b[39myearVocab\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     32\u001b[0m create_folder(global_params[\u001b[39m'\u001b[39m\u001b[39moutput_dir\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     33\u001b[0m BertVocab \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mload_obj(file_config[\u001b[39m'\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "file_config = {\n",
    "        'vocab': '/home/zhengxian/BEHRT/Data/GeneralVocDMProc_25k',\n",
    "    'fulld': '/home/zhengxian/flan_t5/data_censored_6_months/train.parquet',\n",
    "    'testd': '/home/zhengxian/flan_t5/data_censored_6_months/test.parquet',\n",
    "    #'fulld': '/gpfs3/well/rahimi/users/gra027/JNb/general_model_newCutCPRD/Data/MLM_for_pretraining_28M_1985_2020__unique_per6m_50pc_sample___10kdebug.parquet/',\n",
    "    'yearVocab':  '/home/zhengxian/BEHRT/Data/yearVoc_1985_2021',\n",
    "}\n",
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "}\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 128,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:1',\n",
    "    'output_dir':'/home/zhengxian/BEHRT/SavedModels/',\n",
    "    'output_name': 'risk_pred.bin',\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 250,\n",
    "    'max_age': 110,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    'yearOn':True\n",
    "}\n",
    "\n",
    "\n",
    "YearVocab = utils.load_obj(file_config['yearVocab'])\n",
    "create_folder(global_params['output_dir'])\n",
    "BertVocab = utils.load_obj(file_config['vocab'])\n",
    "print(len(BertVocab['token2idx']))\n",
    "\n",
    "ageVocab, _ = utils.age_vocab(max_age=global_params['max_age'], year=global_params['age_year'], symbol=global_params['age_symbol'])\n",
    "fulldata = pd.read_parquet(file_config['fulld'])\n",
    "testdata = pd.read_parquet(file_config['testd'])\n",
    "print('read data....')\n",
    "\n",
    "trainSet = RPLoader(token2idx=BertVocab['token2idx'], dataframe=fulldata, max_len=global_params['max_len_seq'], max_age=global_params['max_age'], year=global_params['age_year'], age_symbol=global_params['age_symbol'],year2idx = YearVocab['token2idx'] )\n",
    "trainload = DataLoader(dataset=trainSet, batch_size=global_params['batch_size'], shuffle=True)\n",
    "\n",
    "testSet = RPLoader(token2idx=BertVocab['token2idx'], dataframe=testdata, max_len=global_params['max_len_seq'], max_age=global_params['max_age'], year=global_params['age_year'], age_symbol=global_params['age_symbol'],year2idx = YearVocab['token2idx'] )\n",
    "testload = DataLoader(dataset=testSet, batch_size=global_params['batch_size'], shuffle=False)\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 150, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.1, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 6, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "    'intermediate_size': 108, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range,\n",
    "    'yearOn':True,\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()),\n",
    "\n",
    "}\n",
    "conf = BertConfig(model_config)\n",
    "model = BertRP(conf, num_labels=1)\n",
    "output_model_file = os.path.join(global_params['output_dir'], \"MLM_CEHR_newcut1985_2020_DMProc__6msummary.bin\")\n",
    "torch.load(output_model_file)\n",
    "model = toLoad(model, output_model_file)\n",
    "model = model.to(global_params['device'])\n",
    "optim = optimizer.adam(params=list(model.named_parameters()), config=optim_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.load(output_model_file)\n",
    "# for name, parameter in model.named_parameters():\n",
    "#     print(f\"Parameter name: {name}\")\n",
    "#     print(torch.equal(parameter, a[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = utils.load_obj(file_config['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(dataset, model):\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataset, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "            age_ids, year_ids, input_ids, posi_ids, segment_ids, attMask, labels = batch\n",
    "            loss, logits = model(input_ids, age_ids, segment_ids, posi_ids, year_ids, attention_mask=attMask, labels=labels)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            \n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return roc_auc(all_logits, all_labels)[0], ap(all_logits, all_labels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_eval(trainload, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch0\n",
      "to__save: output_model_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Loss: 0.0035052013397216795\t| auc-roc: 0.564344746162928, ap: 0.08315189712248536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengxian/BEHRT/pytorch_pretrained_bert/optimization.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid epoch: 14652..... ** ** * Saving fine - tuned model ** ** * \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4233/4233 [07:42<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc-roc, ap 0.96 0.5\n",
      "epoch: 0\t| Loss: 0.11998083509504795\t| auc-roc: 0.5454545454545454, ap: 0.0662452617800666\n",
      "mid epoch: 29452..... ** ** * Saving fine - tuned model ** ** * \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4233/4233 [07:41<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc-roc, ap 1.0 1.0\n",
      "epoch: 0\t| Loss: 0.08306854881346226\t| auc-roc: 0.8870214752567692, ap: 0.8038090936464921\n",
      "mid epoch: 44252..... ** ** * Saving fine - tuned model ** ** * \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output_model_file = os.path.join(global_params['output_dir'], \"HF_RP_6msummary.bin\")\n",
    "best_ap = 0\n",
    "\n",
    "def train(model, e, validload, optim):\n",
    "    global best_ap\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(validload):\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "        # print(batch)\n",
    "        age_ids, year_ids,input_ids, posi_ids, segment_ids, attMask, labels = batch\n",
    "\n",
    "        loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,year_ids, attention_mask=attMask,\n",
    "                                  labels=labels)\n",
    "\n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch: {}\\t| Loss: {}\\t| auc-roc: {}, ap: {}\".format(e, temp_loss / 200, roc_auc(logits, labels)[0], ap(logits, labels)[0]))\n",
    "            temp_loss = 0\n",
    "\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        if (step+1 )%100 ==0:\n",
    "            print(\"mid epoch: \" +str(step *148) + \"..... ** ** * Saving fine - tuned model ** ** * \")\n",
    "            auroc_value, ap_value = do_eval(testload, model)\n",
    "            print(\"auc-roc, ap\", auroc_value, ap_value)\n",
    "            if ap_value > best_ap:\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                best_ap = ap_value\n",
    "                \n",
    "print('starting epoch0')\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "# output_model_file = os.path.join(global_params['output_dir'], \"HF_RP_6msummary_final.bin\")\n",
    "\n",
    "print('to__save: output_model_file')\n",
    "for e in tqdm(range(50), desc='Training epoch'):\n",
    "    train(model, e, trainload, optim)\n",
    "\n",
    "    print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "    # create_folder(global_params['output_dir'])\n",
    "    print('done epoch', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
